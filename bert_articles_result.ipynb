{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert-articles_result.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MargaritaKotova/Sentiment-analysis/blob/master/bert_articles_result.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ifgb7lh4N2I4",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dYoyVhjWL8a",
        "colab_type": "code",
        "outputId": "0af246e7-b3ac-43f6-ed82-15e6b69f57a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!pip install bert-tensorflow"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KECZxOx7Oof7",
        "colab": {}
      },
      "source": [
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_PgN-AbHO55r",
        "outputId": "e5db1305-a7e6-4d20-836c-2eccd41855bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Set the output directory for saving model file\n",
        "# Optionally, set a GCP bucket location\n",
        "\n",
        "OUTPUT_DIR = 'output_dir'#@param {type:\"string\"}\n",
        "#@markdown Whether or not to clear/delete the directory and create a new one\n",
        "DO_DELETE = True #@param {type:\"boolean\"}\n",
        "#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n",
        "USE_BUCKET = False #@param {type:\"boolean\"}\n",
        "BUCKET = 'BUCKET_NAME' #@param {type:\"string\"}\n",
        "\n",
        "if USE_BUCKET:\n",
        "  OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "\n",
        "if DO_DELETE:\n",
        "  try:\n",
        "    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
        "  except:\n",
        "    # Doesn't matter if the directory didn't exist\n",
        "    pass\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Model output directory: output_dir *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omW3n1EzVdiJ",
        "colab_type": "text"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wd0keVaVdiM",
        "colab_type": "text"
      },
      "source": [
        "First, downloading the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD7K5WC7VdiO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vC7fPh1WZeW",
        "colab_type": "code",
        "outputId": "89381489-901e-4bb0-f2c1-43ffd9bac44a",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-78c0c869-f639-42e5-a06e-c934887eb5d2\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-78c0c869-f639-42e5-a06e-c934887eb5d2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving data.csv to data (1).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_plYByuXbfr",
        "colab_type": "code",
        "outputId": "aea0c83a-a448-4b0e-b9f8-cd3f2be5d48a",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-dddd04b5-527f-4f38-a6c9-b0684edbb23b\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-dddd04b5-527f-4f38-a6c9-b0684edbb23b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving data_DataGoogleNews_labeled.csv to data_DataGoogleNews_labeled (1).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp-7MSfLXQyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4QVvWFUVdiW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = pd.read_csv(io.StringIO(uploaded['data.csv'].decode('utf-8')), sep=';')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0gbHcLEVdie",
        "colab_type": "code",
        "outputId": "b7928080-e941-47d9-fe7f-5cf7feb8bd28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        }
      },
      "source": [
        "test.tail(6)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Source</th>\n",
              "      <th>Author</th>\n",
              "      <th>Title</th>\n",
              "      <th>Url</th>\n",
              "      <th>Date</th>\n",
              "      <th>Article</th>\n",
              "      <th>Polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>490</td>\n",
              "      <td>BBC News</td>\n",
              "      <td>https://www.facebook.com/bbcnews</td>\n",
              "      <td>Five ways the British landscape changed in 10 ...</td>\n",
              "      <td>https://www.bbc.co.uk/news/uk-51041436</td>\n",
              "      <td>2020-01-19T01:41:49Z</td>\n",
              "      <td>Britain is more built-on than ever, with 44.8 ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>491</td>\n",
              "      <td>Fortune</td>\n",
              "      <td>Jeremy Kahn</td>\n",
              "      <td>Facebook wants better A.I. tools. But superint...</td>\n",
              "      <td>https://fortune.com/2020/01/20/facebook-artifi...</td>\n",
              "      <td>2020-01-20T09:30:00Z</td>\n",
              "      <td>In early January, Facebook released software t...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>492</td>\n",
              "      <td>The Verge</td>\n",
              "      <td>James Vincent</td>\n",
              "      <td>Alphabet CEO Sundar Pichai says there is ‘no q...</td>\n",
              "      <td>https://www.theverge.com/2020/1/20/21073682/ai...</td>\n",
              "      <td>2020-01-20T10:30:05Z</td>\n",
              "      <td>Google and Alphabet CEO Sundar Pichai has call...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>493</td>\n",
              "      <td>Reuters</td>\n",
              "      <td>Foo Yun Chee and John Chalmers</td>\n",
              "      <td>Google owner calls for 'proportionate approach...</td>\n",
              "      <td>https://ca.reuters.com/article/technologyNews/...</td>\n",
              "      <td>2020-01-20T12:01:46Z</td>\n",
              "      <td>BRUSSELS (Reuters) - The EU’s proposal for a t...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>494</td>\n",
              "      <td>TechCrunch</td>\n",
              "      <td>Natasha Lomas</td>\n",
              "      <td>Google's Sundar Pichai doesn't want you to be ...</td>\n",
              "      <td>https://techcrunch.com/2020/01/20/googles-sund...</td>\n",
              "      <td>2020-01-20T14:11:15Z</td>\n",
              "      <td>Alphabet  and Google CEO, Sundar Pichai, is th...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>495</td>\n",
              "      <td>BBC News</td>\n",
              "      <td>https://www.facebook.com/bbcnews</td>\n",
              "      <td>Google boss Sundar Pichai calls for AI regulation</td>\n",
              "      <td>https://www.bbc.co.uk/news/technology-51178198</td>\n",
              "      <td>2020-01-20T15:40:00Z</td>\n",
              "      <td>The head of Google and parent company Alphabet...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Unnamed: 0  ...  Polarity\n",
              "172         490  ...  positive\n",
              "173         491  ...  positive\n",
              "174         492  ...  negative\n",
              "175         493  ...  negative\n",
              "176         494  ...  negative\n",
              "177         495  ...  negative\n",
              "\n",
              "[6 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOa7FWWGVdiq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv(io.StringIO(uploaded['data_DataGoogleNews_labeled.csv'].decode('utf-8')))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK1YxGyHVdix",
        "colab_type": "code",
        "outputId": "b8876762-6793-4a93-cc03-ec13bd7c5200",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "train.tail(2)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source</th>\n",
              "      <th>Author</th>\n",
              "      <th>Title</th>\n",
              "      <th>Url</th>\n",
              "      <th>Date</th>\n",
              "      <th>Article</th>\n",
              "      <th>Polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>315</th>\n",
              "      <td>Reuters</td>\n",
              "      <td>Reuters Editorial</td>\n",
              "      <td>U.S.-Israeli startup Gong raises $65 million, ...</td>\n",
              "      <td>https://www.reuters.com/article/us-tech-gong-f...</td>\n",
              "      <td>2019-12-03T13:59:16Z</td>\n",
              "      <td>TEL AVIV (Reuters) - U.S.-Israeli startup Gong...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>316</th>\n",
              "      <td>Fortune</td>\n",
              "      <td>jonathanvanian2015</td>\n",
              "      <td>Arguing About Artificial Intelligence Killing ...</td>\n",
              "      <td>https://fortune.com/2019/12/03/artificial-inte...</td>\n",
              "      <td>2019-12-03T16:06:23Z</td>\n",
              "      <td>This is the web version of Eye on A.I., Fortun...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Source  ...  Polarity\n",
              "315  Reuters  ...  positive\n",
              "316  Fortune  ...  negative\n",
              "\n",
              "[2 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M1xCAvh8SnYk",
        "outputId": "424065d4-bd11-40e3-f645-17e765480f3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "test.columns"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'Source', 'Author', 'Title', 'Url', 'Date', 'Article',\n",
              "       'Polarity'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N83a_MqnVdjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(train)):\n",
        "    if train['Polarity'][i]=='negative':\n",
        "            train['Polarity'][i]=0\n",
        "    elif train['Polarity'][i]=='neutral':\n",
        "            train['Polarity'][i]=1\n",
        "    elif train['Polarity'][i]=='positive':\n",
        "            train['Polarity'][i]=2       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFtIa7SkVdjE",
        "colab_type": "code",
        "outputId": "62fc9a8c-fb7d-4373-fc1b-5b9d09b4b75b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "for i in range(len(test)):\n",
        "    if test['Polarity'][i]=='negative':\n",
        "            test['Polarity'][i]=0\n",
        "    elif test['Polarity'][i]=='neutral ' or test['Polarity'][i]=='neutral':\n",
        "            test['Polarity'][i]=1\n",
        "    elif test['Polarity'][i]=='positive ' or test['Polarity'][i]=='positive' or test['Polarity'][i]=='Positive':\n",
        "            test['Polarity'][i]=2 "
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wxBWlJ3VdjJ",
        "colab_type": "code",
        "outputId": "1ee136f3-85b6-41d9-f125-b5c961b6113d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "train['Polarity'].value_counts()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    146\n",
              "0     98\n",
              "1     73\n",
              "Name: Polarity, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCznMdojVdjP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GG7RXMM-VdjV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(train)):\n",
        "    if train['Polarity'][i]==np.nan:\n",
        "            print(train['Polarity'][i])\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syqxfYgUVdja",
        "colab_type": "text"
      },
      "source": [
        "The input data is the 'sentence' column and our label is the 'polarity' column (0, 1 for negative and positive, respecitvely)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW6grahGVdjc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['Polarity'][168]=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wFMW71QVdjh",
        "colab_type": "code",
        "outputId": "a2bd96ef-797f-43d2-ea80-afc3db807ae1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(test['Polarity'][7])"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "int"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lW_pjcXdVdjn",
        "colab_type": "code",
        "outputId": "a9ddfbc1-c182-4edc-87fd-9844e0e944a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test['Polarity'].unique()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 0, 1], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5mue7vWEXcM7",
        "colab": {}
      },
      "source": [
        "DATA_COLUMN = 'Article'\n",
        "LABEL_COLUMN = 'Polarity'\n",
        "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
        "label_list = [0, 1, 2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "enTYpFTzzl1o"
      },
      "source": [
        "**Data** **Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdTzzN52Vdjv",
        "colab_type": "text"
      },
      "source": [
        "We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  InputExample's using the constructor provided in the BERT library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCVT3J9MVdjw",
        "colab_type": "text"
      },
      "source": [
        "text_a is the text we want to classify, which in this case, is the Request field in our Dataframe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq7lFo4VVdjx",
        "colab_type": "text"
      },
      "source": [
        "text_b is used if we're training a model to understand the relationship between sentences (i.e. is text_b a translation of text_a? Is text_b an answer to the question asked by text_a?). This doesn't apply to our task, so we can leave text_b blank."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD0nKOgXVdjy",
        "colab_type": "text"
      },
      "source": [
        "label is the label for our example, i.e. True, False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QTwH-QeaXgi2",
        "colab": {}
      },
      "source": [
        "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
        "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AbtTDVWVdj3",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2_kK3AoVdj4",
        "colab_type": "text"
      },
      "source": [
        "###### 1.Lowercase our text (if we're using a BERT lowercase model)\n",
        "##### 2.Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"]) \n",
        "##### 3.Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"]) \n",
        "##### 4.Map our words to indexes using a vocab file that BERT provides \n",
        "##### 5.Add special \"CLS\" and \"SEP\" tokens (see the readme) \n",
        "##### 6.Append \"index\" and \"segment\" tokens to each input (see the BERT paper)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iFnD0qgjXvQc",
        "outputId": "d579e584-5c41-4029-ec72-d9c3aa30f892",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# This is a path to an uncased (all lowercase) version of BERT\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    with tf.Session() as sess:\n",
        "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                            tokenization_info[\"do_lower_case\"]])\n",
        "      \n",
        "  return bert.tokenization.FullTokenizer(\n",
        "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module()"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2CiTMzN6X53j",
        "outputId": "1ac0c753-27c3-4d77-d96d-95555bcabba7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this',\n",
              " 'here',\n",
              " \"'\",\n",
              " 's',\n",
              " 'an',\n",
              " 'example',\n",
              " 'of',\n",
              " 'using',\n",
              " 'the',\n",
              " 'bert',\n",
              " 'token',\n",
              " '##izer']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EpbTdD_B6VPh",
        "outputId": "580bbdf5-88cc-42ab-c1b8-a11432478f28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "train_InputExamples.head()"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    <bert.run_classifier.InputExample object at 0x...\n",
              "1    <bert.run_classifier.InputExample object at 0x...\n",
              "2    <bert.run_classifier.InputExample object at 0x...\n",
              "3    <bert.run_classifier.InputExample object at 0x...\n",
              "4    <bert.run_classifier.InputExample object at 0x...\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZXFztm-VdkI",
        "colab_type": "text"
      },
      "source": [
        "Using our tokenizer, we'll call run_classifier.convert_examples_to_features on our InputExamples to convert them into features BERT understands."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JcPSeoHkYBLt",
        "outputId": "9306bf87-d28d-4171-9c76-3c8c12f586de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# We'll set sequences to be at most 128 tokens long.\n",
        "MAX_SEQ_LENGTH = 128\n",
        "# Convert our train and test features to InputFeatures that BERT understands.\n",
        "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 317\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 317\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] when president barack obama and his advisors set out to red ##ire ##ct american foreign policy from the middle east to asia , they invoked a metaphor from basketball , the game that the president famously played and loved . the united states would “ pi ##vot ” to asia where our greatest challenges and opportunities in this century will all un ##fold . years later , in the midst of the u . s . - china trade war and the trump administration ’ s new strategic competition with china , it may be basketball that awake ##ns america to the real scale of our bu ##rgeon ##ing china problem . it may be basketball that awake ##ns america to the real scale of our [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] when president barack obama and his advisors set out to red ##ire ##ct american foreign policy from the middle east to asia , they invoked a metaphor from basketball , the game that the president famously played and loved . the united states would “ pi ##vot ” to asia where our greatest challenges and opportunities in this century will all un ##fold . years later , in the midst of the u . s . - china trade war and the trump administration ’ s new strategic competition with china , it may be basketball that awake ##ns america to the real scale of our bu ##rgeon ##ing china problem . it may be basketball that awake ##ns america to the real scale of our [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2043 2343 13857 8112 1998 2010 18934 2275 2041 2000 2417 7442 6593 2137 3097 3343 2013 1996 2690 2264 2000 4021 1010 2027 24959 1037 19240 2013 3455 1010 1996 2208 2008 1996 2343 18172 2209 1998 3866 1012 1996 2142 2163 2052 1523 14255 22994 1524 2000 4021 2073 2256 4602 7860 1998 6695 1999 2023 2301 2097 2035 4895 10371 1012 2086 2101 1010 1999 1996 12930 1997 1996 1057 1012 1055 1012 1011 2859 3119 2162 1998 1996 8398 3447 1521 1055 2047 6143 2971 2007 2859 1010 2009 2089 2022 3455 2008 8300 3619 2637 2000 1996 2613 4094 1997 2256 20934 28242 2075 2859 3291 1012 2009 2089 2022 3455 2008 8300 3619 2637 2000 1996 2613 4094 1997 2256 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2043 2343 13857 8112 1998 2010 18934 2275 2041 2000 2417 7442 6593 2137 3097 3343 2013 1996 2690 2264 2000 4021 1010 2027 24959 1037 19240 2013 3455 1010 1996 2208 2008 1996 2343 18172 2209 1998 3866 1012 1996 2142 2163 2052 1523 14255 22994 1524 2000 4021 2073 2256 4602 7860 1998 6695 1999 2023 2301 2097 2035 4895 10371 1012 2086 2101 1010 1999 1996 12930 1997 1996 1057 1012 1055 1012 1011 2859 3119 2162 1998 1996 8398 3447 1521 1055 2047 6143 2971 2007 2859 1010 2009 2089 2022 3455 2008 8300 3619 2637 2000 1996 2613 4094 1997 2256 20934 28242 2075 2859 3291 1012 2009 2089 2022 3455 2008 8300 3619 2637 2000 1996 2613 4094 1997 2256 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] the early - stage startup community knows that the disrupt main stage is the place to hear and learn from iconic founders , techno ##logists and investment leaders . and the speakers you ’ ll hear at disrupt berlin 2019 on 11 - 12 december will follow that grand tradition . hold up a sec . don ’ t have a ticket yet ? buy your early - bird pass today and save up to € ##500 . okay , back to the point at hand . “ always leave them wanting more ” — a phrase often attributed to p . t . barn ##um — also applies to the disrupt main stage . “ more ” is what disrupt attendees frequently tell us they [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] the early - stage startup community knows that the disrupt main stage is the place to hear and learn from iconic founders , techno ##logists and investment leaders . and the speakers you ’ ll hear at disrupt berlin 2019 on 11 - 12 december will follow that grand tradition . hold up a sec . don ’ t have a ticket yet ? buy your early - bird pass today and save up to € ##500 . okay , back to the point at hand . “ always leave them wanting more ” — a phrase often attributed to p . t . barn ##um — also applies to the disrupt main stage . “ more ” is what disrupt attendees frequently tell us they [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1996 2220 1011 2754 22752 2451 4282 2008 1996 23217 2364 2754 2003 1996 2173 2000 2963 1998 4553 2013 14430 8759 1010 21416 18738 1998 5211 4177 1012 1998 1996 7492 2017 1521 2222 2963 2012 23217 4068 10476 2006 2340 1011 2260 2285 2097 3582 2008 2882 4535 1012 2907 2039 1037 10819 1012 2123 1521 1056 2031 1037 7281 2664 1029 4965 2115 2220 1011 4743 3413 2651 1998 3828 2039 2000 1574 29345 1012 3100 1010 2067 2000 1996 2391 2012 2192 1012 1523 2467 2681 2068 5782 2062 1524 1517 1037 7655 2411 7108 2000 1052 1012 1056 1012 8659 2819 1517 2036 12033 2000 1996 23217 2364 2754 1012 1523 2062 1524 2003 2054 23217 19973 4703 2425 2149 2027 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1996 2220 1011 2754 22752 2451 4282 2008 1996 23217 2364 2754 2003 1996 2173 2000 2963 1998 4553 2013 14430 8759 1010 21416 18738 1998 5211 4177 1012 1998 1996 7492 2017 1521 2222 2963 2012 23217 4068 10476 2006 2340 1011 2260 2285 2097 3582 2008 2882 4535 1012 2907 2039 1037 10819 1012 2123 1521 1056 2031 1037 7281 2664 1029 4965 2115 2220 1011 4743 3413 2651 1998 3828 2039 2000 1574 29345 1012 3100 1010 2067 2000 1996 2391 2012 2192 1012 1523 2467 2681 2068 5782 2062 1524 1517 1037 7655 2411 7108 2000 1052 1012 1056 1012 8659 2819 1517 2036 12033 2000 1996 23217 2364 2754 1012 1523 2062 1524 2003 2054 23217 19973 4703 2425 2149 2027 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] wu ##zh ##en , china ( reuters ) - at one of the world ’ s show ##piece tech conferences in china , ji ##bes at the united states for its ‘ bully behavior ’ lent a cold war tone to proceedings as trade tension once again reared its ugly head in an event that drew a dear ##th of top u . s . executives . file photo : chinese and u . s . flags flutter near the bun ##d , before u . s . trade delegation meet their chinese counterparts for talks in shanghai , china july 30 , 2019 . reuters / al ##y song the state - run world internet conference , one of the country ’ s most prominent [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] wu ##zh ##en , china ( reuters ) - at one of the world ’ s show ##piece tech conferences in china , ji ##bes at the united states for its ‘ bully behavior ’ lent a cold war tone to proceedings as trade tension once again reared its ugly head in an event that drew a dear ##th of top u . s . executives . file photo : chinese and u . s . flags flutter near the bun ##d , before u . s . trade delegation meet their chinese counterparts for talks in shanghai , china july 30 , 2019 . reuters / al ##y song the state - run world internet conference , one of the country ’ s most prominent [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 8814 27922 2368 1010 2859 1006 26665 1007 1011 2012 2028 1997 1996 2088 1521 1055 2265 11198 6627 9281 1999 2859 1010 10147 12681 2012 1996 2142 2163 2005 2049 1520 20716 5248 1521 15307 1037 3147 2162 4309 2000 8931 2004 3119 6980 2320 2153 23295 2049 9200 2132 1999 2019 2724 2008 3881 1037 6203 2705 1997 2327 1057 1012 1055 1012 12706 1012 5371 6302 1024 2822 1998 1057 1012 1055 1012 9245 23638 2379 1996 21122 2094 1010 2077 1057 1012 1055 1012 3119 10656 3113 2037 2822 14562 2005 7566 1999 8344 1010 2859 2251 2382 1010 10476 1012 26665 1013 2632 2100 2299 1996 2110 1011 2448 2088 4274 3034 1010 2028 1997 1996 2406 1521 1055 2087 4069 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 8814 27922 2368 1010 2859 1006 26665 1007 1011 2012 2028 1997 1996 2088 1521 1055 2265 11198 6627 9281 1999 2859 1010 10147 12681 2012 1996 2142 2163 2005 2049 1520 20716 5248 1521 15307 1037 3147 2162 4309 2000 8931 2004 3119 6980 2320 2153 23295 2049 9200 2132 1999 2019 2724 2008 3881 1037 6203 2705 1997 2327 1057 1012 1055 1012 12706 1012 5371 6302 1024 2822 1998 1057 1012 1055 1012 9245 23638 2379 1996 21122 2094 1010 2077 1057 1012 1055 1012 3119 10656 3113 2037 2822 14562 2005 7566 1999 8344 1010 2859 2251 2382 1010 10476 1012 26665 1013 2632 2100 2299 1996 2110 1011 2448 2088 4274 3034 1010 2028 1997 1996 2406 1521 1055 2087 4069 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] ( reuters ) - u . s . quantum computing startup ion ##q said on tuesday it raised $ 55 million in a funding round that was led by venture funds backed by samsung electronics and the government of the united arab emirates . with the investments from samsung catalyst fund and mu ##bad ##ala capital , maryland - based ion ##q said its total funds raised to date reached $ 77 million . the company didn ’ t disclose its valuation . researchers believe quantum computers could operate millions of times faster than today ’ s advanced super ##com ##put ##ers , making potential tasks ranging from mapping complex molecular structures and chemical reactions to boost ##ing the power of artificial intelligence possible . alphabet [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] ( reuters ) - u . s . quantum computing startup ion ##q said on tuesday it raised $ 55 million in a funding round that was led by venture funds backed by samsung electronics and the government of the united arab emirates . with the investments from samsung catalyst fund and mu ##bad ##ala capital , maryland - based ion ##q said its total funds raised to date reached $ 77 million . the company didn ’ t disclose its valuation . researchers believe quantum computers could operate millions of times faster than today ’ s advanced super ##com ##put ##ers , making potential tasks ranging from mapping complex molecular structures and chemical reactions to boost ##ing the power of artificial intelligence possible . alphabet [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1006 26665 1007 1011 1057 1012 1055 1012 8559 9798 22752 10163 4160 2056 2006 9857 2009 2992 1002 4583 2454 1999 1037 4804 2461 2008 2001 2419 2011 6957 5029 6153 2011 19102 8139 1998 1996 2231 1997 1996 2142 5424 14041 1012 2007 1996 10518 2013 19102 16771 4636 1998 14163 9024 7911 3007 1010 5374 1011 2241 10163 4160 2056 2049 2561 5029 2992 2000 3058 2584 1002 6255 2454 1012 1996 2194 2134 1521 1056 26056 2049 26004 1012 6950 2903 8559 7588 2071 5452 8817 1997 2335 5514 2084 2651 1521 1055 3935 3565 9006 18780 2545 1010 2437 4022 8518 7478 2013 12375 3375 8382 5090 1998 5072 9597 2000 12992 2075 1996 2373 1997 7976 4454 2825 1012 12440 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1006 26665 1007 1011 1057 1012 1055 1012 8559 9798 22752 10163 4160 2056 2006 9857 2009 2992 1002 4583 2454 1999 1037 4804 2461 2008 2001 2419 2011 6957 5029 6153 2011 19102 8139 1998 1996 2231 1997 1996 2142 5424 14041 1012 2007 1996 10518 2013 19102 16771 4636 1998 14163 9024 7911 3007 1010 5374 1011 2241 10163 4160 2056 2049 2561 5029 2992 2000 3058 2584 1002 6255 2454 1012 1996 2194 2134 1521 1056 26056 2049 26004 1012 6950 2903 8559 7588 2071 5452 8817 1997 2335 5514 2084 2651 1521 1055 3935 3565 9006 18780 2545 1010 2437 4022 8518 7478 2013 12375 3375 8382 5090 1998 5072 9597 2000 12992 2075 1996 2373 1997 7976 4454 2825 1012 12440 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 2 (id = 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 2 (id = 2)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] video - sharing app ti ##kt ##ok is being used by islamic state militants to spread propaganda to a younger audience , according to the wall street journal . supporters reportedly shared videos showing is fighters with guns and corpses being parade ##d through streets . some of the videos , posted in recent weeks , are said to have included ti ##kt ##ok augmented - reality effects that stream stars or hearts on top of the content . videos flag ##ged by the newspaper have been removed and 24 accounts blocked . the videos were identified by the social - media intelligence sharing company story ##ful . \" this discovery fits with a pattern we ' ve seen over some years of ex ##tre ##mist [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] video - sharing app ti ##kt ##ok is being used by islamic state militants to spread propaganda to a younger audience , according to the wall street journal . supporters reportedly shared videos showing is fighters with guns and corpses being parade ##d through streets . some of the videos , posted in recent weeks , are said to have included ti ##kt ##ok augmented - reality effects that stream stars or hearts on top of the content . videos flag ##ged by the newspaper have been removed and 24 accounts blocked . the videos were identified by the social - media intelligence sharing company story ##ful . \" this discovery fits with a pattern we ' ve seen over some years of ex ##tre ##mist [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2678 1011 6631 10439 14841 25509 6559 2003 2108 2109 2011 5499 2110 17671 2000 3659 10398 2000 1037 3920 4378 1010 2429 2000 1996 2813 2395 3485 1012 6793 7283 4207 6876 4760 2003 7299 2007 4409 1998 18113 2108 7700 2094 2083 4534 1012 2070 1997 1996 6876 1010 6866 1999 3522 3134 1010 2024 2056 2000 2031 2443 14841 25509 6559 19335 1011 4507 3896 2008 5460 3340 2030 8072 2006 2327 1997 1996 4180 1012 6876 5210 5999 2011 1996 3780 2031 2042 3718 1998 2484 6115 8534 1012 1996 6876 2020 4453 2011 1996 2591 1011 2865 4454 6631 2194 2466 3993 1012 1000 2023 5456 16142 2007 1037 5418 2057 1005 2310 2464 2058 2070 2086 1997 4654 7913 23738 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2678 1011 6631 10439 14841 25509 6559 2003 2108 2109 2011 5499 2110 17671 2000 3659 10398 2000 1037 3920 4378 1010 2429 2000 1996 2813 2395 3485 1012 6793 7283 4207 6876 4760 2003 7299 2007 4409 1998 18113 2108 7700 2094 2083 4534 1012 2070 1997 1996 6876 1010 6866 1999 3522 3134 1010 2024 2056 2000 2031 2443 14841 25509 6559 19335 1011 4507 3896 2008 5460 3340 2030 8072 2006 2327 1997 1996 4180 1012 6876 5210 5999 2011 1996 3780 2031 2042 3718 1998 2484 6115 8534 1012 1996 6876 2020 4453 2011 1996 2591 1011 2865 4454 6631 2194 2466 3993 1012 1000 2023 5456 16142 2007 1037 5418 2057 1005 2310 2464 2058 2070 2086 1997 4654 7913 23738 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 178\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 178\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] facebook on friday removed what it called a global network of more than 900 accounts , pages , and groups from its platform and ins ##tagram that allegedly used dec ##eptive practices to push pro - trump narratives to about 55 million users . the network used fake accounts , artificial amp ##li ##fication , and , notably , profile photos of fake faces generated using artificial intelligence to spread polar ##izing , predominantly right - wing content around the web , including on twitter and youtube . it represents an alarm ##ing new development in the information wars , as it appears to be the first large - scale deployment of ai - generated images in a social network . in a report on the [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] facebook on friday removed what it called a global network of more than 900 accounts , pages , and groups from its platform and ins ##tagram that allegedly used dec ##eptive practices to push pro - trump narratives to about 55 million users . the network used fake accounts , artificial amp ##li ##fication , and , notably , profile photos of fake faces generated using artificial intelligence to spread polar ##izing , predominantly right - wing content around the web , including on twitter and youtube . it represents an alarm ##ing new development in the information wars , as it appears to be the first large - scale deployment of ai - generated images in a social network . in a report on the [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 9130 2006 5958 3718 2054 2009 2170 1037 3795 2897 1997 2062 2084 7706 6115 1010 5530 1010 1998 2967 2013 2049 4132 1998 16021 23091 2008 9382 2109 11703 22048 6078 2000 5245 4013 1011 8398 22143 2000 2055 4583 2454 5198 1012 1996 2897 2109 8275 6115 1010 7976 23713 3669 10803 1010 1998 1010 5546 1010 6337 7760 1997 8275 5344 7013 2478 7976 4454 2000 3659 11508 6026 1010 9197 2157 1011 3358 4180 2105 1996 4773 1010 2164 2006 10474 1998 7858 1012 2009 5836 2019 8598 2075 2047 2458 1999 1996 2592 5233 1010 2004 2009 3544 2000 2022 1996 2034 2312 1011 4094 10813 1997 9932 1011 7013 4871 1999 1037 2591 2897 1012 1999 1037 3189 2006 1996 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 9130 2006 5958 3718 2054 2009 2170 1037 3795 2897 1997 2062 2084 7706 6115 1010 5530 1010 1998 2967 2013 2049 4132 1998 16021 23091 2008 9382 2109 11703 22048 6078 2000 5245 4013 1011 8398 22143 2000 2055 4583 2454 5198 1012 1996 2897 2109 8275 6115 1010 7976 23713 3669 10803 1010 1998 1010 5546 1010 6337 7760 1997 8275 5344 7013 2478 7976 4454 2000 3659 11508 6026 1010 9197 2157 1011 3358 4180 2105 1996 4773 1010 2164 2006 10474 1998 7858 1012 2009 5836 2019 8598 2075 2047 2458 1999 1996 2592 5233 1010 2004 2009 3544 2000 2022 1996 2034 2312 1011 4094 10813 1997 9932 1011 7013 4871 1999 1037 2591 2897 1012 1999 1037 3189 2006 1996 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 2 (id = 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 2 (id = 2)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] when mit professor regina bar ##zi ##lay received her breast cancer diagnosis , she turned it into a science project . learning that the disease could have been detected earlier if doctors had recognized the signs on previous ma ##mm ##og ##ram ##s , bar ##zi ##lay , an expert in artificial intelligence , used a collection of 90 , 000 breast x - rays to create software for predicting a patient ’ s cancer risk . bar ##zi ##lay calculate ##s the software could have flag ##ged her own cancer two years before it was diagnosed by conventional means . “ the ai was able to detect smaller details than the human eye could pick up , ” she says . bar ##zi ##lay ’ [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] when mit professor regina bar ##zi ##lay received her breast cancer diagnosis , she turned it into a science project . learning that the disease could have been detected earlier if doctors had recognized the signs on previous ma ##mm ##og ##ram ##s , bar ##zi ##lay , an expert in artificial intelligence , used a collection of 90 , 000 breast x - rays to create software for predicting a patient ’ s cancer risk . bar ##zi ##lay calculate ##s the software could have flag ##ged her own cancer two years before it was diagnosed by conventional means . “ the ai was able to detect smaller details than the human eye could pick up , ” she says . bar ##zi ##lay ’ [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2043 10210 2934 12512 3347 5831 8485 2363 2014 7388 4456 11616 1010 2016 2357 2009 2046 1037 2671 2622 1012 4083 2008 1996 4295 2071 2031 2042 11156 3041 2065 7435 2018 3858 1996 5751 2006 3025 5003 7382 8649 6444 2015 1010 3347 5831 8485 1010 2019 6739 1999 7976 4454 1010 2109 1037 3074 1997 3938 1010 2199 7388 1060 1011 9938 2000 3443 4007 2005 29458 1037 5776 1521 1055 4456 3891 1012 3347 5831 8485 18422 2015 1996 4007 2071 2031 5210 5999 2014 2219 4456 2048 2086 2077 2009 2001 11441 2011 7511 2965 1012 1523 1996 9932 2001 2583 2000 11487 3760 4751 2084 1996 2529 3239 2071 4060 2039 1010 1524 2016 2758 1012 3347 5831 8485 1521 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2043 10210 2934 12512 3347 5831 8485 2363 2014 7388 4456 11616 1010 2016 2357 2009 2046 1037 2671 2622 1012 4083 2008 1996 4295 2071 2031 2042 11156 3041 2065 7435 2018 3858 1996 5751 2006 3025 5003 7382 8649 6444 2015 1010 3347 5831 8485 1010 2019 6739 1999 7976 4454 1010 2109 1037 3074 1997 3938 1010 2199 7388 1060 1011 9938 2000 3443 4007 2005 29458 1037 5776 1521 1055 4456 3891 1012 3347 5831 8485 18422 2015 1996 4007 2071 2031 5210 5999 2014 2219 4456 2048 2086 2077 2009 2001 11441 2011 7511 2965 1012 1523 1996 9932 2001 2583 2000 11487 3760 4751 2084 1996 2529 3239 2071 4060 2039 1010 1524 2016 2758 1012 3347 5831 8485 1521 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 2 (id = 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 2 (id = 2)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] it ' s beginning to look a lot like the end of the year in cyber ##se ##cu ##rity ! in an interview with the pentagon ' s artificial intelligence hon ##cho , we looked forward at how ai will intersect with warfare in the future — and the many un ##res ##olved questions that raises . and in an interview with ve ##ner ##ated author cliff st ##oll , we took a look back a historic moment in cyber ##se ##cu ##rity . we detailed how popular conference room video displays can be hacked , and how what ##sa ##pp group chat security still needs a little work . 5 ##g is coming , and while it ' ll be more secure than 4 ##g [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] it ' s beginning to look a lot like the end of the year in cyber ##se ##cu ##rity ! in an interview with the pentagon ' s artificial intelligence hon ##cho , we looked forward at how ai will intersect with warfare in the future — and the many un ##res ##olved questions that raises . and in an interview with ve ##ner ##ated author cliff st ##oll , we took a look back a historic moment in cyber ##se ##cu ##rity . we detailed how popular conference room video displays can be hacked , and how what ##sa ##pp group chat security still needs a little work . 5 ##g is coming , and while it ' ll be more secure than 4 ##g [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2009 1005 1055 2927 2000 2298 1037 2843 2066 1996 2203 1997 1996 2095 1999 16941 3366 10841 15780 999 1999 2019 4357 2007 1996 20864 1005 1055 7976 4454 10189 9905 1010 2057 2246 2830 2012 2129 9932 2097 29261 2007 8309 1999 1996 2925 1517 1998 1996 2116 4895 6072 16116 3980 2008 13275 1012 1998 1999 2019 4357 2007 2310 3678 4383 3166 7656 2358 14511 1010 2057 2165 1037 2298 2067 1037 3181 2617 1999 16941 3366 10841 15780 1012 2057 6851 2129 2759 3034 2282 2678 8834 2064 2022 28719 1010 1998 2129 2054 3736 9397 2177 11834 3036 2145 3791 1037 2210 2147 1012 1019 2290 2003 2746 1010 1998 2096 2009 1005 2222 2022 2062 5851 2084 1018 2290 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2009 1005 1055 2927 2000 2298 1037 2843 2066 1996 2203 1997 1996 2095 1999 16941 3366 10841 15780 999 1999 2019 4357 2007 1996 20864 1005 1055 7976 4454 10189 9905 1010 2057 2246 2830 2012 2129 9932 2097 29261 2007 8309 1999 1996 2925 1517 1998 1996 2116 4895 6072 16116 3980 2008 13275 1012 1998 1999 2019 4357 2007 2310 3678 4383 3166 7656 2358 14511 1010 2057 2165 1037 2298 2067 1037 3181 2617 1999 16941 3366 10841 15780 1012 2057 6851 2129 2759 3034 2282 2678 8834 2064 2022 28719 1010 1998 2129 2054 3736 9397 2177 11834 3036 2145 3791 1037 2210 2147 1012 1019 2290 2003 2746 1010 1998 2096 2009 1005 2222 2022 2062 5851 2084 1018 2290 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 2 (id = 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 2 (id = 2)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] artificial intelligence is a powerful tool , but it ’ s not a magic wand . applying the technology requires thought and dedication , especially with legacy industries like law and insurance , which are being taken on in this way by lu ##mina ##nce and om ##nius respectively . the companies ’ founders , emily fog ##es and sofie qui ##den ##us - wah ##lf ##ors ##s , spoke with great insight on this on stage at disrupt berlin . lu ##mina ##nce uses ai and natural language processing to help law firms process documents more quickly , not replacing the lawyer but providing additional intelligence and analysis of what may be hundreds or thousands of pages and saving time and money . om ##nius [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] artificial intelligence is a powerful tool , but it ’ s not a magic wand . applying the technology requires thought and dedication , especially with legacy industries like law and insurance , which are being taken on in this way by lu ##mina ##nce and om ##nius respectively . the companies ’ founders , emily fog ##es and sofie qui ##den ##us - wah ##lf ##ors ##s , spoke with great insight on this on stage at disrupt berlin . lu ##mina ##nce uses ai and natural language processing to help law firms process documents more quickly , not replacing the lawyer but providing additional intelligence and analysis of what may be hundreds or thousands of pages and saving time and money . om ##nius [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 7976 4454 2003 1037 3928 6994 1010 2021 2009 1521 1055 2025 1037 3894 23967 1012 11243 1996 2974 5942 2245 1998 12276 1010 2926 2007 8027 6088 2066 2375 1998 5427 1010 2029 2024 2108 2579 2006 1999 2023 2126 2011 11320 22311 5897 1998 18168 20447 4414 1012 1996 3316 1521 8759 1010 6253 9666 2229 1998 26359 21864 4181 2271 1011 22894 10270 5668 2015 1010 3764 2007 2307 12369 2006 2023 2006 2754 2012 23217 4068 1012 11320 22311 5897 3594 9932 1998 3019 2653 6364 2000 2393 2375 9786 2832 5491 2062 2855 1010 2025 6419 1996 5160 2021 4346 3176 4454 1998 4106 1997 2054 2089 2022 5606 2030 5190 1997 5530 1998 7494 2051 1998 2769 1012 18168 20447 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 7976 4454 2003 1037 3928 6994 1010 2021 2009 1521 1055 2025 1037 3894 23967 1012 11243 1996 2974 5942 2245 1998 12276 1010 2926 2007 8027 6088 2066 2375 1998 5427 1010 2029 2024 2108 2579 2006 1999 2023 2126 2011 11320 22311 5897 1998 18168 20447 4414 1012 1996 3316 1521 8759 1010 6253 9666 2229 1998 26359 21864 4181 2271 1011 22894 10270 5668 2015 1010 3764 2007 2307 12369 2006 2023 2006 2754 2012 23217 4068 1012 11320 22311 5897 3594 9932 1998 3019 2653 6364 2000 2393 2375 9786 2832 5491 2062 2855 1010 2025 6419 1996 5160 2021 4346 3176 4454 1998 4106 1997 2054 2089 2022 5606 2030 5190 1997 5530 1998 7494 2051 1998 2769 1012 18168 20447 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 2 (id = 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 2 (id = 2)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] one of the ways you can tell that a new technology is still at the h ##ype stage is that no one talks about its problems . when an innovation is rising towards the ‘ peak of inflated expectations , ’ people talk about it in hushed tones as something almost miraculous — and they hardly ever discuss the obstacles to adoption or the challenges that the new technology will bring . last year , it seemed that people were app ##ending the hall ##owed initials a . i . to every con ##ce ##iva ##ble offering , much like they did with io ##t and cloud in past years . the good news is that people are starting to think much more critically about ai [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] one of the ways you can tell that a new technology is still at the h ##ype stage is that no one talks about its problems . when an innovation is rising towards the ‘ peak of inflated expectations , ’ people talk about it in hushed tones as something almost miraculous — and they hardly ever discuss the obstacles to adoption or the challenges that the new technology will bring . last year , it seemed that people were app ##ending the hall ##owed initials a . i . to every con ##ce ##iva ##ble offering , much like they did with io ##t and cloud in past years . the good news is that people are starting to think much more critically about ai [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2028 1997 1996 3971 2017 2064 2425 2008 1037 2047 2974 2003 2145 2012 1996 1044 18863 2754 2003 2008 2053 2028 7566 2055 2049 3471 1012 2043 2019 8144 2003 4803 2875 1996 1520 4672 1997 29561 10908 1010 1521 2111 2831 2055 2009 1999 24033 12623 2004 2242 2471 26106 1517 1998 2027 6684 2412 6848 1996 15314 2000 9886 2030 1996 7860 2008 1996 2047 2974 2097 3288 1012 2197 2095 1010 2009 2790 2008 2111 2020 10439 18537 1996 2534 15096 20381 1037 1012 1045 1012 2000 2296 9530 3401 11444 3468 5378 1010 2172 2066 2027 2106 2007 22834 2102 1998 6112 1999 2627 2086 1012 1996 2204 2739 2003 2008 2111 2024 3225 2000 2228 2172 2062 11321 2055 9932 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2028 1997 1996 3971 2017 2064 2425 2008 1037 2047 2974 2003 2145 2012 1996 1044 18863 2754 2003 2008 2053 2028 7566 2055 2049 3471 1012 2043 2019 8144 2003 4803 2875 1996 1520 4672 1997 29561 10908 1010 1521 2111 2831 2055 2009 1999 24033 12623 2004 2242 2471 26106 1517 1998 2027 6684 2412 6848 1996 15314 2000 9886 2030 1996 7860 2008 1996 2047 2974 2097 3288 1012 2197 2095 1010 2009 2790 2008 2111 2020 10439 18537 1996 2534 15096 20381 1037 1012 1045 1012 2000 2296 9530 3401 11444 3468 5378 1010 2172 2066 2027 2106 2007 22834 2102 1998 6112 1999 2627 2086 1012 1996 2204 2739 2003 2008 2111 2024 3225 2000 2228 2172 2062 11321 2055 9932 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TMPqfRgSBKqK"
      },
      "source": [
        "\n",
        "**Creating a model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YhRUNMxVdkO",
        "colab_type": "text"
      },
      "source": [
        "create_model does just this below. First, it loads the BERT tf hub module again (this time to extract the computation graph). Next, it creates a single new layer that will be trained to adapt BERT to our sentiment task (i.e. classifying whether a movie review is positive or negative)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D3JTeWCkBPrJ",
        "colab": {}
      },
      "source": [
        "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
        "                 num_labels):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "\n",
        "  bert_module = hub.Module(\n",
        "      BERT_MODEL_HUB,\n",
        "      trainable=True)\n",
        "  bert_inputs = dict(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids)\n",
        "  bert_outputs = bert_module(\n",
        "      inputs=bert_inputs,\n",
        "      signature=\"tokens\",\n",
        "      as_dict=True)\n",
        "\n",
        "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
        "  # Use \"sequence_outputs\" for token-level output.\n",
        "  output_layer = bert_outputs[\"pooled_output\"]\n",
        "\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  # Create our own layer to tune for politeness data.\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "\n",
        "    # Dropout helps prevent overfitting\n",
        "    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "    # Convert labels into one-hot encoding\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
        "    if is_predicting:\n",
        "      return (predicted_labels, log_probs)\n",
        "\n",
        "    # If we're train/eval, compute loss between predicted and actual label\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "    return (loss, predicted_labels, log_probs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjoXW73hVdkT",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Next we'll wrap our model function in a model_fn_builder function that adapts our model to work for training, evaluation, and prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1O1Kzd50B1Le",
        "colab": {}
      },
      "source": [
        "# model_fn_builder actually creates our model function\n",
        "# using the passed parameters for num_labels, learning_rate, etc.\n",
        "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
        "                     num_warmup_steps):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "\n",
        "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
        "    \n",
        "    # TRAIN and EVAL\n",
        "    if not is_predicting:\n",
        "\n",
        "      (loss, predicted_labels, log_probs) = create_model(\n",
        "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      train_op = bert.optimization.create_optimizer(\n",
        "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
        "\n",
        "      # Calculate evaluation metrics. \n",
        "      def metric_fn(label_ids, predicted_labels):\n",
        "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
        "        f1_score = tf.contrib.metrics.f1_score(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        auc = tf.metrics.auc(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        recall = tf.metrics.recall(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        precision = tf.metrics.precision(\n",
        "            label_ids,\n",
        "            predicted_labels) \n",
        "        true_pos = tf.metrics.true_positives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        true_neg = tf.metrics.true_negatives(\n",
        "            label_ids,\n",
        "            predicted_labels)   \n",
        "        false_pos = tf.metrics.false_positives(\n",
        "            label_ids,\n",
        "            predicted_labels)  \n",
        "        false_neg = tf.metrics.false_negatives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            \"f1_score\": f1_score,\n",
        "            \"auc\": auc,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"true_positives\": true_pos,\n",
        "            \"true_negatives\": true_neg,\n",
        "            \"false_positives\": false_pos,\n",
        "            \"false_negatives\": false_neg\n",
        "        }\n",
        "\n",
        "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
        "\n",
        "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        return tf.estimator.EstimatorSpec(mode=mode,\n",
        "          loss=loss,\n",
        "          train_op=train_op)\n",
        "      else:\n",
        "          return tf.estimator.EstimatorSpec(mode=mode,\n",
        "            loss=loss,\n",
        "            eval_metric_ops=eval_metrics)\n",
        "    else:\n",
        "      (predicted_labels, log_probs) = create_model(\n",
        "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      predictions = {\n",
        "          'probabilities': log_probs,\n",
        "          'labels': predicted_labels\n",
        "      }\n",
        "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "\n",
        "  # Return the actual model function in the closure\n",
        "  return model_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0gwp4ALVfP0T",
        "colab": {}
      },
      "source": [
        "# Compute train and warmup steps from batch size\n",
        "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 6.0\n",
        "# Warmup is a period of time where hte learning rate \n",
        "# is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 500\n",
        "SAVE_SUMMARY_STEPS = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sGJnphaVCLgw",
        "colab": {}
      },
      "source": [
        "# Compute # train and warmup steps from batch size\n",
        "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F1XX-mUtCh7J",
        "colab": {}
      },
      "source": [
        "# Specify outpit directory and number of checkpoint steps to save\n",
        "run_config = tf.estimator.RunConfig(\n",
        "    model_dir=OUTPUT_DIR,\n",
        "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IERCj_FoCm3S",
        "outputId": "f7a1da5c-2856-4dfc-e1ae-b3a772529926",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "model_fn = model_fn_builder(\n",
        "  num_labels=len(label_list),\n",
        "  learning_rate=LEARNING_RATE,\n",
        "  num_train_steps=num_train_steps,\n",
        "  num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "estimator = tf.estimator.Estimator(\n",
        "  model_fn=model_fn,\n",
        "  config=run_config,\n",
        "  params={\"batch_size\": BATCH_SIZE})"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'output_dir', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f83915c1978>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'output_dir', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f83915c1978>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5eVDL6sVdky",
        "colab_type": "text"
      },
      "source": [
        "Next we create an input builder function that takes our training feature set (train_features) and produces a generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bDSktUNcCv9e",
        "colab": {}
      },
      "source": [
        "# Create an input function for training. drop_remainder = True for using TPUs.\n",
        "train_input_fn = bert.run_classifier.input_fn_builder(\n",
        "    features=train_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=True,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00orUO8_Vdk3",
        "colab_type": "text"
      },
      "source": [
        "Now we train our model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TSEEiF51DEK0",
        "outputId": "7f36ddc7-0540-4cf4-b72b-a145e861bcb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "print(f'Beginning Training!')\n",
        "current_time = datetime.now()\n",
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "print(\"Training took time \", datetime.now() - current_time)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Beginning Training!\n",
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "/tensorflow-1.15.0/python3.6/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning:\n",
            "\n",
            "Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 0 into output_dir/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 0 into output_dir/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.9344081, step = 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.9344081, step = 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 59 into output_dir/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 59 into output_dir/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Loss for final step: 0.10846711.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Loss for final step: 0.10846711.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training took time  0:42:13.575810\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OKefiQ0hQ246",
        "colab": {}
      },
      "source": [
        "test_input_fn = run_classifier.input_fn_builder(\n",
        "    features=test_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=False,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6GyXyDsD6hM2"
      },
      "source": [
        "Now let's write code to make predictions on new sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cSyyaRpW6Uma",
        "colab": {}
      },
      "source": [
        "def getPrediction(in_sentences):\n",
        "  labels = [0,1,2]\n",
        "  input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n",
        "  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
        "  predictions = estimator.predict(predict_input_fn)\n",
        "  return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1cQSpVeVdlG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_sentences = test['Article']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i_kEIor36l1V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "68a91419-27ae-4ee4-ceb0-02e3280987b8"
      },
      "source": [
        "predictions = getPrediction(pred_sentences)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 178\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 178\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] facebook on friday removed what it called a global network of more than 900 accounts , pages , and groups from its platform and ins ##tagram that allegedly used dec ##eptive practices to push pro - trump narratives to about 55 million users . the network used fake accounts , artificial amp ##li ##fication , and , notably , profile photos of fake faces generated using artificial intelligence to spread polar ##izing , predominantly right - wing content around the web , including on twitter and youtube . it represents an alarm ##ing new development in the information wars , as it appears to be the first large - scale deployment of ai - generated images in a social network . in a report on the [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] facebook on friday removed what it called a global network of more than 900 accounts , pages , and groups from its platform and ins ##tagram that allegedly used dec ##eptive practices to push pro - trump narratives to about 55 million users . the network used fake accounts , artificial amp ##li ##fication , and , notably , profile photos of fake faces generated using artificial intelligence to spread polar ##izing , predominantly right - wing content around the web , including on twitter and youtube . it represents an alarm ##ing new development in the information wars , as it appears to be the first large - scale deployment of ai - generated images in a social network . in a report on the [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 9130 2006 5958 3718 2054 2009 2170 1037 3795 2897 1997 2062 2084 7706 6115 1010 5530 1010 1998 2967 2013 2049 4132 1998 16021 23091 2008 9382 2109 11703 22048 6078 2000 5245 4013 1011 8398 22143 2000 2055 4583 2454 5198 1012 1996 2897 2109 8275 6115 1010 7976 23713 3669 10803 1010 1998 1010 5546 1010 6337 7760 1997 8275 5344 7013 2478 7976 4454 2000 3659 11508 6026 1010 9197 2157 1011 3358 4180 2105 1996 4773 1010 2164 2006 10474 1998 7858 1012 2009 5836 2019 8598 2075 2047 2458 1999 1996 2592 5233 1010 2004 2009 3544 2000 2022 1996 2034 2312 1011 4094 10813 1997 9932 1011 7013 4871 1999 1037 2591 2897 1012 1999 1037 3189 2006 1996 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 9130 2006 5958 3718 2054 2009 2170 1037 3795 2897 1997 2062 2084 7706 6115 1010 5530 1010 1998 2967 2013 2049 4132 1998 16021 23091 2008 9382 2109 11703 22048 6078 2000 5245 4013 1011 8398 22143 2000 2055 4583 2454 5198 1012 1996 2897 2109 8275 6115 1010 7976 23713 3669 10803 1010 1998 1010 5546 1010 6337 7760 1997 8275 5344 7013 2478 7976 4454 2000 3659 11508 6026 1010 9197 2157 1011 3358 4180 2105 1996 4773 1010 2164 2006 10474 1998 7858 1012 2009 5836 2019 8598 2075 2047 2458 1999 1996 2592 5233 1010 2004 2009 3544 2000 2022 1996 2034 2312 1011 4094 10813 1997 9932 1011 7013 4871 1999 1037 2591 2897 1012 1999 1037 3189 2006 1996 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] when mit professor regina bar ##zi ##lay received her breast cancer diagnosis , she turned it into a science project . learning that the disease could have been detected earlier if doctors had recognized the signs on previous ma ##mm ##og ##ram ##s , bar ##zi ##lay , an expert in artificial intelligence , used a collection of 90 , 000 breast x - rays to create software for predicting a patient ’ s cancer risk . bar ##zi ##lay calculate ##s the software could have flag ##ged her own cancer two years before it was diagnosed by conventional means . “ the ai was able to detect smaller details than the human eye could pick up , ” she says . bar ##zi ##lay ’ [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] when mit professor regina bar ##zi ##lay received her breast cancer diagnosis , she turned it into a science project . learning that the disease could have been detected earlier if doctors had recognized the signs on previous ma ##mm ##og ##ram ##s , bar ##zi ##lay , an expert in artificial intelligence , used a collection of 90 , 000 breast x - rays to create software for predicting a patient ’ s cancer risk . bar ##zi ##lay calculate ##s the software could have flag ##ged her own cancer two years before it was diagnosed by conventional means . “ the ai was able to detect smaller details than the human eye could pick up , ” she says . bar ##zi ##lay ’ [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2043 10210 2934 12512 3347 5831 8485 2363 2014 7388 4456 11616 1010 2016 2357 2009 2046 1037 2671 2622 1012 4083 2008 1996 4295 2071 2031 2042 11156 3041 2065 7435 2018 3858 1996 5751 2006 3025 5003 7382 8649 6444 2015 1010 3347 5831 8485 1010 2019 6739 1999 7976 4454 1010 2109 1037 3074 1997 3938 1010 2199 7388 1060 1011 9938 2000 3443 4007 2005 29458 1037 5776 1521 1055 4456 3891 1012 3347 5831 8485 18422 2015 1996 4007 2071 2031 5210 5999 2014 2219 4456 2048 2086 2077 2009 2001 11441 2011 7511 2965 1012 1523 1996 9932 2001 2583 2000 11487 3760 4751 2084 1996 2529 3239 2071 4060 2039 1010 1524 2016 2758 1012 3347 5831 8485 1521 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2043 10210 2934 12512 3347 5831 8485 2363 2014 7388 4456 11616 1010 2016 2357 2009 2046 1037 2671 2622 1012 4083 2008 1996 4295 2071 2031 2042 11156 3041 2065 7435 2018 3858 1996 5751 2006 3025 5003 7382 8649 6444 2015 1010 3347 5831 8485 1010 2019 6739 1999 7976 4454 1010 2109 1037 3074 1997 3938 1010 2199 7388 1060 1011 9938 2000 3443 4007 2005 29458 1037 5776 1521 1055 4456 3891 1012 3347 5831 8485 18422 2015 1996 4007 2071 2031 5210 5999 2014 2219 4456 2048 2086 2077 2009 2001 11441 2011 7511 2965 1012 1523 1996 9932 2001 2583 2000 11487 3760 4751 2084 1996 2529 3239 2071 4060 2039 1010 1524 2016 2758 1012 3347 5831 8485 1521 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] it ' s beginning to look a lot like the end of the year in cyber ##se ##cu ##rity ! in an interview with the pentagon ' s artificial intelligence hon ##cho , we looked forward at how ai will intersect with warfare in the future — and the many un ##res ##olved questions that raises . and in an interview with ve ##ner ##ated author cliff st ##oll , we took a look back a historic moment in cyber ##se ##cu ##rity . we detailed how popular conference room video displays can be hacked , and how what ##sa ##pp group chat security still needs a little work . 5 ##g is coming , and while it ' ll be more secure than 4 ##g [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] it ' s beginning to look a lot like the end of the year in cyber ##se ##cu ##rity ! in an interview with the pentagon ' s artificial intelligence hon ##cho , we looked forward at how ai will intersect with warfare in the future — and the many un ##res ##olved questions that raises . and in an interview with ve ##ner ##ated author cliff st ##oll , we took a look back a historic moment in cyber ##se ##cu ##rity . we detailed how popular conference room video displays can be hacked , and how what ##sa ##pp group chat security still needs a little work . 5 ##g is coming , and while it ' ll be more secure than 4 ##g [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2009 1005 1055 2927 2000 2298 1037 2843 2066 1996 2203 1997 1996 2095 1999 16941 3366 10841 15780 999 1999 2019 4357 2007 1996 20864 1005 1055 7976 4454 10189 9905 1010 2057 2246 2830 2012 2129 9932 2097 29261 2007 8309 1999 1996 2925 1517 1998 1996 2116 4895 6072 16116 3980 2008 13275 1012 1998 1999 2019 4357 2007 2310 3678 4383 3166 7656 2358 14511 1010 2057 2165 1037 2298 2067 1037 3181 2617 1999 16941 3366 10841 15780 1012 2057 6851 2129 2759 3034 2282 2678 8834 2064 2022 28719 1010 1998 2129 2054 3736 9397 2177 11834 3036 2145 3791 1037 2210 2147 1012 1019 2290 2003 2746 1010 1998 2096 2009 1005 2222 2022 2062 5851 2084 1018 2290 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2009 1005 1055 2927 2000 2298 1037 2843 2066 1996 2203 1997 1996 2095 1999 16941 3366 10841 15780 999 1999 2019 4357 2007 1996 20864 1005 1055 7976 4454 10189 9905 1010 2057 2246 2830 2012 2129 9932 2097 29261 2007 8309 1999 1996 2925 1517 1998 1996 2116 4895 6072 16116 3980 2008 13275 1012 1998 1999 2019 4357 2007 2310 3678 4383 3166 7656 2358 14511 1010 2057 2165 1037 2298 2067 1037 3181 2617 1999 16941 3366 10841 15780 1012 2057 6851 2129 2759 3034 2282 2678 8834 2064 2022 28719 1010 1998 2129 2054 3736 9397 2177 11834 3036 2145 3791 1037 2210 2147 1012 1019 2290 2003 2746 1010 1998 2096 2009 1005 2222 2022 2062 5851 2084 1018 2290 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] artificial intelligence is a powerful tool , but it ’ s not a magic wand . applying the technology requires thought and dedication , especially with legacy industries like law and insurance , which are being taken on in this way by lu ##mina ##nce and om ##nius respectively . the companies ’ founders , emily fog ##es and sofie qui ##den ##us - wah ##lf ##ors ##s , spoke with great insight on this on stage at disrupt berlin . lu ##mina ##nce uses ai and natural language processing to help law firms process documents more quickly , not replacing the lawyer but providing additional intelligence and analysis of what may be hundreds or thousands of pages and saving time and money . om ##nius [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] artificial intelligence is a powerful tool , but it ’ s not a magic wand . applying the technology requires thought and dedication , especially with legacy industries like law and insurance , which are being taken on in this way by lu ##mina ##nce and om ##nius respectively . the companies ’ founders , emily fog ##es and sofie qui ##den ##us - wah ##lf ##ors ##s , spoke with great insight on this on stage at disrupt berlin . lu ##mina ##nce uses ai and natural language processing to help law firms process documents more quickly , not replacing the lawyer but providing additional intelligence and analysis of what may be hundreds or thousands of pages and saving time and money . om ##nius [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 7976 4454 2003 1037 3928 6994 1010 2021 2009 1521 1055 2025 1037 3894 23967 1012 11243 1996 2974 5942 2245 1998 12276 1010 2926 2007 8027 6088 2066 2375 1998 5427 1010 2029 2024 2108 2579 2006 1999 2023 2126 2011 11320 22311 5897 1998 18168 20447 4414 1012 1996 3316 1521 8759 1010 6253 9666 2229 1998 26359 21864 4181 2271 1011 22894 10270 5668 2015 1010 3764 2007 2307 12369 2006 2023 2006 2754 2012 23217 4068 1012 11320 22311 5897 3594 9932 1998 3019 2653 6364 2000 2393 2375 9786 2832 5491 2062 2855 1010 2025 6419 1996 5160 2021 4346 3176 4454 1998 4106 1997 2054 2089 2022 5606 2030 5190 1997 5530 1998 7494 2051 1998 2769 1012 18168 20447 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 7976 4454 2003 1037 3928 6994 1010 2021 2009 1521 1055 2025 1037 3894 23967 1012 11243 1996 2974 5942 2245 1998 12276 1010 2926 2007 8027 6088 2066 2375 1998 5427 1010 2029 2024 2108 2579 2006 1999 2023 2126 2011 11320 22311 5897 1998 18168 20447 4414 1012 1996 3316 1521 8759 1010 6253 9666 2229 1998 26359 21864 4181 2271 1011 22894 10270 5668 2015 1010 3764 2007 2307 12369 2006 2023 2006 2754 2012 23217 4068 1012 11320 22311 5897 3594 9932 1998 3019 2653 6364 2000 2393 2375 9786 2832 5491 2062 2855 1010 2025 6419 1996 5160 2021 4346 3176 4454 1998 4106 1997 2054 2089 2022 5606 2030 5190 1997 5530 1998 7494 2051 1998 2769 1012 18168 20447 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] one of the ways you can tell that a new technology is still at the h ##ype stage is that no one talks about its problems . when an innovation is rising towards the ‘ peak of inflated expectations , ’ people talk about it in hushed tones as something almost miraculous — and they hardly ever discuss the obstacles to adoption or the challenges that the new technology will bring . last year , it seemed that people were app ##ending the hall ##owed initials a . i . to every con ##ce ##iva ##ble offering , much like they did with io ##t and cloud in past years . the good news is that people are starting to think much more critically about ai [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] one of the ways you can tell that a new technology is still at the h ##ype stage is that no one talks about its problems . when an innovation is rising towards the ‘ peak of inflated expectations , ’ people talk about it in hushed tones as something almost miraculous — and they hardly ever discuss the obstacles to adoption or the challenges that the new technology will bring . last year , it seemed that people were app ##ending the hall ##owed initials a . i . to every con ##ce ##iva ##ble offering , much like they did with io ##t and cloud in past years . the good news is that people are starting to think much more critically about ai [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2028 1997 1996 3971 2017 2064 2425 2008 1037 2047 2974 2003 2145 2012 1996 1044 18863 2754 2003 2008 2053 2028 7566 2055 2049 3471 1012 2043 2019 8144 2003 4803 2875 1996 1520 4672 1997 29561 10908 1010 1521 2111 2831 2055 2009 1999 24033 12623 2004 2242 2471 26106 1517 1998 2027 6684 2412 6848 1996 15314 2000 9886 2030 1996 7860 2008 1996 2047 2974 2097 3288 1012 2197 2095 1010 2009 2790 2008 2111 2020 10439 18537 1996 2534 15096 20381 1037 1012 1045 1012 2000 2296 9530 3401 11444 3468 5378 1010 2172 2066 2027 2106 2007 22834 2102 1998 6112 1999 2627 2086 1012 1996 2204 2739 2003 2008 2111 2024 3225 2000 2228 2172 2062 11321 2055 9932 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2028 1997 1996 3971 2017 2064 2425 2008 1037 2047 2974 2003 2145 2012 1996 1044 18863 2754 2003 2008 2053 2028 7566 2055 2049 3471 1012 2043 2019 8144 2003 4803 2875 1996 1520 4672 1997 29561 10908 1010 1521 2111 2831 2055 2009 1999 24033 12623 2004 2242 2471 26106 1517 1998 2027 6684 2412 6848 1996 15314 2000 9886 2030 1996 7860 2008 1996 2047 2974 2097 3288 1012 2197 2095 1010 2009 2790 2008 2111 2020 10439 18537 1996 2534 15096 20381 1037 1012 1045 1012 2000 2296 9530 3401 11444 3468 5378 1010 2172 2066 2027 2106 2007 22834 2102 1998 6112 1999 2627 2086 1012 1996 2204 2739 2003 2008 2111 2024 3225 2000 2228 2172 2062 11321 2055 9932 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from output_dir/model.ckpt-59\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from output_dir/model.ckpt-59\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kJudi4xR6qOi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dae206cd-dd78-4c7c-cf20-2ebe410ec163"
      },
      "source": [
        "predictions"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Facebook on Friday removed what it called a global network of more than 900 accounts, pages, and groups from its platform and Instagram that allegedly used deceptive practices to push pro-Trump narratives to about 55 million users. The network used fake accounts, artificial amplification, and, notably, profile photos of fake faces generated using artificial intelligence to spread polarizing, predominantly right-wing content around the web, including on Twitter and YouTube.\\nIt represents an alarming new development in the information wars, as it appears to be the first large-scale deployment of AI-generated images in a social network. In a report on the influence operation, researchers from disinformation groups Graphika and DFRLab noted that this was the first time they had seen the technology used to support an inauthentic social media campaign.\\nProfile pictures for Facebook accounts “Mary Keen” and “Jacobs Guillermo,” admins on groups associated with The BL highlighted by Graphika.\\nCOURTESY OF GRAPHIKA\\nThe images were used to enhance the authenticity of some of the 610 Facebook accounts, 156 groups, 89 pages, and 72 Instagram accounts associated with “The BL,” a digital news outlet that described itself as a “pure mountain spring, moistening the heart of every reader” on Facebook before it was removed. Facebook’s investigation connected The BL to The Epoch Times, a conservative media organization with ties to Chinese spiritual group Falun Gong and a history of aggressive support for Donald Trump.\\nKeep Reading\\nThe latest on artificial intelligence, from machine learning to computer vision and more\\nOstensibly a US-based media organization, The BL network’s pages were operated by users in Vietnam and the US, who Facebook says made widespread use of fake accounts to evade detection and funnel traffic to its own websites. A report by Graphika and DFRLab researchers, found that the majority of The BL network’s fake accounts were used to maintain a cluster of over 80 groups and pages promoting President Trump. The accounts acting as administrators for these pages—which boasted names such as “America Needs President Trump,” “Trump for America’s President,” and “WE STAND WITH TRUMP & PENCE!”—were predominantly fake and created in Vietnam.\\n“Fake accounts served as the administrators of Facebook groups, increased the membership numbers of those groups, liked posts on the Pages, and posted large quantities of content from TheBL-related assets,” the report notes. “This structure constituted a large-scale artificial amplification factory whose only observable function was to boost content from TheBL and, to a lesser extent, from the Epoch Times.”\\nOn the left, admins for “Patriots for President Trump,” nine out of 15 of which used AI-generated faces, researchers said. On the right, admins for “President Trump KAG 2020,\" eight out of 16 of which are fake faces, according to a report.\\nCOURTESY OF GRAPHIKA\\nADVERTISEMENT\\nMany of these accounts used profile pictures that researchers said appeared to be created using neural networks trained on images of real human faces to create photos of people that did not actually exist. Researchers from Graphika and DFRLab found 18 images of faces in profile photos used by The BL network that they said had likely been generated using generative adversarial networks.\\nWhile the technology is capable of creating passable fake faces, it has a few obvious tells that tipped the researchers off. It works largely by mimicking visual patterns, but doesn’t understand human anatomy, and struggles with backgrounds and symmetrical features, like glasses or earrings.\\nA profile photo for the account “Alfonzo Macias” with AI-generated inconsistencies around his glasses that Graphika said suggested the work of artificial intelligence.\\nCOURTESY OF GRAPHIKA\\nThe above image, from the Graphika report, was taken from the profile photo for “Alfonzo Macias,” an admin for one of the groups in The BL network. “Note the asymmetry in the glasses and also the poorly defined background,” the report says. “The authors checked this image with experts at the University Federico II of Naples, who assessed that this image was GAN-generated with 99.9 percent certainty.”',\n",
              "  array([-0.5121273, -2.8696733, -1.0669303], dtype=float32),\n",
              "  0),\n",
              " ('When MIT professor Regina Barzilay received her breast cancer diagnosis, she turned it into a science project. Learning that the disease could have been detected earlier if doctors had recognized the signs on previous mammograms, Barzilay, an expert in artificial intelligence, used a collection of 90,000 breast x-rays to create software for predicting a patient’s cancer risk.\\nBarzilay calculates the software could have flagged her own cancer two years before it was diagnosed by conventional means. “The AI was able to detect smaller details than the human eye could pick up,” she says.\\nBarzilay’s work is featured in the ninth installment of the Sleepwalkers podcast, which tours the effects of artificial intelligence revolution. The episode explores how AI is transforming health care—and perhaps also what it means to be mortal.\\nKeep Reading\\nThe latest on artificial intelligence, from machine learning to computer vision and more\\nSelf-driving-car projects might be more glamorous, but health care has also benefited from a rush to apply recent developments in AI. In one case, researchers at Stanford demonstrated software that could diagnose skin cancer at an accuracy rivaling dermatologists.\\n“People die of cancer a lot,” says Sebastian Thrun, who worked on the skin cancer project and is now CEO of flying car company Kitty Hawk. (Thrun also started Google’s self-driving car program.) “I believe many of those deaths are actually preventable using artificial\\nIntelligence.”\\nOncologist and writer Siddhartha Mukherjee cheers the potential of medical AI. But he also warns that we should be wary of some of the less desirable aspects of the technology. One is increased collection of data on patients and their conditions. All that information could be tempting to health insurers or government agencies, which might not always have a patient’s interest as their first priority.\\nAI systems that can predict a person’s health far into the future—even before birth—raise even trickier questions. Researchers have shown that algorithms can process genomic data to predict individual characteristics, such as a person’s height or risk of certain diseases. Indicators of intelligence or other traits could be next, tempting parents, armed with techniques like IVF and gene editing, to shape—and not just learn—their offspring’s destiny.\\nMukherjee says such powerful knowledge will require physicians to think carefully about their role in patients’ lives and in society. “In medicine, because we’re intervening on bodies, we’re intervening on culture,” he says.',\n",
              "  array([-4.0325685 , -4.2178593 , -0.03299731], dtype=float32),\n",
              "  2),\n",
              " ('It\\'s beginning to look a lot like the end of the year in cybersecurity! In an interview with the Pentagon\\'s artificial intelligence honcho, we looked forward at how AI will intersect with warfare in the future—and the many unresolved questions that raises. And in an interview with venerated author Cliff Stoll, we took a look back a historic moment in cybersecurity.\\nWe detailed how popular conference room video displays can be hacked, and how WhatsApp group chat security still needs a little work.\\n5G is coming, and while it\\'ll be more secure than 4G it\\'s still not perfect. Chrome will check your passwords to make sure they\\'re not already in some data breach somewhere. And set aside some time to read this tale of an Army veteran who thought he found romance on a dating site—but ran into a terrifying scam instead.\\nAnd there\\'s still more. Every Saturday we round up the security and privacy stories that we didn’t break or report on in-depth but think you should know about nonetheless. Click on the headlines to read them, and stay safe out there.\\nFacebook Finally Stops Using Your Two-Factor Phone Number for Friend Suggestions\\nFor far too long, if you provided Facebook with a phone number for two-factor authentication, the company turned around and used it to serve you targeted ads and present you with people you might know. It\\'s the sort of duplicitous, privacy-disregarding behavior that earned the company a $5 billion FTC fine. But while Zuck and company quit using 2FA numbers to feed its ad machine in the summer, it took until this week to announce that it would do the same with friend-finding. Even now, though, the change won\\'t roll out globally until next year. More annoying still, according to Reuters, to uncouple your 2FA number from Facebook\\'s friend-connections, you need to delete and then enter it once more. Or maybe go ahead and delete your Facebook account altogether, just a thought!\\nWawa Stores Were Infected With Point-of-Sale Malware\\nBad news, fans of hoagies and slightly stale soft pretzels in and around Pennsylvania: Convenience store chain Wawa on Friday revealed that its point-of-sale servers had been infected with malware that stole credit card information, potentially affecting all 700 of its stores across five states. The malware had been present on Wawa\\'s systems as early as March, but was only discovered on December 10. The company assures customers that debit card PINs, credit card CVV2 codes and ATMs at the stores weren\\'t affected, but it\\'s nonetheless offering credit monitoring to affected customers.\\nHow a Single Hacker\\'s DDoS Attack Took Liberia Offline\\nBloomberg this week told the in-depth story of a young Israeli hacker named Daniel Kaye, also known by his handle Spdrman, who launched a record-setting cyberattack that took down the largest telecommunications network in Libera. In the fall of 2016, Kaye\\'s distributed denial of service attacks launched gargantuan waves of junk traffic from his botnet of half a million hijacked internet-connected security cameras, one of several botnets known as Mirai, at the Liberian telecom network Lonestar. Kaye had been hired by the CEO of one of Lonestar\\'s competitors, Avishai Marziano. The attack knocked 1.5 million Liberians off the internet, about a third of the country\\'s population, including its largest hospital and infectious disease specialists dealing with the aftermath of the Ebola outbreak that had hit the country the year before. At other points, Kaye allegedly rented out parts of its botnet to other hackers who used it for attacks on banks and gaming rivals. Kaye was arrested by British police in February of 2017 while trying to board a flight to Cyprus, and was later given a 32-month prison sentence.\\nThe New York Times Used App Location Data to Track the President\\nThe New York Times Opinions desk this week revealed that it had obtained a massive cache of location data that included 50 billion\"pings\"representing the detailed locations of 12 million Americans, as captured by their smartphones as they move about their daily lives. The Times declined to reveal the source of that data, saying only that it was a firm that collects location data on Americans, and that such data is captured by programs as seemingly harmless as weather apps and coupon savers. The Times then went on to demonstrate the application of that data for targeted surveillance, showing that it could track the detailed whereabouts of a Secret Service agent accompanying President Trump, following him to Mar-a-lago and a round of golf the president played with Japan\\'s Prime Minister Shinzo Abe. To drive the point home, the Times also published obscured location patterns of other officials including a Pentagon staffer and a senator\\'s advisor.\\nA Russian Spy Ship Is Getting Reckless off the US Coast\\nRussian spy ships hanging out near the US is surprisingly common. But US officials told CNN this week that the Viktor Leonov was acting in an \"unsafe manner\" and engaging in \"erratic maneuvers,\" which are generally not words you want to hear in association with a nearby spy ship from a hostile nation.',\n",
              "  array([-1.710292  , -1.7299899 , -0.44332144], dtype=float32),\n",
              "  2),\n",
              " ('Artificial intelligence is a powerful tool, but it’s not a magic wand. Applying the technology requires thought and dedication, especially with legacy industries like law and insurance, which are being taken on in this way by Luminance and Omnius respectively. The companies’ founders, Emily Foges and Sofie Quidenus-Wahlforss, spoke with great insight on this on stage at Disrupt Berlin.\\nLuminance uses AI and natural language processing to help law firms process documents more quickly, not replacing the lawyer but providing additional intelligence and analysis of what may be hundreds or thousands of pages and saving time and money. Omnius applies AI not just to the text of insurance claims, but to the process of handling them, ensuring rapidity not only in documentation but in results like payouts.\\nOmnius has raised about $30 million in multiple small rounds and grants, while Luminance has raised some $23M mainly in its A and B rounds.\\nI’ve edited and contextualized our conversation here, but you can also watch the full panel below. I’ve made some slight changes for readability but left things mostly intact. Pull quotes belonging to Emily are on the left, Sofie’s are on the right.\\nThe first thing I wanted to hear from the founders was why they chose these industries, and why now? After all, law and insurance are notoriously old-fashioned, some would even say backwards in many ways. How could they be sure this was an opportunity, and not a folly?\\nEmily Foges (Luminance): It had more to do with the capabilities of the technology, actually. We started with technology that can read a lot of language, and then we looked at what industry would benefit most from that. It was that way around.\\nI think the timing is 80 percent of the battle; The fact that the legal profession had got to a point of being ready to accept the use of that kind of technology was more luck than anything. But there’s been such an explosion in enterprise data that lawyers just can’t possibly cope with reading and all of the documentation that they need to — so the market was ready.\\nSofie Quidenus-Wahlforss (Omnius): I think we come from a very similar background. We started on a horizontal level, with deep document understanding, and at some point we understood, if you really want to ship business value, you need to dive into one vertical.\\nWe have different verticals to choose: manufacturing, legal, pharma… so then we were like, okay, which area is the biggest that is not transformed yet? And do we see decision makers aware of the of the need to do something? And do they have money?\\nThe insuretech world is of course making a lot of pressure, all the new insurance companies like Lemonade, WeFox, Coya, because they claim to settle a claim in minutes. So the big guys like Alliance, they got nervous. And on the other hand you see, on the technology side, improvements in the areas of computing power, way more access to data, more flexible models. So we thought, the industry is ready, the technology’s ready, I was ready to build a big company. It’s my fourth company and I was like, this time I’ll build something huge. So everything fell into place.\\nThey don’t call them legacy industries for nothing, though. These domains, and some companies, that have existed for decades or even a century or more. That means legacy systems and legacy people, to put it kindly, that may not be amenable to change. Emily had some surprising stats on that, while Sofie advocated an AI-like approach to classifying and selecting clients.\\nEmily: Some of them are more ready than others, and I think the ones who aren’t ready need to really catch up, because we got to critical mass really quickly. We’re only three and a half years old, but we’ve got 185 law firms around the world signed up. The interesting thing was the most ready people were the law firms outside of the UK, outside of the US. It was European law firms, APAC-based law firms, South and Central American law firms who got on board first. They were more ready because to be honest, the commercial pressure was greater. And then the pressure on the US and UK law firms came from them.\\nThis is something I can really recommend for every startup trying to transform an industry from scratch: classifying your customers. We had 16-17 criteria, how we defined the companies we really want to spend time with.\\nSofie: We thought, cool, the transformation is happening already. But after a while, 2018, we were like, okay, this market is not moving as fast as we thought . We looked at our proof of concept, our pilots we did with insurance companies and were like, wow, every big insurance company in Europe wants to have an AI pilot project but who’s really ready to start with AI full production?\\nAnd this is something I can really recommend for every startup trying to transform an industry from scratch: classifying your customers. Who is a laggard, who is an early adopter, who is early mainstream, is an innovator? Then we decided together with the board, okay, we’ll only focus on innovators and early adopters, and the rest should wait, or we can both wait for each other — but we cannot waste our time.',\n",
              "  array([-4.8914614 , -4.3072643 , -0.02120397], dtype=float32),\n",
              "  2),\n",
              " ('One of the ways you can tell that a new technology is still at the hype stage is that no one talks about its problems. When an innovation is rising towards the ‘peak of inflated expectations,’ people talk about it in hushed tones as something almost miraculous — and they hardly ever discuss the obstacles to adoption or the challenges that the new technology will bring.\\nLast year, it seemed that people were appending the hallowed initials A.I. to every conceivable offering, much like they did with IoT and cloud in past years. The good news is that people are starting to think much more critically about AI and, as a result, are starting to recognize and talk about these challenges.\\nSo, if 2019 was the year that artificial intelligence (AI) started living up to its hype, 2020 will be about discussing and, of course, solving the key challenges that increasingly widespread AI adoption has revealed.\\nAnd some of these challenges are difficult and intractable indeed, not least how we can make AI available to every organization and how we can solve the thorny ethical problems that such a powerful technology presents to humanity. Here, then, are what I believe to be the most important developments in the field of AI in the year ahead.\\nBirth of the ‘AI ethicist’\\nAs we entrust more parts of our working and personal lives to AI-powered applications, the ethics of AI is becoming an increasingly important consideration.\\nOne of the most important ethical questions is about intention — for example, where, when and how AI interacts with the analogue reality that we have all been accustomed to live in. Who is going to decide what is the right playing field and what constitutes acceptable or unacceptable uses of AI.\\nRegulators and legislators are trying to define and control the degrees of freedom, but they are working at analogue speeds. We need faster answers to questions that we’re already facing today.\\nIn 2020, we will start to see enterprises employ people or even teams of people whose main role will be to formulate the ethics of our new AI-powered world. These AI Ethicists will need to liaise with the ecosystem of affiliated AI entities and gradually create, from the bottom up, the rules and conditions that will define the field of play.\\n5G as a catalyst\\nIf we’re all to enjoy the benefits of AI, we need an infrastructure technology that will enable end users to live, work and interact in the cloud. Future AI applications — and indeed current ones — require vastly increased speeds together with location-agnostic access and minimal latency, which is exactly what 5G will bring.\\nIt’s not hyperbole to suggest that 5G will be the key catalyst for a revolution in the way we experience reality. Truly connected homes and workspaces; advanced, telematics-enabled healthcare services, and “almost real” (VR / AR) interactive experiences – these are only a few of the ways where 5G could easily become the highway to the adoption of AI technologies from all industries, functions, and users.\\nAI-as-a-Service\\nIt had to come, didn’t it? Today it seems like everything is available as-a-Service (including, bizarrely, Proof of Concept-as-a-Service). But let’s not mock, because this model has genuinely revolutionized the way that organizations procure technology, bringing enterprise-grade tech within the ambit of even the smallest company.\\nWhat’s so exciting about AI-as-a-service is not just that the huge economies of scale will make the technology available to every organization that wants to use it. It will also give us the much-missed ability to harness all the infrastructure, platforms, and knowledge towards creating real and sustainable value.\\nSolutions center stage\\nIt used to be products and vendors that accelerated value. With AI, however, it will be the solutions themselves, since they will provide holistic approaches to resolving burning business questions rather than today’s piecemeal approach.\\nFrom 2020, businesses will use methodologies for best of breed, more cost-efficient of breed, or even open source ways of answering the key questions facing their organization. These solutions will focus on four categories:\\nML for the masses — Data augmentation and data-healing techniques to enable the creation of test sets, allowing for insightful outcomes upon choosing the right algorithm\\nCustomer-centric analytics — Utilizing AI to “translate” data rich journey touchpoints to humanly-digestible experiences in an easy, UX friendly way\\nSmart X / Smart Everything — Leveraging IoT technologies to enhance real experiences\\nHigher degree automation — RPA and machine-to-machine automation that identifies and prioritizes holistic reactive and proactive value creation across entire value chains\\nOf course, this can only scratch the surface of how AI will evolve in the coming twelve months. And that’s the fun of moving past the hype into the often challenging but ultimately incredibly rewarding world where AI powers everything.',\n",
              "  array([-1.0109742, -1.5238645, -0.8716344], dtype=float32),\n",
              "  2),\n",
              " ('Humans should get the credit for AI-made art\\nby AEON — in SYNDICATION\\n88\\nSHARES\\nIn December 1964, over a single evening session in Englewood Cliffs, New Jersey, John Coltrane, and his quartet recorded the entirety of A Love Supreme. This jazz album is considered Coltrane’s masterpiece – the culmination of his spiritual awakening – and sold a million copies. What it represents is all too human: a climb out of addiction, a devotional quest, a paean to God.\\nFive decades later and 50 miles downstate, over 12 hours this April and fuelled by Monster energy drinks in a spare bedroom in Princeton, New Jersey, Ji-Sung Kim wrote an algorithm to teach a computer to teach itself to play jazz. Kim, a 20-year-old Princeton sophomore, was in a rush – he had a quiz the next morning. The resulting neural network project, called deep jazz, trended on GitHub, generated a buzz of excitement and skepticism from the Hacker News commentariat, got 100,000 listens on SoundCloud, and was big in Japan.\\n*chirp chirp*\\nWho’s there? It’s early bird tickets to TNW2020\\nCOME IN\\nThis half-century gulf, bracketed by saxophone brass and Python code, has seen a rise in computer-generated music and visual art of all methods and genres. Computer art in the era of big data and deep learning, though, is a reckoning for algorithms, capital-A. We must now embrace – either to wrestle or to caress – computer art.\\nIn industry, there is blunt-force algorithmic tension – “Efficiency, capitalism, commerce!” versus ‘Robots are stealing our jobs!’ But for algorithmic art, the tension is subtler. Only 4 percent of the work done in the United States economy requires ‘creativity at a median human level’, according to the consulting firm McKinsey and Company. So for computer art – which tries explicitly to zoom into this small piece of that vocational pie – it’s a question not of efficiency or equity, but of trust. Art requires emotional and phrenic investments, with the promised return of a shared slice of the human experience. When we view computer art, the pestering, creepy worry is: who’s on the other end of the line? Is it human? We might, then, worry that it’s not art at all.\\nAlgorithms’ promise holds potent popular allure. A search for the word ‘algorithm’ in the webpages of the empirically minded site FiveThirtyEight (where I’m on staff) returns 516 results, as I write. I’m personally responsible for more than a few of those. In the age of big data, algorithms are meant to treat disease, predict the decisions of the Supreme Court, revolutionize sports and predict the beauty of sunsets. They will also, it’s said, prevent suicide, improve your arugula, predict police misconduct, and tell if a movie will bomb.\\nThe more grandiose would-be applications of algorithms and artificial intelligence (AI) are often preceded by ostensibly more manageable proving grounds – games, say. Before IBM’s question-answering computer, Watson, treats cancer, for example, it goes on the TV quiz show Jeopardy! Google’s AlphaGo took on a top human Go champion in a “grand challenge” for AI. But these contests aren’t trivial stepping stones – they can be seen as affronts to humankind. One commentator, realizing that Google’s program would win a match, said he “felt physically unwell”.\\nIt’s much the same for computer art projects. Kim and his friend Evan Chow, whose code is used in deep jazz, are members of the youngest generation of a long lineage of computer “artists.” (These two aren’t exactly starving artists, though. This summer, Kim’s working at Merck, and Chow’s at Uber.) As the three of us sat in a high-backed wooden booth in Cafe Vivian, on the Princeton campus, actual, honest-to-God human jazz played over the speakers – Rahsaan Roland Kirk’s frenetic ‘Pedal Up’ (1973) – and as Kim played me samples generated by deepjazz from his laptop, we were awash in an unholy jazz + jazz = jazz moment.\\n“The idea is pretty profound,” Kim said, as I strained to decipher what was human in the cacophony. “You can use AI to create art. That’s normally a process that we think of as immutably human.” Kim agreed that deepjazz, and computer art is often a proving ground, but he saw ends as well as means. ‘I’m not going to use the word “disruptive,” he said, then continued: “It’s crazy how AI could shape the music industry,’ imagining an app built on tech like deepjazz. ‘You hum a melody and the phone plays back your own custom, AI-generated song.”\\nLike a profitless startup, the value of many computer-art projects thus far is their perceived promise. The public deepjazz demo is limited, and improvises off just one song, ‘And Then I Knew’ (1995) by the Pat Metheny Group (Kim wasn’t quite sure how to pronounce ‘Metheny’). But the code is public, and it’s been tweaked to noodle the Friends theme song, for example.\\nOf course, it’s not just jazz music, and not just deepjazz, that has gotten the computer treatment – jigs and folk songs, a “Genetic Jammer”, polyphonic music, and quite a bit else has been put through the algorithmic ringer.\\nVisual art, too, has been subjected to algorithms for decades now. Two engineers created this image – probably the first computer nude – at Bell Labs in Murray Hill, New Jersey, somewhere geographically between Coltrane and Kim, in 1966. The piece was exhibited at the Museum of Modern Art in 1968.\\nThe New York Times reviewed one of the first exhibitions of computer art, in 1965 (just a few months after Coltrane’s recording session) featuring work by two scientists and an IBM #7094 digital computer, at a New York gallery, now long shuttered. ‘So far the means are of greater interest than the end,’ the Times wrote. But the review, by the late Stuart Preston, goes on to strike a surprisingly enthusiastic tone:\\nNo matter what the future holds – and scientists predict a time when almost any kind of painting can be computer-generated – the actual touch of the artist will no longer play any part in the making of a work of art. When that day comes, the artist’s role will consist of mathematically formulating, by arranging an array of points in groups, a desired pattern. From then on, all will be entrusted to the deus ex machina. Freed from the tedium of technique and the mechanics of picture-making, the artist will simply ‘create.’\\nThe machine is just the brush – a human holds it. There are, indeed, examples of computers helping musicians to simply “create.”\\nEmily Howell is a computer program. A 1990s creation of David Cope, now a professor emeritus at the University of California at Santa Cruz, ‘she’ was born out of Cope’s frustrating struggle to finish an opera of his own. (Howell’s compositions are performed by human musicians.)\\nThis music is passable. It might even be good and, for me, is safely on the right bank of the uncanny valley. But another thing that makes it more interesting is the simple fact that I know it was composed by a computer. I’m interested in that as a medium – an amplification of Cope’s artistic expression, rather than a sublimation. But the tension persists.\\nI’ve fallen down other rabbit holes, too: for one, the work of Manfred Mohr, an early algorithmic art pioneer who is himself a (human) jazz musician, as well as an artist. Namely his painting, P‑706/B (2000), based on a six-dimensional hypercube. I spent the next hour reading about Mohr, the man.\\nCourtesy Manfred Mohr\\nSometimes in ‘computer music’ it’s also the other way around – humans name the tune, software dances to it. And in one of these cases, the market has spoken loudly. Vocaloids are singing synthesizers, developed by Yamaha, and anthropomorphized by the Japanese company Crypton. One popular Vocaloid, Hatsune Miku (the name translates to ‘the first sound from the future’), headlined a barn-burning North American tour this year, where Miku appeared as a hologram, drawing lines around the block for $75 tickets at New York’s Hammerstein Ballroom. Miku is a huge pop star, but not a human. ‘She’ also appeared on the Late Show with David Letterman.\\nSo it’s increasingly not just dorm-room hackers and cloistered academics pecking at computer art to show off their chops or get papers published. Last month, the Google Brain team announced Magenta, a project to use machine learning for exactly the purposes described here, and asked the question: “Can we use machine learning to create compelling art and music?” (The answer is pretty clearly already “Yes,” but there you go.) The project follows in the footsteps of Google’s Deep Dream Generator, which reimagines images in arty, dreamy (or nightmarish) ways, using neural networks.\\nBut the honest-to-God truth, at the end of all of this, is that this whole notion is in some way a put-on: a distinction without a difference. “Computer art” doesn’t really exist in any more provocative sense than ‘paint art’ or ‘piano art’ does. The algorithmic software was written by a human, after all, using theories thought up by a human, using a computer built by a human, using specs written by a human, using materials gathered by a human, at a company staffed by humans, using tools built by a human, and so on. Computer art is human art – a subset rather than a distinction. It’s safe to release the tension.\\nA different human commentator, after witnessing the program beat the human champ at Go, felt physically fine and struck a different note: “An amazing result for technology. And a compliment to the incredible capabilities of the human brain.” So it is with computer art. It’s a compliment to the human brain – and a complement to oil paints and saxophone brass.',\n",
              "  array([-4.222076  , -2.7039182 , -0.08513416], dtype=float32),\n",
              "  2),\n",
              " ('How technology made us bid farewell to privacy in the last decade\\nJefferson Graham USA TODAY\\nPublished 1:08 PM EST Jan 2, 2020\\nIn 2011, Apple unveiled its first iPhone with artificial intelligence, a personal assistant named Siri that could answer questions and help keep track of our daily lives. \\nThe AI revolution had begun, and it gave way to higher resolution cameras on phones, such as the then-new iPhone 4S, microphones and cameras in the home, everything from connected speakers, security devices, computers and even showers and sinks.\\nBy the end of the decade, we were carrying and or living with devices that are capable of tracking our every movement. \\nCounties and states are selling our personal information to data brokers to resell it back to us, in the form of \"people search engines.\" Facebook and Google have refined their tracking skills, in the pursuit of selling targeted advertising to marketers, that many people believe they are listening to us at all times. They are that good at serving up ads based on our interests, whether we want it or not. \\nHave an iPhone 4S? Then Siri is your new best friend.\\nPhoto by Eileen Blass, USA TODAY\\nGoodbye privacy!\\nThe \"10s\" were the decade in which our privacy went away if we were connected to the Internet, which means most of us. Apple went on a crusade to protect our privacy, which could be argued as a competitive advantage over rivals, and groups ranging from the Electronic Frontier Foundation (EFF) and the Privacy Coalition began speaking out. In Europe, major changes were made to privacy laws on behalf of consumers, and a new California law goes into effect in January that will make it harder for companies to take our data and resell it. \\nOr so the language says. \\nAs more people became aware of privacy issues, and companies like Facebook announced several security breaches of our data, the bottom line is that the social network has more users and makes more money. Ditto for Google. \\n\"The biggest difference between then and now, is that people didn\\'t really think about what companies were collecting on us,\" says Chris Jordan, CEO of Fluency, a data analysis company. \"We weren\\'t worried about privacy. Now we are.\"\\nFood in 2020: Cauliflower? Kimchi? Pho? They\\'re what\\'s for dinner\\nRemember Vine? These social networking sites defined the past decade\\nNot that it wasn\\'t brought to our attention. In 2011, then hacker/security researcher Samy Kamkar discovered that the iPhone, Android and (the then still operating) Windows Phone mobile devices were sending back GPS information to their makers, even when the location services option were turned off, and made his findings public. \\nBluetooth is always on, despite the settings\\nIn 2019, Kamkar demonstrated for a USA TODAY reporter how little has changed. From the general settings of the iPhone, turn off the blue bluetooth icon in the Control Panel, and then go to the Bluetooth section in General, and Bluetooth is still running. \\n\"When you disable, you\\'re not disconnecting the software that continues to broadcast the information,\" says Kamkar, who is now the chief security officer and co-founder of Openpath, a company that aims to replace the office badge with app-based tokens for entrances. \"I can still get your name and phone number simply by being in the vicinity\" and picking up the bluetooth signal. \\nApple Watch shopper\\'s guide: Apple Watch shopper\\'s guide: What you need to know before buying\\nAnd there are more sensors reading you than ever before. Google now tracks your every movement, if you\\'re a user of the Google Maps smartphone app, and records a public history of your whereabouts, whether or not the app is even open and turned on. \\n\"We knew we were being tracked on phones, but didn\\'t realize that the companies were using the data in ways most people don\\'t approve of, or even realize it was capable of,\" says Danny O\\'Brien, director of strategy for the EFF. \\nPrivacy concerns went from something people were \"benign about, to genuinely anxious,\" he adds. \\nJust ask comedian/actress Tanjareen Martin Thomas. \\nCover your webcam and your phone\\n\"Most people cover their webcam cameras, but don\\'t think about the phones,\" says Kamkar. \\nThomas does. She brings her phone to the bathroom, but always places a lens cloth around the cameras.  \\n\"I don\\'t want some stranger watching me change my clothes,\" she says. \"I cover everything.\"\\nComedian/actress Tanjareen Martin Thomas\\nJefferson Graham\\nFrom the bathroom to the living room, the major innovation in TVs over the decade has been the smart TV, which eliminates the need for an external streaming device to bring internet programming from the likes of Netflix, Hulu and Disney+ direct to the set, without having to change the HDMI input settings. \\nThe sets themselves got so cheap, resellers are practically giving them away, with many Black Friday deals offering 40- and 50-inch models in the $200 and $300 range. These same size sets were selling for around $1,000 in 2010. \\nBlack Friday shoppers wait in line to check out at the Nebraska Furniture Mart store in Omaha, Neb., Friday, Nov. 23, 2018.\\nNati Harnik, AP\\nHow to make money selling TVs — by reselling our data\\nThat\\'s the good news. The bad: to turn a profit, manufacturers now make up the difference by selling your viewing habits to data brokers, letting them know what shows and networks you watch, your demographic and real estate locations and more.\\nSamsung has a TV with a built-in video camera, to enable video chat, but it also makes the TV even more susceptible for hacking. The onus is on the consumer to protect their smart devices with strong passwords, especially for the home network. \\nWhich brings us to the ever-present security doorbell cameras that are increasingly showing up in people\\'s homes. Ring, a company owned by Amazon, has come under attack by privacy groups for being allegedly easy to hack, not just for the doorbell product. \\nThe group Fight for the Future put out its own product warning, saying Ring cameras are not safe. Recently, several families have reported that their Ring cameras were hacked. In response, Ring said its owners needed to use stronger passwords. \\nMeanwhile, as we close off the decade, yes, people are fighting back against the privacy invasion, politicians have taken up the cause, with a vow to break up big tech, but what will it all look like in another 10 years. There\\'s artificial intelligence and facial recognition to add to all the tracking that\\'s going on now. \\nThe age of \"Minority Report,\" the sci-fi novel and film where government could pre-determine what you were going to do with visions of the future \"will happen,\" says Kamkar. \"It\\'s just a question of how far we\\'ll let it go.\"\\nFollow USA TODAY\\'s Jefferson Graham, @jeffersongraham on Twitter. \\nPublished 1:08 PM EST Jan 2, 2020',\n",
              "  array([-4.9167438 , -4.415022  , -0.01960823], dtype=float32),\n",
              "  2),\n",
              " ('SHANGHAI (Reuters) - China will expand the scope of its blockchain cross-border financing pilot platform, a senior official at the country’s foreign exchange regulator said on Tuesday.\\nLu Lei, deputy head of the State Administration of Foreign Exchange (SAFE) make his remarks at a forum in Beijing, where he said the regulator will strengthen the integration of fintech and the foreign exchange market, while maintaining a grip on supervising technology development.\\n“We will gradually expand the scope of the pilot and the application scenarios of blockchain technology in cross-border financing and macro prudential management,” Lu said.\\n“At the same time, (the government) will push forward a prospective study on foreign exchange reforms to deal with cryptocurrency and explore the construction of the foreign exchange regulation and technology system under the new situation.”\\nLu added that the SAFE’s cross-border financing blockchain platform is currently the only one registered by a central state agency at the Cyberspace Administration of China (CAC).\\nThe platform, first launched in March, has expanded to 19 provinces and cities in November from 9, according to Global Times, a newspaper published by the official People’s Daily.\\nChina has studied the application of blockchain and artificial intelligence in cross-border financing with a focus on risk management, and will further liberalise its capital markets.\\nBlockchain has attracted a lot of attention in the past few months after Chinese President Xi Jinping said China should accelerate the development of blockchain technology, a digital ledger that forms the backbone of many cryptocurrencies such as bitcoin.\\nHis remarks sparked a rush into the shares of firms engaged in, or believed to be engaged in blockchain or digital currency-related businesses.',\n",
              "  array([-4.347678  , -2.876151  , -0.07180548], dtype=float32),\n",
              "  2),\n",
              " ('How the digital revolution can tackle the climate crisis\\nby THE CONVERSATION — in SYNDICATION\\n38\\nSHARES\\nLife has changed almost beyond recognition in the last few decades. Artificial intelligence (AI) has substituted entire job fields – intelligent software can now review legal documents, a job which was previously only carried out by lawyers. Machine learning means technical systems can pull together entire libraries of information in a single handheld device. Virtual spaces now exist where people from all over the world can share, connect and chat instantly.\\nIn 2019, it’s clear that digital innovations will continue to change society and the economy, but it’s uncertain whether these new technologies will benefit the global transformation to sustainability. Will digital technologies allow everyone to live in a world where their development isn’t dependent on exhausting finite resources and increasing emissions?\\n*chirp chirp*\\nWho’s there? It’s early bird tickets to TNW2020\\nCOME IN\\nThere is certainly reason to be optimistic. Digital technologies can make energy and resource use more efficient. By analyzing the optimal amount of water each crop needs and by using a smart irrigation system accordingly, farming can become infinitely more efficient. Digital systems can assess the optimal use of vehicles too. Instead of one person owning one car which is only used for an hour each day and then sits parked for the rest of the day, several people can share one electric car, reducing the number of vehicles needed overall.\\nDigital technology could make resource and energy use much more efficient. Golden Sikorka/Shutterstock\\nFor the first time, digital technologies can comprehensively trace, document, and analyze each resource and product as it flows across global supply chains. This could create circular economies, in which resources such as water and rare Earth metals arrive where they’re needed and waste such as plastic rubbish doesn’t escape to pollute the environment.\\nDigital monitoring can help scientists to better understand how ecosystems around the world – such as forests, reefs and glaciers – are changing in real time. This can help conservationists understand how to protect and restore the environment, while ensuring that governments and private companies are held accountable for their public commitments to preserve natural habitats. The Global Forest Watch already does this – it’s an open-source web system that uses satellite data to monitor forests and their destruction.\\nBut so far, this potential hasn’t been harnessed to make the global economy greener. Instead, there’s been an explosion in high energy computing centers, which are needed to sustain the expanding internet, its social media platforms and big data analysis. By 2022, global data flows will have more than tripled since 2017. As long as fossil fuels dominate the global energy mix, expanding these computing centers will accelerate climate change.\\nIt’s been estimated that Bitcoin data mining centers consume as much energy per year as the whole of Ireland. DR MANAGER/Shutterstock\\nSolutions to global problems like climate change and the instability of financial markets need the cooperation of all countries. But a knowledge divide between the global north – which could secure access to sophisticated digital infrastructure with its wealth – and the global south would hold worldwide solutions back.\\nThe digital revolution is transforming societies just as the printing press and the steam engine triggered new eras of human civilization. But without using these digital disruptions to make the global economy more sustainable, the powerful tools of the digital revolution will just multiply existing problems.\\nAI systems can substitute a major part of a well qualified work force, but without policies in place to support workers to retrain or enjoy the benefits of working less, unemployment and poverty could skyrocket. Meanwhile, authoritarian regimes already use digital monitoring to control their citizens and influence their behavior.\\nWorld leaders need to ensure public administrations are up to speed to effectively and ethically govern these digital disruptions. Most governments around the world currently lack expertise on AI, machine learning, and virtual reality – many bureaucracies are still paper-based. Governments are struggling to adapt to the new realities, which pose important questions about data privacy laws and the need and desire to increasingly benefit from digital systems.\\nGovernments need to ramp up investments in digital infrastructure, channelling it towards making everything about the modern world sustainable. They need to ensure that everyone can profit from the new opportunities of the digital age and that no one is left behind. This includes investing in broadband networks and expanding education for the next generation of digital engineers.\\nCombining AI, big data analysis, genome research and developments in cognitive sciences opens the door to a completely new world. In many ways, the pace of the digital revolution can redefine what it means to be human, enhancing our physical, psychological, and cognitive capabilities.\\nThe digital era is the new reality whether people like it or not. The world stands at a crossroads – it’s in all humankind’s interest to take the pathway towards sustainability. Digital technology can take us there – better and faster than ever before – if we steer it in the right direction.',\n",
              "  array([-3.2805214, -3.5724177, -0.067954 ], dtype=float32),\n",
              "  2),\n",
              " ('Five years ago this Christmas, some people who woke up to a new Xbox or PlayStation hoping to play the latest video games might as well have received a lump of coal. It turned out to be a harbinger of holiday computer grief to come.\\nA hacking group called the Lizard Squad picked Dec. 24, 2014, to launch an attack against the computer networks of the two gaming systems, and they succeeded in knocking the networks offline for much of the next two days — temporarily turning $350 gaming consoles into pieces of junk.\\n“It’s really like the Grinch,” said Patrick Sullivan, senior director of security at Akamai Technologies, a security firm that specializes in stopping attacks.\\n“Everyone gets this present and they’re excited to play it and they’re deprived of that opportunity,” Sullivan said. “They have to talk to their family, I guess.”\\nEvery year since, Christmas Day has become notorious in the cybersecurity industry. While presents are getting unwrapped under trees, computer systems — and some unfortunate security professionals — are guarding against deluges of fake traffic.\\nFor hackers, Christmas Day is now a peak time to demonstrate their skills, purposefully anger people or try to extort money from companies or businesses that might be understaffed and vulnerable. For online retailers or video game networks, the attacks may hit when consumers have the highest expectations that computer systems will work — and tie many computer professionals to their desks on the holiday.\\nThe idea of ruining Christmas morning by downing a corporate or government computer network is so tempting to some hackers that there may not be another day on the calendar that compares, with the possible exception of the Black Friday shopping bonanza after Thanksgiving, security experts said.\\n“It’s going to be much more effective than if you do it on some random day in April. They’re much more likely to be able to get somebody to pay up,” said Andrew Shoemaker, CEO of NimbusDDOS, a company that helps companies test their cyberdefenses.\\nHacker accessed ‘Ring’ camera inside little girl’s room, her family says\\nDEC. 12, 201903:03\\nThe type of attack that hackers often deploy on Christmas — including during the Xbox and PlayStation outages — is called a distributed denial-of-service attack, or DDoS, in which the attackers try to overload a network’s servers with artificial traffic. The attacks have become more sophisticated over time, with new ways to flood corporate servers.\\nLaw enforcement authorities stepped up their attention to Christmastime attacks a year ago, pre-emptively seizing the domains of 15 DDoS-for-hire services and announcing criminal charges against three people less than a week before the holiday.\\nThe motive for an attack isn’t always known, but at least some of the time, hacking groups may be trying to market abilities that they plan to sell as services later on, Shoemaker said.\\n“People in the black-hat community, a lot of times they’ll showcase their abilities within the community by picking a target and showing off for a few hours,” he said.\\nOne such hacker was a Utah man who, according to the Justice Department, would use Twitter to announce his attacks and post screenshots as evidence. In July, he was sentenced to more than two years in prison. Prosecutors said his attacks included the first high-profile Christmas DDoS attack against gaming companies in late 2013.\\nAfter the Lizard Squad’s turn in the spotlight in late 2014, the group claimed responsibility for other attacks, including one against the website of Malaysia Airlines and another targeting Taylor Swift’s Twitter account.\\nSome members of the Lizard Squad have since been arrested. One of them, Zachary Buchta of Maryland, was sentenced last year to three months in prison after he pleaded guilty, and the judge scolded him for crimes that had real-world consequences, “not a fantasy.”\\nSony, which makes the PlayStation, and Microsoft, owner of the Xbox system, did not respond to requests for comment on the lasting impact of the 2014 attacks.\\nThere’s no comprehensive public data for how much of a spike occurs in late December, because digital attacks often aren’t reported, even when they’re successful.\\nBut the Christmastime imitators have continued similar threats against gaming companies in the years since the attacks first gained wide attention.\\nAnd it’s not just video game platforms. In 2015, a Christmas Day DDoS attack targeted Linode, a cloud computing company, knocking its services intermittently offline for days. The private intelligence group Stratfor Global Intelligence was the target of an attack on Christmas Eve 2011 in which attackers said they had obtained access to a huge cache of emails.\\nSecurity companies have raced to keep up. Lawrence Orans, an analyst at the research firm Gartner, said DDoS mitigation firms have invested heavily in their infrastructure to add capacity in the past five years, rerouting suspicious internet traffic through “scrubbing” servers before the traffic can knock a system offline.\\nMuch of the preparation for Christmas security takes place in the months before the holiday, as corporate security staff run drills and test their defenses, experts said. But for some information technology workers, Christmas will mean a day at the office.\\n“The big organizations do have 24/7 coverage of trained personnel — similar to the way that police departments, hospitals and the military don’t stop their services just because it’s Christmas,” said Gene Spafford, a Purdue University computer scientist who studies security.',\n",
              "  array([-1.835138  , -3.1111155 , -0.22833541], dtype=float32),\n",
              "  2),\n",
              " ('Human art evades understanding so how will we ever ‘get’ machinic art?\\nby AEON — in SYNDICATION\\n77\\nSHARES\\nAssuming that the emergence of consciousness in artificial minds is possible, those minds will feel the urge to create art. But will we be able to understand it? To answer this question, we need to consider two subquestions: when does the machine become an author of an artwork? And how can we form an understanding of the art that it makes?\\nEmpathy, we argue, is the force behind our capacity to understand works of art. Think of what happens when you are confronted with an artwork. We maintain that, to understand the piece, you use your own conscious experience to ask what could possibly motivate you to make such an artwork yourself – and then you use that first-person perspective to try to come to a plausible explanation that allows you to relate to the artwork.\\nBirds are cool\\nEarly birds are even cooler. Get cheap tickets to TNW2020 right now\\nYEAH\\nYour interpretation of the work will be personal and could differ significantly from the artist’s own reasons, but if we share sufficient experiences and cultural references, it might be a plausible one, even for the artist. This is why we can relate so differently to a work of art after learning that it is a forgery or imitation: the artist’s intent to deceive or imitate is very different from the attempt to express something original. Gathering contextual information before jumping to conclusions about other people’s actions – in art, as in life – can enable us to relate better to their intentions.\\nBut the artist and you share something far more important than cultural references: you share a similar kind of body and, with it, a similar kind of embodied perspective. Our subjective human experience stems, among many other things, from being born and slowly educated within a society of fellow humans, from fighting the inevitability of our own death, from cherishing memories, from the lonely curiosity of our own mind, from the omnipresence of the needs and quirks of our biological body, and from the way it dictates the space- and time-scales we can grasp. All conscious machines will have embodied experiences of their own, but in bodies that will be entirely alien to us.\\nWe are able to empathize with nonhuman characters or intelligent machines in human-made fiction because they have been conceived by other human beings from the only subjective perspective accessible to us: ‘What would it be like for a human to behave as x?’ In order to understand machinic art as such – and assuming that we stand a chance of even recognizing it in the first place – we would need a way to conceive a first-person experience of what it is like to be that machine.\\nThat is something we cannot do even for beings that are much closer to us. It might very well happen that we understand some actions or artifacts created by machines of their own volition as art, but in doing so we will inevitably anthropomorphize the machine’s intentions. Art made by a machine can be meaningfully interpreted in a way that is plausible only from the perspective of that machine, and any coherent anthropomorphized interpretation will be implausibly alien from the machine perspective. As such, it will be a misinterpretation of the artwork.\\nBut what if we grant the machine privileged access to our ways of reasoning, to the peculiarities of our perception apparatus, to endless examples of human culture? Wouldn’t that enable the machine to make art that a human could understand? Our answer is yes, but this would also make the artworks human – not authentically machinic. All examples so far of ‘art made by machines’ are actually just straightforward examples of human art made with computers, with the artists being the computer programmers. It might seem like a strange claim: how can the programmers be the authors of the artwork if, most of the time, they can’t control – or even anticipate – the actual materializations of the artwork? It turns out that this is a long-standing artistic practice.\\nSuppose that your local orchestra is playing Beethoven’s Symphony No 7 (1812). Even though Beethoven will not be directly responsible for any of the sounds produced there, you would still say that you are listening to Beethoven. Your experience might depend considerably on the interpretation of the performers, the acoustics of the room, the behavior of fellow audience members or your state of mind. Those and other aspects are the result of choices made by specific individuals or of accidents happening to them. But the author of the music? Ludwig van Beethoven.\\nLet’s say that, as a somewhat odd choice for the program, John Cage’s Imaginary Landscape No 4 (March No 2) (1951) is also played, with 24 performers controlling 12 radios according to a musical score. In this case, the responsibility for the sounds being heard should be attributed to unsuspecting radio hosts, or even to electromagnetic fields. Yet, the shaping of sounds over time – the composition – should be credited to Cage. Each performance of this piece will vary immensely in its sonic materialization, but it will always be a performance of Imaginary Landscape No 4.\\nWhy should we change these principles when artists use computers if, in these respects at least, computer art does not bring anything new to the table? The (human) artists might not be in direct control of the final materializations, or even be able to predict them but, despite that, they are the authors of the work. Various materializations of the same idea – in this case formalized as an algorithm – are instantiations of the same work manifesting different contextual conditions.\\nIn fact, a common use of computation in the arts is the production of variations of a process, and artists make extensive use of systems that are sensitive to initial conditions, external inputs or pseudo-randomness to deliberately avoid repetition of outputs. Having a computer executing a procedure to build an artwork, even if using pseudo-random processes or machine-learning algorithms, is no different than throwing dice to arrange a piece of music, or to pursuing innumerable variations of the same formula. After all, the idea of machines that make art has an artistic tradition that long predates the current trend of artworks made by artificial intelligence.\\nMachinic art is a term that we believe should be reserved for art made by an artificial mind’s own volition, not for that based on (or directed towards) an anthropocentric view of art. From a human point of view, machinic artworks will still be procedural, algorithmic and computational. They will be generative, because they will be autonomous from a human artist. And they might be interactive, with humans or other systems. But they will not be the result of a human deferring decisions to a machine, because the first of those – the decision to make art – needs to be the result of a machine’s volition, intentions and decisions. Only then will we no longer have human art made with computers, but proper machinic art.\\nThe problem is not whether machines will or will not develop a sense of self that leads to an eagerness to create art. The problem is that if – or when – they do, they will have such a different Umwelt that we will be completely unable to relate to it from our own subjective, embodied perspective. Machinic art will always lie beyond our ability to understand it because the boundaries of our comprehension – in art, as in life – are those of the human experience.',\n",
              "  array([-0.38167575, -2.8296177 , -1.3538342 ], dtype=float32),\n",
              "  0),\n",
              " ('Why your cat is lousy at chess yet way smarter than even the most advanced AI\\nby THE CONVERSATION — 26 days ago in SYNDICATION\\n64\\nSHARES\\nIf you share your home with a dog or a cat, look at it carefully and you will get a good overview of everything we don’t know how to do in artificial intelligence.\\n“But my cat does nothing all day except sleep, eat and wash herself,” you may think. And yet your cat knows how to walk, run, jump (and land on her feet), hear, see, watch, learn, play, hide, be happy, be sad, be afraid, dream, hunt, eat, fight, flee, reproduce, educate her kittens – and the list is still very long.\\nVolume 0%\\nEarly birds get all the good stuff\\nLike a great price on TNW2020 tickets\\nGIMME\\nEach of these actions requires processes that are not directly “intelligence” in the most common sense but are related to cognition and animal intelligence. All animals have their own cognition, from the spider that weaves its web to the guide dogs that help people find their way. Some can even communicate with us. Not by speech, of course, but cats and dogs don’t hesitate to use body language and vocalization – meowing, barking, wagging their tails – to get what they want.\\nLet’s look again at your cat. When she comes carelessly to rub up against you or sits in front of her bowl or in front of a door, the message is quite clear. She is looking for a caress, is hungry or wants to go out (then get in, then out, then in…). She has learned to interact with you to achieve her goals.\\nWalking, a complex problem\\nAmong all these cognitive skills, there is only a handful that we are beginning to know how to reproduce artificially. For example, bipedal locomotion – walking with two legs. It might be easy and natural to us, but it is actually something extremely complicated for robotics and it took decades of intensive research to build and program a robot that more or less walks properly on its own two legs. That is, without falling because of small pebble under its foot or when a person simply walked by a little too close.\\nRemember that it takes a baby an average of one full year to learn how to walk, demonstrating the complexity of what may seem like a “simple” problem. And I’m only talking about walking, not hopscotch or, say, soccer.\\nToday, one of the biggest challenges in autonomous robotics is to design and built two-legged robots that can successfully play one of the most popular human team sports. The Robocup 2020, which brings together nearly 3,500 researchers and 3,000 bipedal robots, will take place next year in Bordeaux, France. There you’ll be able to observe them playing soccer, and while great strides have been made (literally), they remain distinctly clumsy and a long way from the thrills of the human World Cup.\\nIdentifying isn’t understanding\\nWhat about object recognition? Today we know how to create computer algorithms that can do that, don’t we? While it is true that some can now name the content of almost any image, this does not relate to intelligence or cognition.\\nTo understand this, you have to look at how these algorithms work. Supervised learning, which remains the most popular method, consists of presenting images and a label describing the content of the image to the program. The total number of images is generally much higher than the number of labels. Each label is associated with a very large number of images representing the object in different situations, under different angles of view, under different lights, etc.\\nFor example, for an AI program to be able to recognize cats, up to one million images must be presented. By doing so, it will build an internal visual representation of the object by calculating a kind of average of all the images. But this representation is ultimately only a simple description that is not anchored in any reality. Humans can recognize a cat from its purr, the feel of its fur against the leg, the delicate scent of a litter box that’s overdue for a cleaning. All these and a hundred more say “cat” to us, but mean nothing to even the most sophisticated AI program.\\nTo do so, an algorithm would need a body that allows it to experience the world. But then, could it understand what a drink is if it’s never thirsty? Could it understand fire if it’s never been burned? Could it understand the cold if it never shudders? When an algorithm “recognizes” an object, it doesn’t understand at all – really, not at all – the nature of that object. It only proceeds by cross-checking with examples previously presented. This explains why there have been a number of autonomous-car crashes. While roadways are a highly constrained form of the world, they remain visually and functionally complex – vulnerable users such as pedestrians and cyclists can too easily be overlooked, or one street element mistaken for another. And the consequences of AI’s shortcomings have sometimes been fatal.\\nThe sensible experience of the world\\nWhat about humans? Try the experience of showing a real puppy to a child and she will be able to recognize any other puppy (even if she doesn’t know the word yet). Parents, by designating and naming things, will help the child develop language based on concepts that she has experienced before. But this learning, which may seem easy, even obvious, to us is not.\\nThis is beautifully illustrated by the life of Helen Keller, who lost her hearing, sight and power of speech at the age of two. Her educator, Anne Sullivan, tried for a long time to teach her the words by drawing signs on the palm of Helen’s hand and then touching the corresponding object. Anne Sullivan’s efforts were initially unsuccessful because Helen did not have the entry points for this strange dictionary. Until the day that Anne took Helen to a well, let water run over her hand and…\\n“Suddenly I felt a misty consciousness as of something forgotten – a thrill of returning thought; and somehow the mystery of language was revealed to me. I knew then that “w-a-t-e-r” meant the wonderful cool something that was flowing over my hand. That living word awakened my soul, gave it light, hope, joy, set it free! … Everything had a name, and each name gave birth to a new thought. As we returned to the house every object which I touched seemed to quiver with life.“\\nThose are the words of Helen Keller herself. She wrote them a few years later in her book The Story of My Life (1905). For her, on that precise day, the symbols were forever grounded in reality.\\nWhile spectacular progress has been made in the field of machine learning, grounding the digital symbols into the real world remains completely unresolved. And without the resolution of this problem, which is necessary but probably not a sufficient, there will be no general artificial intelligence. So there are still a lot of things that we are far from knowing how to do with artificial intelligence. And remember, “elephants don’t play chess.”',\n",
              "  array([-2.2088525 , -2.6683192 , -0.19747013], dtype=float32),\n",
              "  2),\n",
              " ('YouTube will shine even more attention on learning in 2020\\nJefferson Graham USA TODAY\\nPublished 8:01 AM EST Dec 26, 2019\\nMention YouTube to a parent, and you get several thoughts. At best, it\\'s a babysitter and an educational resource. \\nYet it\\'s also a site to be wary of, first, because children under the age of 13 are prohibited from even watching, per the user policy. (Ignored by many parents.) Then there is the avalanche of hate videos and the company\\'s run-ins over comments that exploit children, videos that harass people and the like. \\nSo what to do for YouTube in 2020? Look to see an even bigger push on its most acclaimed feature, learning.  \\nAnyone can learn how to do anything on YouTube, taught by real teachers, average people, company-made productions and now big-ticket, YouTube-financed videos, starring people such as Robert Downey, Jr., Michelle Obama and author James Patterson.\\nOn YouTube, everyone\\'s a teacher, or so it seems. Have trouble with a toilet that\\'s running? Someone has a tutorial on how to fix it. Everything from math comprehension to how to fly a drone and fix a cracked iPhone screen is out there, most from your neighbor\\'s living rooms. \\n Now the former first lady has a Book Club. Downey teaches about artificial intelligence. Daniel Schiffman, a professor at New York University, teaches coding, and YouTuber Marques Brownlee is doing a retrospective on antique tech devices. YouTube reached into the best of its learning videos to create playlists at youtube.com/learning to show how to start a business, learn math and fact-check the Internet.\\nRejoice: Cord cutters, you can finally stream your PBS stations online – on YouTube TV\\nTimeshift: Controversial YouTube star PewDiePie taking a break in 2020\\nYouTube touts the Learning section as a place to find the \"best how to, DIY, tutorial, and educational videos on YouTube.\"\\nIn other words, instead of relying on the traditional anything goes method, YouTube programmers curate and choose the cream to showcase the kind of videos YouTube wants to be known for.\\nWill it eliminate the stench from that $170 million fine by the Federal Trade Commission and New York attorney general for what they said was collecting information about children without their consent? In response, YouTube cracked down on abuses within its children\\'s related programming (its most popular genre) by eliminating comments, shedding many channels and requiring creators to identify if their programming targets kids. \\n\"YouTube had to do this,\" says Joshua Cohen, the editor of TubeFilter, a website that tracks online video, about the Learning initiative. \"It\\'s a great PR move and lets YouTube talk about the more positive aspects of the service.\"\\nIt\\'s not just altruism at work. YouTube\\'s CEO, Susan Wojcicki, is the daughter of two teachers. Her mom teaches high school English and journalism, and her dad is a Stanford University physics professor. It runs in the family.\\nThere\\'s a big incentive for people to post how-to videos to YouTube. People who make them get paid. YouTube rewards creators with a following (minimum: 1,000 subscribers, plus 4,000 hours of watch time in a 12-month period) with a share of the advertising revenue generated.\\nFor the Learning initiative, Katie Kurtz, YouTube\\'s director of content partnerships, says YouTube seeks \"highly credible\" educators who have used the platform for awhile, who can create \"trustworthy, authentic\" content that will \"further enhance the learning experience\" on YouTube. \\nHow popular is education on YouTube? Kurtz says hundreds of millions people come to YouTube to learn, and about 4 billion hours of how-to videos were watched on YouTube last year. \\nStill, education videos take a backseat to the more popular music videos and children\\'s videos that generate the most views, Cohen says. \"The vast majority of science and education videos don’t get huge views\" individually, he says, but when added together, they do generate a sizable audience. \\nGregory Brown has produced science content for his ASAPScience YouTube channel since 2012. A former science teacher, he started making videos on the side, and the work took off fast, enabling him to quit his day job and focus full-time on YouTube. \\nWhen the company asked Brown and partner Mitchell Moffit to be part of the Learning initiative, he was thrilled. Since science videos aren’t as popular as a music video or funny cat moment, “this really helps,” he says, “to make sure we’re financially thriving.”\\nUnlike YouTube\\'s \"Partner\" program, in which anyone can gain a following, apply to share in ad revenue and start making money, the Learning channel isn\\'t open to all. Creators need to work hard, make great videos, develop a rapport with the audience and be consistent.\\nThen YouTube may find you and ask you to join the program, which is heavily promoted and thus more likely to find mega-viewers.\\nMarques Brownlee, a 20-year-old YouTube tech reviewer, at his Hoboken, N.J., apartment.\\nEileen Blass, USA TODAY\\nOne tool that will be open to everyone creates Learning playlists to showcase how-tos in a curated fashion. YouTube offered it to select creators this year and will open it to the wider community in 2020, Kurtz says. \\nThe playlists can be built like a traditional lesson plan, from beginning to advanced, and won\\'t be littered, Kurtz says, with the YouTube algorithm that offers \"suggested\" videos to watch. \"We know that would be distracting.\"\\nIt\\'s that algorithm that got YouTube in trouble in the first place, when it began suggesting hate and conspiracy videos to consumers of news content. \\nFollow USA TODAY\\'s Jefferson Graham (@jeffersongraham) on Twitter\\nPublished 8:01 AM EST Dec 26, 2019',\n",
              "  array([-1.6968049 , -2.1229603 , -0.36089095], dtype=float32),\n",
              "  2),\n",
              " ('Over the past 10 years, thousands of movies have hit the world’s multiplexes. It’s nearly impossible to watch, let alone review, all of them. Yet, looking back over the past decade, it’s easy to recall the ones that left indelible marks. The ones that caused audiences to leave the theater gobsmacked (or heartbroken, or mind-blown). For us at WIRED, this list (in chronological order) represents those movies. Not everything here is a genre film—our specialty—but there are probably more sci-fi, fantasy, and comic-book movies here than on any other best-of roundup. Good. We love this stuff. Hope you do too.\\nDecade in Review: WIRED looks back at the promises and failures of the past 10 years\\nThe Social Network (2010)\\n“The movie,” as Facebook executives still indignantly call it, set the tone for the decade in both film and the tech metanarrative. Aaron Sorkin’s best script, a dolphin-skin-smooth nightmare, and Jesse Eisenberg’s best performance, megalomaniacal paranoia at its most delicious, nailed (spiritually, if not entirely factually) Facebook’s slippery origins and presaged its assaults on privacy, democracy, and consciousness. All of that was topped off with an Academy Award-winning techno-industrial-horror score that launched Trent Reznor and Atticus Ross as the decade’s composers of America’s anxieties (Gone Girl, Bird Box, Watchmen). \"Hand Covers Bruise,\" the opening track, which underscores Mark Zuckerberg’s scampering between Harvard’s redbrick dorms with baleful foreshadowing, might as well be the soundtrack to the decade. We the people are the lone piano, plinking nervously in the foreground, straining for a melody. A jittery drone and disquieting bass blasts (scandals, notifications, atrocities) slowly drown us out, until all that’s left are discordance, disunity, devolution. —Zak Jason\\nAttack the Block (2011)\\nJoe Cornish’s Attack the Block is a funny, nerve-rattling adventure flick about a bunch of teens in South London who defend their home from an alien invasion. Aside from its antics, it’s the film that launched future Star Wars star John Boyega to prominence. (It also features future Doctor Who Time Lord Jodie Whittaker.) Pulsing, scary, often hilarious—Attack the Block is what teen action-adventure movies should all strive to be. —Angela Watercutter\\nLooper (2012)\\nYears before he entered the Star Wars universe with The Last Jedi, writer-director Rian Johnson upended time travel tropes with Looper. Set in a near future where hitmen have to one day eighty-six their future selves, Johnson’s story is ultimately a noir, but more than that it is a wickedly intelligent look at what anyone would do if they could try to fix the past—or the future. More specifically, it was the best time-travel movie of the decade, even if it wasn’t really about time travel at all. —Angela Watercutter\\nSnowpiercer (2013)\\nScience fiction works metaphorically, of course. The aliens are really us, etc. Here, in Bong Joon Ho’s future, the metaphor works more, erm, literally. The Snowpiercer is a train plowing through the icy remains of Earth. It’s all that’s left. It’s society, it’s social stratification—verticalized and energized. The people at the back, led by Chris Evans, fight their way to the front. They subsist on gelatinous bars of ground-up bugs. Tilda Swinton and her fake teeth commit atrocities. As the poor overtake the privileged, secrets are exposed and compromises are made. It’s a breathless ride, a narrow premise opening up so capaciously the metaphor threatens to overtake the meaning. Grandly, it never does. —Jason Kehe\\nHer (2013)\\nWhen striving to be prescient, science fiction often becomes a twisted, myopic portrait of the present. (Lookin’ at you, Ready Player One.) Spike Jonze’s strange romance, Her, starring Scarlett Johannson as an operating system and Joaquin Phoenix as a sensitive, heartbroken man in high-waisted pants, leaps over that pitfall with ease. Rather than imbuing the story of a man literally falling in love with a computer with an aura of freakshow, Her is oddly sweet, sympathetic to Phoenix’s Theodore and Johannson’s “Samantha.” It’s that sympathy—or, really, empathy—that made it last this decade, and will likely make it last the next. Jonze’s vision has, in the age of social media and artificial intelligence, come true. Whether they worship Instagram influencers or consider themselves true digisexuals, many people are now truly in love with their computers, and that ardor shows no sign of fading. —Emma Grey Ellis\\nUpstream Color (2013)\\nShhh. You’re confused. That’s OK. There are pigs. Weird flowers. Some kind of evil foley artist. A man and a woman. Focus on them, those two. Clearly, they’re falling in love. This is a love story. Not an easy one—but when was love easy? It’s hurt and repair, sensitivity and devotion. The pigs and the parasites mean something, surely. Something about cycles and resonances and the value of life. That’s enough. Find meaning in the moments, not in the whole. This outing—Shane Carruth’s second—is uncrackable. It asks you to give up the burden to know or understand. It’s make-believe of the boldest, truest kind, a precious achievement not only in the genre but in the history of film: a story that denies you entry, even as it welcomes you inside. —Jason Kehe\\nBirdman (2014)\\nHere at WIRED we write a lot about superheroes and antiheroes. Alejandro G. Iñárritu’s Birdman is both—and neither. Its protagonist, Riggan Thompson (Michael Keaton), is an actor who lost his credibility as a serious thespian because he donned a super-suit in his earlier years (much the way Keaton himself did as Batman). His path is an open-eyes look at the value put on fame and what it means to truly find redemption. It’s also beautifully shot and full of brilliant performances from Keaton, Emma Stone, and Edward Norton. —Angela Watercutter\\nEdge of Tomorrow (2014)\\nIt’s well established that the title of this movie stinks. Edge of Tomorrow—is this a song by Lady Gaga? Maybe that’s why they tried remarketing it as Live Die Repeat, which is somehow, for being a second try, even worse. No matter. The movie itself is a keeper for the ages, Groundhog Day for those who found that Hallmarker a bit of a grating bore. Tom Cruise (whose effectiveness as a movie star has been one career-long live-die-repeat) and Emily Blunt do normally linear-time things like fall in love and kill aliens over a looping single day. Even as the narrative repeats and repeats, it never once feels repetitive. Instead, it drives relentlessly forward, toward the inevitable boss battle and chronological bust-out—the best movie based on a videogame that never existed. —Jason Kehe\\nMad Max: Fury Road (2015)\\nIf the early 2000s have been marred by anything, it’s an overabundance of reboots. If there was one movie that helped remove that tarnish, it was Mad Max: Fury Road. Thirty years after the last Max installment, Mad Max Beyond Thunderdome, writer-director George Miller brought all of the beauty and grit of his earlier movies to Fury Road and then cranked it up to 11 with the kind of stunts, practical effects, and feminist messaging that never made it into those previous chapters. It was an adrenaline-fueled death race that also managed to take on environmental issues and sex slavery. It might have been a revival of a massive franchise, but it was also unlike anything anyone had ever seen before—or since. —Angela Watercutter\\nTangerine (2015)\\nShot on an iPhone 5, using lead actors whose real-life stories informed the plot, with a soundtrack influenced by Vine, Tangerine is the kind of movie that could only have been made in 2015. Even though Vine is gone and the Donut Time restaurant that served as the movie’s setting is closed, the film stands as a testament to doing amazing things with little means. Director Sean Baker made his movie with $100,000 from Mark Duplass and actresses—Mya Taylor and Kitana Kiki Rodriguez—he met at Los Angeles’ LGBTQ center. Its story must be seen to be believed, but its beauty is obvious in the first shot. —Angela Watercutter\\nThe Lobster (2015)\\nGreek filmmaker Yorgos Lanthimos’ bleak dystopian film is the epitome of strange. Set in a near-future where single people are sent to “The Hotel” to find a mate (if they don’t pair up in 45 days, they’re turned into animals and sent into the wild), it’s ultimately a story about connection. Or a story about the manufactured values that are placed on coupledom. Its premise may be futuristic and bizarre, but its long look into the soul of relationships—or lack thereof—is heart-wrenchingly profound. Plus, it has a Pi-like ending no one can ever forget. —Angela Watercutter\\nMoonlight (2016)\\nThis decade, images defined us. It was unavoidable, mostly, given that the chief cultural engines of the 2010s were image-centric innovations: updates to the iPhone camera, Instagram, the permanence of surveillance culture, TikTok. Comparatively, films could feel a little less exciting. The structures of Hollywood simply don’t allow for the same kind of cultural disruption, no matter how hard Netflix has tried to shatter that model. In 2016, that shifted with the release of Moonlight, a queer black love story that went mainstream. Originally adapted from playwright Tarell Alvin McCraney’s In Moonlight Black Boys Look Blue, the film is flush with scenes of tenderness that keenly measure the depths of belonging, vulnerability, and black male intimacy. The anguished triptych is an extraordinary study in distance: Juan (Mahershala Ali) teaching a terrified Little (Alex Hibbert) how to swim; Chiron (Trevonte Rhodes) reuniting with Kevin (Andre Holland) in a Miami diner, transforming the eatery into an Eden of unspoken desire. The beauty of the Barry Jenkins-directed feature, which went on to win Best Picture at the Academy Awards, is how it forsook any kind of neat remedy on identity, sexual orientation, or gender performance. The resulting image gave us a new way to see ourselves. —Jason Parham\\nThe Handmaiden (2016)\\nIs it unacceptable overstatement to call a work of art Shakespearean? So be it: The Handmaiden, Chan-wook Park\\'s outrageous lesbian psychodrama about thievery and art and loyalty in Japanese-occupied Korea, is positively Shakespearean in scope and splendor. The relationships, the characters, the turns and reversals: In two hours and 48 minutes, not a single one wasted, the film gives you everything you didn’t know you wanted. Didn’t know you needed. Nothing condescends to your intelligence; everything feels earned. (The octopus Choi Min-sik eats alive in Oldboy gets a kind of revenge here, in the movie’s creepiest reveal.) Much of the time, not even Shakespeare plays feel Shakespearean. That quality has more to do with the enlargement of our spirit. The Handmaiden might just makes yours burst. —Jason Kehe\\nArrival (2016)\\nLouise Banks (Amy Adams) cuts a strange figure in the gallery of science-fiction movie heroes. She isn’t a military man or Chosen One or a spacefarer of any kind. She’s a linguistics professor tasked with mastering a stunningly strange alien alphabet, and she is wondrous. Without ever being preachy or dull, Arrival dares to put academic research at the center of a blockbuster, and it unfolds as the most thoughtful sci-fi story of the decade. It doesn’t imagine a future where humanity dominates the galaxy. It doesn’t battle or conquer. Instead, the movie revolves entirely around a quest to communicate with the huge, seven-legged aliens who have landed in 12 locations on Earth in enormous, enigmatic spacecrafts. With Banks, Arrival reveals that understanding a people so alien requires a great deal of humanity. —Emma Grey Ellis\\nGet Out (2017)\\nJordan Peele wasn’t always a rising horror master—a nimble, stylish experimentalist able to fuse the frictions of the modern world (racial strife, class immobility) with genre touchstones (notice how he slickly remixed the final girl trope in Us). With Get Out, Peele’s 2017 breakout vehicle, he all but revolutionized the conventions of horror, journeying deep into the twisted interior of our minds and projecting what many black people had long suspected but feared saying aloud: Some white people are fucking crazy. On its face, the story of Chris Washington (Daniel Kaluuya) and Rose Armitage (Allison Williams) is a simple one. Guy meets girl. Girl invites guy to meet her family for the weekend. Only, the Armitages aren’t just any white liberal American family (or are they?!?)—they’re body-harvesting psychos who kidnap black people and sell them to the highest bidder. The film, like the best of the genre, bent toward reality. It was a social thriller high on racial paranoia but anchored in everyday dread. Get Out was more than a box-office success; with the film, Peele became his own Dr. Frankenstein, injecting the genre with fresh nuance and ultimately showing that horror could be more than what we had come to expect. —Jason Parham\\nStar Wars: The Last Jedi (2017)\\nShut up, internet. Just shut it. Your poseury is showing. If you hated this movie, if you rage all over the forums in despair over this “betrayal,” you’re a faker. You’re not a real fan. Simple. Oh, you might think you are. You grew up with these movies. You know the name of every Jedi on the Council, even that fish-faced one. But you missed it. The whole point. The spirit of the enterprise. Like Empire before it, Last Jedi did what every worthy midpoint in a trilogy is supposed to do: blow shit up. Lop off some body parts. Take chances on a side quest that’s maybe more narrative convenience than coherent thematic enrichment—but who cares! Otherworldly casinos and stampeding space horses! Also, that silent scene where Laura Dern does the suicidal slice took guts none of you haters have. So don’t take out your personal unhappiness and shrinking self-worth on Rian Johnson’s awesome, expansive contribution to the franchise—the best, indeed, since Empire. Get offline. Take a walk. Maybe go as far as an unlocatable island in the middle of the ocean, where you can meditate on your failures, Luke-like, for the rest of time. —Jason Kehe\\nThor: Ragnarok (2017)\\nIn this decade of superhero blockbusters, we’ve spent a lot of time puzzling over what superhero movies should be. One answer is empowering, and for that we have movies like Black Panther and Wonder Woman and Spider-Man: Into the Spiderverse on this list. The other answer is fun. Hence, Thor: Ragnarok, which is absolutely the funnest (and queerest) superhero movie of the decade. Stars Chris Hemsworth and Tessa Thompson are indispensable, but much of the credit must go to the comedic chops of director Taika Waititi, who took a Norse myth about the end of the world and made it a psychedelic space romp set to Led Zeppelin. —Emma Grey Ellis\\nThe Shape of Water (2017)\\nDecades from now, The Shape of Water will likely be remembered for two things: (1) earning Guillermo del Toro a much-deserved Oscar for directing, and (2) fish sex. What it should be remembered for, though, is being an utterly transfixing love story between a woman and a fish that ended up being the most effective film of 2017. At the end of that year, I wrote that Shape of Water was “a sensitive examination of how society treats ‘the other’ and a wonderful testament to the fact that love can, truly, take any form.” It was just as true then as it is now. —Angela Watercutter\\nWonder Woman (2017)\\nI’m not going to use this space to re-litigate the bleakness of the DC movie universe (it’s bleak, go ahead and @ me), but if there has been one shining light in the whole morass, it was Wonder Woman. Directed by Patty Jenkins and starring Gal Gadot as Diana Prince, it was everything Justice League et al. were not: cunning, fun (and funny), light on its feet, full of purpose and rhythm, enjoyable. Plenty a thinkpiece has been written about the importance of the first female-led superhero movie, and those are valid, but more than anything, Wonder Woman just succeeded at being an excellent romp that just happened to feature a Themysciran demigoddess. —Angela Watercutter\\nBlack Panther (2018)\\nTo truly encapsulate the greatness of director Ryan Coogler’s Black Panther, I’m going to have to borrow a sentiment from my colleague Jason Parham: “What should a superhero movie be? What can it be? With Black Panther, we finally have an answer worthy of our time.” As he pointed out then, prior to T’Challa, black superheroes were never given the same cinematic deification as their white counterparts, whether they were billionaire science bros like Tony Stark or Norse gods like Thor. On the stage of Black Panther, T’Challa was given the opportunity not only to lead one of the best superhero movies of the decade but also to lead a movie that almost effortlessly weaved in Marvel heroics, black cultural touchstones, and commentary on colonialism. It was a marvel to behold. —Angela Watercutter\\nAnnihilation (2018)\\nWe could’ve picked Alex Garland’s other sci-fi stunner from this decade, Ex Machina, and slept soundly. It’s a fantastic film—smart, subversive, with eminently welcome hip swivels by Oscar Isaac. But it’s still about (the terrors of) AI, a not unfamiliar obsession of the genre. Comparatively, Annihilation has no touchpoints, nothing for us to hold onto as it thrusts us into a surrealist eco-nightmare, with five women (led by Natalie Portman) as our troubled guides. Based on the first book in Jeff VanderMeer’s Southern Reach Trilogy, the film is a true act of adaptation. Garland, one of our most committed auteurs, said he didn’t even reread the book to prepare; he made the movie based on his sense memories, his impressions, of VanderMeer’s foreboding themes. Let it wash over you, the startling imagery and grotesqueries—shrines to an alien wilderness. You most certainly will not sleep soundly. —Jason Kehe\\nSpider-Man: Into the Spider-Verse (2018)\\nAlright, I’ll say it: Spider-Man has always been my least favorite superhero. He’s dull as a muddy puddle, a teenage boy with a bug bite who is puppyish at best and insufferably emo at worst. Then I saw Spider-Man: Into the Spider-Verse. In the words of my colleague Angela Watercutter, “after umpteen versions of Peter Parker, the new animated feature gives fans the multidimensional hero they deserve.” Spider-Verse centers around a lesser known (but much beloved) Spider-Man, Myles Morales, an Afro-Latino teenager who, like all the other Spideys who suddenly crash-land in his universe, has become a web-slinging vigilante after being bitten by a radioactive spider. The box office went mad for this one for a reason: It’s funny, it’s stunningly animated, and it’s unquestionably the future of Spider-Man. —Emma Grey Ellis\\nSorry to Bother You (2018)\\nBoots Riley’s late-capitalist debut feature, Sorry to Bother You, is as disturbed as they come. But gloriously so. The veteran activist and former rapper expertly flips an age-old American custom—the exploitation of labor—into a surreal joy ride starring some of the decade’s most alluring talent (LaKeith Stanfield, Tessa Thompson, Armie Hammer). What begins as a painless chronicle of a young man trying to scrounge up rent money blossoms into a complex racial allegory about class and the ills of society. A credit to its razor-smart script, the film unpacked the perversions of human capital—the gig economy, mass incarceration—and hinted at a reality that doesn’t feel too far off from the one we inhabit now. Best of all, Sorry to Bother You was unblinking in its approach, brazenly investigating the question of black futurity. It proposed one fundamental question: Who gets control in the future? The answer was as hair-raising as it was hilarious. —Jason Parham\\nParasite (2019)\\nWe won’t reiterate our entire Best Movies of 2019 list here, but we will say that writer-director Bong Joon Ho’s Parasite was awesome. You should watch it. —Angela Watercutter',\n",
              "  array([-2.6709108, -2.6661909, -0.149319 ], dtype=float32),\n",
              "  2),\n",
              " ('All manner of startups fail for all manner of reasons. But there’s one constant: this is an incredibly difficult business. Launching a successful company isn’t just a matter of drive and finding the right people (though both, clearly, are important). Doing well in this business requires the stars to align perfectly on a billion different things.\\nA cursory look at this year’s batch of companies doesn’t find any story quite as spectacular as last year’s big Theranos flameout, which gave us a best-selling book, documentary, podcast series and upcoming Adam McKay/Jennifer Lawrence film. Some, like MoviePass, however, may have come close.\\nAnd for every Theranos, there are dozens of stories of hardworking founders with promising products that simply couldn’t make it to the finish line. There’s also room for debate about what is and isn’t a startup. For our purposes, we’re focusing here on independent startups, not digital initiatives from larger companies — though in at least one case, the startup was acquired by a larger company before shutting down.\\nSo without further ado, here are some of the biggest and most fascinating startups that closed up shop in 2019. \\nAnki (2010 – 2019)\\nTotal raised: $182 million\\nIn 2013, a promising young hardware startup showcased a new generation of slot cars onstage at the World Wide Developer Conference keynote. It was quite an honor for a young company. Apple was clearly impressed with how Overdrive pushed the limits of what could be done on the iPhone.\\nThree years later, Anki released Cozmo. The plucky little robot was the result of large investment, including the hiring of ex-Pixar and Dreamworks animators brought on board to craft a high range of emotions in the robot’s eyes. In late 2018, the company launched the similar but adult-focused Vector robot. By April 2019, Anki had shut its doors, in spite of selling 1.5 million robots and “hundreds of thousands” of Cozmo models.\\nChariot (2014 – 2019)\\nTotal raised: $3 million, acquired by Ford in 2017\\nChariot was a shuttle startup hoping to reinvent mass transit with a fleet of vans for commuters. The routes, supposedly, were determined based on a “crowdsourced” vote.\\nAfter acquiring the service two years ago, Ford shut it down at the beginning of 2019. The company didn’t offer many details, except to say that “in today’s mobility landscape, the wants and needs of customers and cities are changing rapidly.”\\nDaqri (2010 – 2019)\\nTotal raised: $132 million\\nDaqri, another high-flying, heavily funded AR headset business, shut its doors around September and completed an asset sale. The company is one of many in the sector that failed to succeed in its efforts to court enterprise customers, as well as in its efforts to compete with Magic Leap, Microsoft and others.\\nDaqri was, at one point, speaking with a large private equity firm about financing ahead of a potential IPO, but as the technical realities facing other AR companies came to light, the firm backed out and the deal crumbled, according to earlier TechCrunch reporting. Sadly, Daqri wasn’t the only AR business to crumble this year.\\nHomeShare\\nTotal raised: $4.7 million\\nHomeShare tried to deal with the challenge of rapidly rising housing costs by matching roommates who shared apartments split into “micro-rooms.” The company said that as of March, it had about 1,000 active residents.\\nAs part of the shutdown, HomeShare said residents would not be getting back the deposits for their partitions — but they would be able to keep the divider or sell it.\\nJibo (2012 – 2018/19)\\nTotal raised: $72.7 million\\nBetween Anki and Jibo, you could say it was a tough year for consumer social robots. But then, there’s never been a great year for the category. Not yet, at least. Like the sad death of the original Aibo before it, Jibo’s end was punctuated by the incredibly depressing nature of watching an adorable robot friend draw its final breath. Jibo did just that in April, telling consumers, “I want to say I’ve really enjoyed our time together. Thank you very, very much for having me around.”\\nJibo technically died in late-2018, but we’re making an exception due to the dramatic nature of its demise. The end came in spite of a successful crowdfunding campaign and a healthy amount of venture capital raised. In spite of it all, the startup was forced to lay off most of its staff and then, ultimately, send Jibo upstate to live on the robo-farm.\\nMoviePass (2011 – 2019)\\nTotal raised: $68.7 million, acquired by Helios and Matheson in 2017\\nImage: Bryce Durbin / TechCrunch\\nHoly hell. Where to even start with this one? When we were putting this list together, one TechCruncher remarked that he swore MoviePass shut down years ago. That’s because (not unlike some current political events), the ticket subscription service’s magnificent train wreck of a demise appeared to unfold over the course of several years, in excruciating slow motion. We wrote a lot about it. A lot, a lot.\\nIn fact, there seemed to be a new disaster every week, as the company hemorrhaged money, limited its service, experience outages, borrowed even more money, was forced to enter a kind of zombie state and had a massive data breech. Oh, and then there was the John Gotti movie it financed that was arguably even worse. By the end of it all, MoviePass’ ultimate demise almost felt like an act of mercy.\\nMunchery (2010 – 2019)\\nTotal raised: $125 million\\nOne of the first startup scandals of 2019 involved a once well-known meal delivery startup, Munchery . After the business emailed its customers notifying them of its imminent shutdown, its vendors came forward with a slew of accusations. Namely, the food delivery startup took advantage of them in its final hours, knowingly allowing them to continue making deliveries it couldn’t pay for.\\nThe company’s sudden demise sparked a debate around accountability. While the CEO and its venture capital investors stayed largely silent, its vendors cried out for an explanation and even protested outside the offices of Sherpa Capital, one of Munchery’s backers, in search of answers and payments.\\nNomiku (2012 – 2019)\\nTotal raised: $145,000\\nOne of the most recent additions to this list, Bay Area-based food startup Nomiku called it quits earlier this month. The company helped pioneer the consumer sous vide category, only to see the market flooded by competing devices. In multiple successful Kickstarter campaigns totaling $1.3 million, backing from Samsung Ventures and an attempted pivot into meal plans, the startup just couldn’t survive.\\n“The total climate for food tech is different than it used to be,” CEO Lisa Fetterman told TechCrunch. “There was a time when food tech and hardware were much more hot and viable. I think a company can survive a few hurdles, and a few challenges [ …] For me, it was the perfect storm of all these things.”\\nODG (1999 – 2019)\\nTotal raised: $58 million\\nA pioneer in the AR glasses space, news emerged of Osterhout Design Group’s (ODG) demise in the first few weeks of January. Only a couple of years ago, the company raised a $58 million financing — less than a year later, it had burned through its funding and couldn’t pay employees. By early 2018, ODG had lost half of its workforce as it sought loans to pay back employees. By early 2019, only a skeleton crew awaited a patent sale after acquisitions from several large tech companies, including Facebook and Magic Leap, fell through.\\n“I hope Magic Leap is a huge success. I want everyone in AR to be a huge success,” Osterhout said in an interview with TechCrunch in 2017. “[Augmented reality] is going to be transformative.”\\nOmni (2014 – 2019)\\nTotal raised: $35.3 million\\nThe startup began as a physical storage company, then tried to pivot after selling off its physical storage operations to competitor Clutter in May — it tried, unsuccessfully, to build a white-label software platform that would allow brick-and-mortar merchants to operate their own businesses for renting and selling products.\\nAs part of the shutdown, roughly 10 Omni engineers were hired by Coinbase.\\nScaled Inference (2014 – 2019)\\nTotal raised: $17.6 million \\nFounded by former Googlers Olcan Sercinoglu and Dmitry Lepikhin, Scaled Inference made headlines in 2014 with a plan to build machine learning and artificial intelligence technology similar to what’s used internally by companies like Google, and making it available as a cloud service that can be used by anyone. The ambitions were grand and attracted investors like Felicis Ventures, Tencent and Khosla Ventures.\\nUnfortunately, the company was forced to call it quits recently. Former CEO Sercinoglu tells us the shutdown was a result of a lack of funding due to insufficient commercial traction. “We were working on various options until the last minute and retained the team as long as we could, but it did not work out. On the plus side, we were able to be transparent with the team throughout the process,” he said.\\nSinemia (2015 – 2019)\\nTotal raised: $1.9 million\\nIt was a rough year for MoviePass -style movie ticket subscription services in general. Sinemia seemed at first to be a more sustainable competitor, but it was plagued by subscriber complaints and even lawsuits around app issues, hidden charges and policies for shuttering accounts.\\nIn April, the company announced that it was ending U.S. operations. To be clear, it did not say that it was shutting down entirely (much of its staff was based in Turkey), but the company’s website has since gone offline. If Sinemia survives in some form, it has disappeared from view.\\nUnicorn Scooters (2018 – 2019)\\nTotal raised: $150,000\\nUnicorn Scooters was one of the first fatalities of the electric scooter craze of 2018, though certainly not the last. As the story goes, the business spent way too much money on Facebook and Google ads; the startup quickly shut down with no money left over to issue refunds for more than 300 of its $699 scooters that had been ordered.\\nThe not-so-aptly named Unicorn had completed the Y Combinator startup accelerator only a few months before it called it quits, likely making it one of the fastest YC grads to shutter post-graduation. “Unfortunately, the cost of the ads were just too expensive to build a sustainable business,” Unicorn’s CEO Nick Evans wrote, according to The Verge. “And as the weather continued to get colder throughout the US and more scooters from other companies came on to the market, it became harder and harder to sell Unicorns, leading to a higher cost for ads and fewer customers.”\\nVreal (2015 – 2019)\\nTotal raised: $15 million\\nvia @VrealOfficial twitter\\nVreal was an ambitious game-streaming platform that aimed to let VR users explore the worlds in which live-streamers were playing. Those users could walk around streamers as avatars, or they could explore on their own as passive observers while listening to the live-streamer blast their way through zombies.\\n“Unfortunately, the VR market never developed as quickly as we all had hoped, and we were definitely ahead of our time,” the company said in a blog post. “As a result, Vreal is shutting down operations and our wonderful team members are moving on to other opportunities.”',\n",
              "  array([-3.6211658 , -1.9993088 , -0.17695242], dtype=float32),\n",
              "  2),\n",
              " ('Lab-grown mini brains could soon outsmart us\\nby THE CONVERSATION — in SYNDICATION\\n75\\nSHARES\\nThe cutting-edge method of growing clusters of cells that organize themselves into mini versions of human brains in the lab is gathering more and more attention. These “brain organoids,” made from stem cells, offer unparalleled insights into the human brain, which is notoriously difficult to study.\\nBut some researchers are worried that a form of consciousness might arise in such mini-brains, which are sometimes transplanted into animals. They could at least be sentient to the extent of experiencing pain and suffering from being trapped. If this is true – and before we consider how likely it is – it is absolutely clear in my mind that we must exert a supreme level of caution when considering this issue.\\nBirds are cool\\nEarly birds are even cooler. Get cheap tickets to TNW2020 right now\\nYEAH\\nBrain organoids are currently very simple compared to human brains and can’t be conscious in the same way. Due to a lack of blood supply, they do not reach sizes larger than around five or six millimeters. That said, they have been found to produce brain waves that are similar to those in premature babies. A study has showed they can also grow neural networks that respond to light.\\nThere are also signs that such organoids can link up with other organs and receptors in animals. That means that they not only have a prospect of becoming sentient, they also have the potential to communicate with the external world, by collecting sensory information. Perhaps they can one day actually respond through sound devices or digital output.\\nAs a cognitive neuroscientist, I am happy to conceive that an organoid maintained alive for a long time, with a constant supply of life-essential nutrients, could eventually become sentient and maybe even fully conscious.\\nTime to panic?\\nThis isn’t the first time biological science has thrown up ethical questions. Gender reassignment shocked many in the past, but, whatever your beliefs and moral convictions, sex change narrowly concerns the individual undergoing the procedure, with limited or no biological impact on their entourage and descendants.\\nGenetic manipulation of embryos, in contrast, raised alert levels to hot red, given the very high likelihood of genetic modifications being heritable and potentially changing the genetic makeup of the population down the line. This is why successful operations of this kind conducted by Chinese scientist He Jianku raised very strong objections worldwide.\\nHuman cerebral organoids range in size from a poppy seed to a small pea. NIH/Flickr\\nBut creating mini-brains inside animals, or even worse, within an artificial biological environment, should send us all frantically panicking. In my opinion, the ethical implications go well beyond determining whether we may be creating a suffering individual. If we are creating a brain – however small –– we are creating a system with a capacity to process information and, down the line, given enough time and input, potentially the ability to think.\\nSome form of consciousness is ubiquitous in the animal world, and we, as humans, are obviously on top of the scale of complexity. While we don’t know exactly what consciousness is, we still worry that human-designed AI may develop some form of it. But thought and emotions are likely to be emergent properties of our neurons organized into networks through development, and it is much more likely it could arise in an organoid than in a robot. This may be a primitive form of consciousness or even a full blown version of it, provided it receives input from the external world and finds ways to interact with it.\\nIn theory, mini-brains could be grown forever in a laboratory – whether it is legal or not – increasing in complexity and power for as long as their life-support system can provide them with oxygen and vital nutrients. This is the case for the cancer cells of a woman called Henrietta Lacks, which are alive more than 60 years after her death and multiplying today in hundreds of thousands of labs throughout the world.\\nDisembodied super intelligence?\\nBut if brains are cultivated in the laboratory in such conditions, without time limit, could they ever develop a form of consciousness that surpasses human capacity? As I see it, why not?\\nAnd if they did, would we be able to tell? What if such a new form of mind decided to keep us, humans, in the dark about their existence – be it only to secure enough time to take control of their life-support system and ensure that they are safe?\\nWhen I was an adolescent, I often had scary dreams of the world being taken over by a giant computer network. I still have that worry today, and it has partly become true. But the scare of a biological super-brain taking over is now much greater in my mind. Keep in mind that such new organism would not have to worry about their body becoming old and dying, because they would not have a body.\\nThis may sound like the first lines of a bad science fiction plot, but I don’t see reasons to dismiss these ideas as forever unrealistic.\\nThe point is that we have to remain vigilant, especially given that this could all happen without us noticing. You just have to consider how difficult it is to assess whether someone is lying when testifying in court to realise that we will not have an easy task trying to work out the hidden thoughts of a lab grown mini-brain.\\nSlowing the research down by controlling organoid size and life span, or widely agreeing a moratorium before we reach a point of no return, would make good sense. But unfortunately, the growing ubiquity of biological labs and equipment will make enforcement incredibly difficult – as we’ve seen with genetic embryo editing.\\nIt would be an understatement to say that I share the worries of some of my colleagues working in the field of cellular medicine. The toughest question that we can ask regarding these mesmerizing possibilities, and which also applies to genetic manipulations of embryos, is: can we even stop this?',\n",
              "  array([-3.101826  , -3.8789926 , -0.06789208], dtype=float32),\n",
              "  2),\n",
              " ('5 milestones that shaped 50 years of internet history\\nby THE CONVERSATION — 25 days ago in SYNDICATION\\n102\\nSHARES\\nFifty years ago, a UCLA computer science professor and his student sent the first message over the predecessor to the internet, a network called ARPANET.\\nOn Oct. 29, 1969, Leonard Kleinrock and Charley Kline sent Stanford University researcher Bill Duval a two-letter message: “lo.” The intended message, the full word “login,” was truncated by a computer crash.\\nVolume 0%\\nEarly birds get all the good stuff\\nLike a great price on TNW2020 tickets\\nGIMME\\nMuch more traffic than that travels through the internet these days, with billions of emails sent and searches conducted daily. As a scholar of how the internet is governed, I know that today’s vast communications web is a result of governments and regulators making choices that collectively built the internet as it is today.\\nHere are five key moments in this journey.\\n1978: Encryption failure\\nEarly internet pioneers, in some ways, were remarkably farsighted. In 1973, a group of high school students reportedly gained access to ARPANET, which was supposed to be a closed network managed by the Pentagon.\\nComputer scientists Vinton Cerf and Robert Kahn suggested building encryption into the internet’s core protocols, which would have made it far more difficult for hackers to compromise the system.\\nBut the U.S. intelligence community objected, though officials didn’t publicly say why. The only reason their intervention is public is because Cerf hinted at it in a 1983 paper he co-authored.\\nAs a result, basically all of today’s internet users have to handle complex passwords and multi-factor authentication systems to ensure secure communications. People with more advanced security needs often use virtual private networks or specialized privacy software like Tor to encrypt their online activity.\\nHowever, computers may not have had enough processing power to effectively encrypt internet communications. That could have slowed the network, making it less attractive to users – delaying, or even preventing, wider use by researchers and the public.\\nVinton Cerf and Robert Kahn with President George W. Bush at the ceremony where Cerf and Kahn were given the Presidential Medal of Freedom for their contributions to developing the internet. Paul Morse/White House/Wikimedia Commons\\n1983: ‘The internet’ is born\\nFor the internet to really be a global entity, all kinds of different computers needed to speak the same language to be able to communicate with each other – directly, if possible, rather than slowing things down by using translators.\\nHundreds of scientists from various governments collaborated to devise what they called the Open Systems Interconnection standard. It was a complex method that critics considered inefficient and difficult to scale across existing networks.\\nCerf and Kahn, however, proposed another way, called Transmission Control Protocol/Internet Protocol. TCP/IP worked more like the regular mail – wrapping up messages in packages and putting the address on the outside. All the computers on the network had to do was pass the message to its destination, where the receiving computer would figure out what to do with the information. It was free for anyone to copy and use on their own computers.\\nTCP/IP – given that it both worked and was free – enabled the rapid, global scaling of the internet. A variety of governments, including the United States, eventually came out in support of OSI but too late to make a difference. TCP/IP made the internet cheaper, more innovative and less tied to official government standards.\\n1996: Online speech regulated\\nBy 1996, the internet boasted more than 73,000 servers, and 22% of surveyed Americans were going online. What they found there, though, worried some members of Congress and their constituents – particularly the rapidly growing amount of pornography.\\nIn response, Congress passed the Communications Decency Act, which sought to regulate indecency and obscenity in cyberspace.\\nThe Supreme Court struck down portions of the law on free-speech grounds the next year, but it left in place Section 230, which stated: “No provider or user of an interactive computer service shall be treated as the publisher or speaker of any information provided by another information content provider.”\\nThose 26 words, as various observers have noted, released internet service providers and web-hosting companies from legal responsibility for information their customers posted or shared online. This single sentence provided legal security that allowed the U.S. technology industry to flourish. That protection let companies feel comfortable creating a consumer-focused internet, filled with grassroots media outlets, bloggers, customer reviews and user-generated content.\\nCritics note that Section 230 also allows social media sites like Facebook and Twitter to operate largely without regulation.\\n1998: US government steps up\\nThe TCP/IP addressing scheme required that every computer or device connected to the internet have its own unique address – which, for computational reasons, was a string of numbers like “192.168.2.201.”\\nBut that’s hard for people to remember – it’s much easier to recall something like “indiana.edu.” There had to be a centralized record of which names went with which addresses, so people didn’t get confused, or end up visiting a site they didn’t intend to.\\nOriginally, starting in the late 1960s, that record was kept on a floppy disk by a man named Jon Postel. By 1998, though, he and others were pointing out that such a significant amount of power shouldn’t be held by just one person. That year saw the U.S. Department of Commerce lay out a plan to transition control to a new private nonprofit organization, the Internet Corporation for Assigned Names and Numbers – better known as ICANN – that would manage internet addresses around the world.\\nFor nearly 20 years, ICANN did that work under a contract from the Commerce Department, though objections over U.S. government control grew steadily. In 2016, the Commerce Department contract expired, and ICANN’s governance continued its shift toward a broader, more globalized structure.\\nOther groups that manage key aspects of internet communications have different structures. The Internet Engineering Task Force, for instance, is a voluntary technical organization open to anyone. There are drawbacks to that approach, but it would have lessened both the reality and perception of U.S. control.\\nThis 2007 photo shows an Iranian nuclear enrichment facility in Natanz, which was apparently the target of the first known cyberweapon to cause physical damage. AP Photo/Hasan Sarbakhshian\\n2010: War comes online\\nIn June 2010, cybersecurity researchers revealed the discovery of a sophisticated cyber weapon called Stuxnet, which was designed specifically to target equipment used by Iran’s effort to develop nuclear weapons. It was among the first known digital attacks that actually caused physical damage.\\nAlmost a decade later, it’s clear that Stuxnet opened the eyes of governments and other online groups to the possibility of wreaking significant havoc through the internet. These days, nations use cyberattacks with increasing regularity, attacking a range of military and even civilian targets.\\nThere’s certainly cause for hope for online peace and community, but these decisions – along with many others – have shaped cyberspace and with it millions of people’s daily lives. Reflecting on those past choices can help inform upcoming decisions – such as how international law should apply to cyberattacks, or whether and how to regulate artificial intelligence.\\nMaybe 50 years from now, events in 2019 will be seen as another key turning point in the development of the internet.',\n",
              "  array([-4.245038  , -3.3826013 , -0.04949933], dtype=float32),\n",
              "  2),\n",
              " ('TOKYO (Reuters) - Japan’s SoftBank Group Corp (9984.T) said Tadashi Yanai, founder and CEO of Uniqlo parent Fast Retailing (9983.T), will resign as independent board member at the end of the month after 18 years on the job to focus on his fashion business.\\nFILE PHOTO: Tadashi Yanai, chairman and Chief Executive Officer of Fast Retailing Co., attends a joint news conference in Tokyo, Japan, April 7, 2016. REUTERS/Yuya Shino\\nA longtime ally and sometime critic of SoftBank founder and CEO Masayoshi Son, the billionaire is one of only three external members of a board filled with SoftBank executives and heads of its portfolio companies.\\nThe resignation comes at a time when SoftBank is battling with the fallout from the failed IPO of WeWork, with Son saying he misjudged co-founder Adam Neumann’s character and pledging to strengthen corporate governance at the group’s investments.\\nHowever, experts are critical of SoftBank’s governance, saying it has few truly independent voices that can question Son’s judgement.\\n“They have low governance standards,” said Nicholas Benes of The Board Director Training Institute of Japan, a non-profit focused on corporate governance training.\\n“If they don’t require higher standards of themselves, it might be hard to require them of investee companies,” he said in an interview last month.\\nSon in November defended the board’s rigour after reporting the group’s first quarterly loss in 14 years, saying that Yanai was among the board members who excoriated him for the WeWork investment.\\n“Almost all the board members gave me a hard time. I ended up being very exhausted,” he said.\\nYanai, Japan’s richest man, along with Son, are among a handful of Japanese founder-CEOs who are also household names.\\nYanai was known for being willing to voice his dissent at some of Son’s decisions.\\nHis successor has not been decided, a SoftBank spokeswoman said.\\nSoftBank lost one of its most outspoken voices when another outside director, Shigenobu Nagamori, founder and CEO of Nidec Corp (6594.T), stepped down two years ago.\\nThe other independent directors currently on SoftBank’s board are Masami Iijima, chairman of trading house Mitsui & Co (8031.T), and professor Yutaka Matsuo from the University of Tokyo, a highly renowned artificial intelligence expert with little corporate experience.\\nBoth Yanai and Son have made and then shelved plans to hand over the reins of their companies in the past, with 70-year-old Yanai previously saying he would retire at 65.\\nYanai has said he does not want either of his two sons to take over as CEO, but both were promoted and joined the ranks of company directors last year.\\nPossible successors named by media have included finance chief Takeshi Okazaki and Pan Ning, the head of Uniqlo’s China operations, as well as Maki Akaida, head of Uniqlo’s Japan operations.',\n",
              "  array([-3.726436 , -2.12692  , -0.1546469], dtype=float32),\n",
              "  2),\n",
              " ('LONDON (Reuters) - Helicopter cash, climate crises, smart cities and the space economy — investors have all those possibilities ahead as they enter the third decade of the 21st century.\\nThey go into the new decade with a spring in their step, after watching world stocks add over $25 trillion in value in the past 10 years and a bond rally put $13 trillion worth of bond yields below zero.\\nThey also saw internet-based firms transform the way humans work, shop and relax. Now investors are positioning for the tech revolution’s next 10 years.\\nCould we see a repeat of the roaring twenties, as the 1920s were known — years of prosperity, technological innovation and such social developments as women winning the right to vote?\\nPossibly. But there’s unease, along with all the euphoria. The current economic cycle is already the longest in U.S. history and a recession looks inevitable in the new decade — which also will mark 100 years since the Wall Street crash of 1929.\\n(Graphic: U.S. economic growth click, here)\\nAnd solutions may need to be unconventional, even more so than the extraordinary policies of negative interest rates and bond-buying that eased the post-2008 global funk.\\nWith those policies maxed out, “in the 2020s it seems inevitable that a world of helicopter money awaits,” Deutsche Bank predicts.\\nThat would entail central banks or governments providing citizens with large amounts of money, as though it was being dropped from helicopters, a strategy rejected even by the unorthodox policymakers of the 2010s.\\nAnother radical option under discussion is modern monetary theory, when governments create and spend as much money as needed, so long as inflation stays low.\\n“Central banks have effectively invited governments to experiment with more unconventional policies,” Deutsche said. However, those policies may pile up even more global debt, already at record highs.\\n(Graphic: On the rise: U.S. debt-to-GDP ratio click, here)\\nSo what will markets do?\\nA decade of rock-bottom interest rates didn’t revive growth and inflation in developed nations, but they certainly inflated markets, as prices for bonds, equities and real estate show.\\nThe inequality they spawned have also triggered a widespread backlash against globalization. The result is a de-globalizing world, or as Morgan Stanley puts it, “slow-balisation”.\\nThe bank expects tech investments to outperform, in particular smaller internet firms in China, as protectionism hurts larger rivals.\\nBut it predicts less exciting returns — “a lower and flatter frontier compared to prior decades, and especially compared to the ten years post-GFC (global financial crisis).”\\nGLOBAL WARMING, PEOPLE AGEING\\nAs market returns cool, the planet will continue to heat up. Carbon emissions, temperatures, sea levels and thus climate-induced poverty and immigration are expected to rise.\\nThat should increasingly lead asset managers to seek alternatives to pollutants, especially coal, use of which must cease in OECD nations by 2030 for the Paris Agreement to be met.\\nBofA expects clean energy and electric-vehicle companies to emerge as winners, estimating the clean energy market to be already worth $300 billion.\\nAgeing populations are another challenge, making demographics a key investment criterion. Deutsche Bank names Ireland, Rwanda, Ghana, Botswana and Laos as among the 22 nations in line for a “demographic dividend”, benefiting from growing working-age populations.\\nIt also backed sectors like e-commerce as Generation Z, those who will be in their mid 20s to early 30s by 2030, exercise growing spending power.\\nFILE PHOTO: Traders work on the floor at the New York Stock Exchange (NYSE) in New York, U.S., December 17, 2019. REUTERS/Brendan McDermid\\n(Graphic: Growing old too fast: China’s workforce must support a larger population of elderly dependents Growing old too fast: China’s workforce must support a larger population of elderly dependents click, here)\\nBut in some countries, affluent older spenders will still carry clout. By 2030, over 80s will represent 5.4% of the U.S. population, up from 3.7% in 2015, driving demand for retirement homes, healthcare and long-life innovations.\\n“Immortality may prove the most interesting secular theme in the 2020s,” BofA predicts.\\nTECH TIPPING POINTS\\nA World Economic Forum survey in 2017 predicted a series of \"technological tipping points\" for the coming decade. They included 3D-printed cars, driverless vehicles and the first artificial-intelligence machine on a company\\'s board. (Graphic: Robot Shipments Hit New Record png click, here)\\nThe ‘20s could be an era of smart cities, where big data and robotics ensure better governance, health and connectivity, UBS forecast. It expects annual spending to turn cities smart will reach $2 trillion in 2025 and internet-connected devices will multiply more than four-fold to 46 billion.\\nTo take advantage of these shifts, investors will focus on areas such as autonomous vehicles — automated forklift shipments will grow to 455,000 in 2030 from 4,000 next year, ABI Research said.\\nFinally, advances in rocket and satellite technology are opening investment access to the final frontier. The first exchange-traded fund dedicated to the space industry opened in 2019.\\nUBS sees \"parallels with how the global internet ... opened up vast opportunities at the turn of the century.\" It predicts the \"space economy\" tmsnrt.rs/2YpSX6Z will reach $1 trillion in the next couple of decades, from $340 billion now.\\nThe bank backs existing listed aerospace, satellite and communications companies and new space start-ups in private markets.',\n",
              "  array([-3.2571373, -0.8677049, -0.6132509], dtype=float32),\n",
              "  2),\n",
              " ('If President Trump’s trade deal with Beijing works as planned, Chinese purchases from American manufacturers and farmers will more than double over the next two years and American investors will finally be welcome to own some of China’s financial services companies.\\nYet while the “Phase One” deal suggests the United States and China are drawing closer, the two countries actually are edging toward a partial economic divorce.\\nAway from the trade talks, the Commerce Department is poised to issue new regulations to prohibit exports of advanced technologies such as artificial intelligence to China. An interagency panel chaired by the Treasury Department is intensifying scrutiny of Chinese investments in cutting-edge U.S. companies. And the Justice Department last month announced its latest indictment of a Chinese national accused of pilfering U.S. trade secrets.\\nAD\\n“Selective decoupling is really the unstated policy that’s driving all of this,” said David Hanke, a partner at Arent Fox, who worked on China-related issues while a staffer on the Senate Intelligence Committee.\\nChina likewise is taking steps to extricate itself from a relationship of mutual dependence with the United States. After Chinese telecom giant ZTE was nearly put out of business by U.S. sanctions last year, Chinese President Xi Jinping re-emphasized efforts to reduce China’s reliance upon American high-tech suppliers. He also is pushing a state-backed campaign for Chinese companies to dominate 10 futuristic industries including artificial intelligence and new energy vehicles, with specific sector-by-sector market-share goals.\\nIndeed, tomorrow’s technology is the crux of the slow-motion split. After 40 years of ever-closer economic ties, including collaborating on Internet, computing and telecommunications breakthroughs, officials in Beijing and Washington increasingly see a potential enemy where they once saw a partner.\\nAD\\nU.S. officials worry that depending upon Chinese components could leave critical military, communications and public transit systems vulnerable to sabotage or spying. The administration already has strictly limited the ability of China’s Huawei to buy American parts, barred U.S. government agencies from buying the company’s equipment and sought to persuade U.S. allies to keep the telecom giant out of their most advanced 5G networks.\\nLast month, Commerce Secretary Wilbur Ross recommended evaluating on a case-by-case basis whether proposed purchases of any foreign information, communications and technology gear were in the national interest. The defense spending bill President Trump signed earlier this month includes provisions aimed at avoiding suspect foreign telecommunications equipment, which analysts say are aimed mostly at China.\\n“Trump has set in motion an anti-China train that will not be derailed just because the Trade War has been temporarily settled,” Andrew Collier, managing director of Orient Capital Research, said via email.\\nAD\\nAftershocks from the trade conflict are encouraging multinational corporations to shift portions of their supply chains from China to other low-cost countries, including Vietnam and Malaysia. Continuing uncertainty about future tariff rates may chill business investment “at the expense of global economic growth,” wrote Collier, a Hong Kong-based adviser to institutional investors.\\nThe tougher stance toward China enjoys bipartisan support on Capitol Hill. In a recent speech, Sen. Marco Rubio (R-Fla.), a member of the Senate Foreign Relations Committee, warned that China was seeking to replace the United States as the “world leader in industries such as aerospace, quantum computing and industrial machinery” and needed to be stopped.\\n“For the first time in three decades, we are confronted with a near-peer rival that seeks to displace us militarily, economically, technologically and geopolitically,” Rubio said.\\nAD\\nThe Senate’s top Democrat, Minority Leader Charles E. Schumer of New York, meanwhile, attacked Trump’s limited trade deal as a “surrender to China.”\\nYet moves to decouple the two countries’ technology sectors are being resisted by U.S. industry and its Washington allies. The Commerce Department was expected to make public its export control regulations in August, but officials have been stalling, according to Derek Scissors, a China expert at the American Enterprise Institute, who supports tighter controls.\\nThe agency last year began work on draft regulations designed to limit exports to China of 14 technologies, such as robotics, biotechnology and quantum computing. Now, nongovernment analysts who are tracking the regulations say the first installment, which is expected soon, will focus on just a handful.\\nAD\\n“Commerce’s export control draft is so bad. They’re going to get attacked for it,” Scissors said. “It’s a blatant undermining of the will of Congress. They’re not doing what they’re supposed to.”\\nA department spokesman said only that the rulemaking process was “ongoing.”\\nThe export control regulations help inform the Treasury Department-led Committee on Foreign Investment in the United States (CFIUS), which lawmakers gave new powers last year to reject even non-controlling foreign investments in American companies if they affected national security.\\nPreviously, CFIUS could act only if foreign buyers attempted to gain control of a U.S. enterprise. The focus now is to prevent China from buying knowledge it can’t develop on its own.\\nAt the Justice Department, investigators and prosecutors are in the second year of a “China initiative” designed to thwart Beijing’s efforts to steal American technology secrets. John Demers, the assistant attorney general for national security, describes China’s approach as “rob, replicate and replace.”\\nAD\\nThis year alone, U.S. attorneys have charged Chinese nationals with stealing turbine technologies from General Electric and medical trade secrets for the treatment of pediatric diseases from an Ohio hospital’s research institute.\\nLast month, a federal grand jury in the Eastern District of Missouri returned an indictment of Haitao Xiang, 42, a former Monsanto employee.\\nXiang is accused of stealing a secret Monsanto algorithm called the “Nutrient Optimizer,” part of a cloud-based software program that allowed farmers to track the condition of their fields from a desktop.\\nWhile working for Monsanto, Xiang applied for the Chinese government’s “Hundred Talents Program,” designed to reward Chinese citizens who returned from abroad with valuable skills. Beijing uses the program, which often dangles rewards and future employment, “to encourage employees to steal intellectual property from their U.S. employers,” according to Demers.\\nAD\\nOne day after quitting his job in June 2017, Xiang bought a one-way ticket to China and headed for Chicago O’Hare International Airport, the indictment alleges. Federal agents intercepted him before he could board the aircraft and confiscated a 32-gigabyte memory card containing a proprietary Monsanto algorithm.\\nXiang entered a plea of not guilty to the charges, according to court records. Eric Selig, his St. Louis-based attorney, declined to comment.\\nIf convicted on all counts, the Chinese man faces up to 100 years in prison and fines totaling $21 million.\\nEven as various federal departments toughen their approach, the administration has yet to articulate a consistent strategy toward China. The president has oscillated between accusing China of the “rape” of the U.S. economy and proclaiming China’s Xi a blameless “good friend.”\\nAD\\nA months-long State Department project to produce a statement meant to unite the competing threads of administration thinking on China has yet to bear fruit, leaving room for Treasury to promote investment with a country that the Pentagon regards as an adversary.\\n“It is not a holistic policy; it’s an opportunistic policy,” said Michael Wessel, a member of the U.S.-China Economic and Security Review Commission.\\nDetails of the president’s initial trade deal with China, announced Dec. 13, remain unclear. Robert E. Lighthizer, the president’s chief negotiator, says the agreement will mean a sharp increase in Chinese orders for American farm, energy and manufactured goods of $200 billion over two years.\\nBut the text, which is being translated and given final checks by both sides, has not yet been made public.\\nLighthizer told reporters that the accord was aimed at resetting a troubled trans-Pacific relationship.\\n“I’m not one who thinks you can at this point decouple the economy of the United States from China,” he said. “On the other hand, we do have major technology issues and other issues of market access and the like that we have to deal with. So, hopefully, this is one building block in building up something that will actually lead to the ability for the two systems to work together to mutual benefit. That’s my hope.”\\nThe president has said negotiations over a second agreement, which would grapple with unresolved U.S. demands for far-reaching changes in China’s economic model, will begin “immediately.” But no date for resumed talks has been set, according to Lighthizer.\\nThe administration hopes a “Phase Two” deal would finally force meaningful changes in China’s arsenal of subsidies, preferential treatment for strategic companies and cyber hacking. But Chinese officials have told American corporate executives they do not anticipate quick progress.\\nAfter more than 18 months of Trump’s unpredictable mix of flattery and bullying, the Chinese have concluded they need to redouble their efforts to separate from the United States. Even before Trump took office, Xi sought to spur a domestic semiconductor industry to reduce China’s overwhelming dependence upon American supplies.\\nThat impulse has only intensified following the tortuous trade talks. Earlier this month, the Communist Party gave government offices three years to replace all of their foreign-made computer hardware and software.\\nChina’s “Great Firewall” already keeps staples of the U.S. digital economy such as Facebook, Google and Twitter off-limits to 1.4 billion Chinese citizens. A more comprehensive technology split would sever commercial and scientific bonds that have thickened over decades. Intel, for example, established its first Chinese research center in Beijing in 1998 and started an institute focused on autonomous driving in 2018.\\nGovernment officials rarely mention the “Made in China 2025” program that called for domestic technology companies to supplant foreign rivals. Its aggressive market share targets spooked U.S. and other foreign officials. But even without the publicity, the work continues.\\nThe development of two distinct digital jurisdictions will be driven by national security concerns on both sides, said economist Mary Lovely of Syracuse University. But it will carry enormous economic implications.\\n“Decoupling doesn’t mean what you think it means,” Lovely said. “It means higher costs for the U.S. and fewer jobs here. If Microsoft can’t sell in China, there’ll be fewer jobs here.”',\n",
              "  array([-2.2746084 , -0.14479603, -3.443145  ], dtype=float32),\n",
              "  1),\n",
              " ('These days it’s easy to bemoan the state of innovation and the dynamism coming from America’s cradle of technological development in Silicon Valley.\\nThe same companies that were praised for reimagining how people organized and accessed knowledge, interacted publicly, shopped for goods and services, conducted business, and even the devices on which all of these things are done, now find themselves criticized for the ways in which they’ve abused the tools they’ve created to become some of the most profitable and wealthiest ventures in human history.\\nBefore the decade was even half over, the concern over the poverty of purpose inherent in Silicon Valley’s inventions were given voice by Peter Thiel — a man who has made billions financing the creation of the technologies whose paucity he then bemoaned.\\n“We are no longer living in a technologically accelerating world,” Thiel told an audience at Yale University in 2013. “There is an incredible sense of deceleration.”\\nIn the six years since Thiel spoke to that audience, the only acceleration has been the pace of technology’s contribution to the world’s decline.\\nHowever, there are some investors who think that the next wave of big technological breakthroughs are just around the corner — and that 2020 will be the year that they enter the public consciousness in a real way.\\nThese are the venture capitalists who invest in companies that develop so-called “frontier technologies” (or “deep tech”) — things like computational biology, artificial intelligence or machine learning, robotics, the space industry, advanced manufacturing using 3D printing, and quantum computing.\\nContinued advancements in computational power, data management, imaging and sensing technologies, and materials science are bridging researchers’ ability to observe and understand phenomena with the potential to manipulate them in commercially viable ways.\\nAs a result increasing numbers of technology investors are seeing less risk and more rewards in the formerly arcane areas of investing in innovations.\\n“Established funds will spin up deep tech teams and more funds will be founded to address this market, especially where deep tech meets sustainability,” according to Fifty Years investor, Seth Bannon. “This shift will be driven from the bottom up (it’s where the best founder talent is heading) and also from the top down (as more and more institutional LPs want to allocate capital to this space).”\\nIn some ways, these investments are going to be driven by political necessity as much as technological advancement, according to Matt Ocko, a managing partner at the venture firm DCVC.\\nEarlier this year, DCVC closed on $725 million for two investment funds focused on deep technology investing. For Ocko, the geopolitical reality of continuing tensions with China will drive adoption of new technologies that will remake the American industrial economy.\\n“Whether we like it or not, US-government-driven scrutiny of China-based technology will continue in 2020. Less of it will be allowed to be deployed in the US, especially in areas of security, networking, autonomous transportation and space intelligence,” writes Ocko, in an email. “At the same time, US DoD efforts to streamline procurement processes will result in increasingly tighter partnerships between the DoD and tech sector. The need to bring complex manufacturing, comms, and semiconductor technology home to the US will support a renaissance in distributed manufacturing/advanced manufacturing tech and a strong wave of semiconductor and robotic innovation.”',\n",
              "  array([-0.09285462, -3.214169  , -3.0264964 ], dtype=float32),\n",
              "  0),\n",
              " ('Within the Data Science community, there’s a joke that goes like this:\\n\\nQ: What’s the difference between AI and Machine Learning (ML)?\\nA: If it’s written in Python, it’s ML. If it’s written in PowerPoint, it’s AI.\\n\\nThere are a lot of companies out there that claim to do artificial intelligence (AI). You’ll find the omnipresent initials appended to domain names or within the first few words of a pitch deck, but, frankly, a lot of it is BS. Usually, it’s a set of simple regressions and/or heuristics that require a human monitor, making it more of a semi-automation system. A true, robust AI system enables in-the-moment decision-making with the influx of new data — all without human oversight (*some initial assembly required). More often than not, many solutions are labeled “AI” simply because it excites executives and VCs. \\nDespite the noise that crowds the market, there are, in fact, companies that offer true AI solutions. Take one of the many major autonomous vehicle companies that are dealing with very tight control tolerances: the risk of inaccuracy could mean a major accident. \\nSo, if the legitimate AI companies are mixed in with vaporware like puffed cheese snacks in party mix, how can you identify the good ones?\\nVet the executive team\\nWhen PE or VC firms evaluate startups, it’s not entirely the idea or product that determines an investment decision. A major factor is the executive team — how well they work together, and how their backgrounds prepare them for success. When evaluating an AI/machine learning product, adopt the same process. \\nConsider the backgrounds and expertise of the key contributors to the product. A majority of their backgrounds should fall within a technical domain: Computer Science, Mathematics, Engineering, etc. Another plus: team members with experience in your vertical, which can shape how a solution is deployed and ensure it’s a good fit for your company and processes.\\nAsk some basic technical questions\\nMore often than not, companies with vaporware have not done their technical homework. If the AI product is totally bogus, you can often sniff that out with some basic questions that true AI experts can answer. If needed, bring someone with a machine learning and statistics background to spot BS. \\nI once spoke to a company selling AI for predictive automobile maintenance. A colleague of mine asked how many hidden layers were in their neural network, to which the startup responded “11,000.” \\nThis answer provoked 15 more minutes of awkward conversation and backtracking, because that figure is roughly 500x more hidden layers than the best pre-trained neural networks geared towards object recognition. This one simple question flagged many alarms for our team and needless to say, we ended our discussions. \\nYou should ask questions like:\\nWhat types of models/techniques do you employ?\\nHow do you perform model validation?\\nWhat would you check for in our data to ensure proper models are used?\\nCan your solution integrate with our current systems?\\nConsider the fit within your company\\nIf you’re evaluating a product for your business, it’s not enough to ask someone with a technical background to weigh in on the validity of an AI claim. There should be a team with experience in project management, technical product deployment and/or change management to gauge an AI product’s ability to adapt to your existing systems, processes, and even cultural norms. \\nOther areas of technical/managerial expertise may be needed depending on the problem you are hoping to solve with a particular AI product. For instance, if you’re a manufacturing company looking to integrate AI into your quality control processes, you should also involve Manufacturing and Quality engineers to examine and validate the predicted responses.\\nThe next time you see “AI” on anything (which is inevitable these days), be skeptical by default. There are companies who are truly doing amazing work in this field, but do your homework. The above isn’t a complete checklist, but a good starting point to help you avoid buying into snake oil and creating a big mess at your company.',\n",
              "  array([-2.0811255 , -3.4734032 , -0.16936702], dtype=float32),\n",
              "  2),\n",
              " ('\"I\\'m so 20/20 with my vision, 20/20 with precision,\" sings Charli XCX on her latest album.\\nWe can\\'t claim similar levels of foresight - but here are a few of the musical highlights we\\'re expecting over the next 12 months.\\n1) Adele\\'s drum and bass album\\n\"I\\'m calling my next album ADELE,\" tweeted Lady Gaga in October, apropos of nothing.\\nShe offered no further explanation, so let\\'s wildly speculate she was challenging Adele to finish her fourth album, otherwise she\\'d steal her thunder.\\nAdele\\'s certainly been keeping a low profile since her world tour ended in 2017; only surfacing periodically on Instagram to support Tottenham Hotspur or fangirl over the Spice Girls. But \"sources\" have told the tabloids she\\'s back in the studio, so it must be true.\\nPrivately, the star\\'s been through a painful break-up with her husband Simon Konecki and, on her 31st birthday, she wrote that she\\'d \"changed drastically in the last couple years\".\\nThat\\'s bound to affect her music. Writing on Instagram, she told fans her new album would be \"a drum\\'n\\'bass record to spite you\".\\n2) The new Bond theme\\nThe 25th James Bond film is due for release in April; which means that, somewhere on the planet, one of pop\\'s biggest stars is holed up in a recording studio, trying to find the perfect rhyme for No Time To Die.\\nSheeran, Ed Sheeran is the bookie\\'s favourite to record the title song and, as luck would have it, he\\'s already got one oven ready.\\n\"I\\'ve had a theme tune written for about three years,\" he told Ireland\\'s Late, Late Show in 2017. Pressed to reveal the title, Sheeran added: \"I\\'m not going to say \\'cause someone might steal it, but it\\'s good.\"\\nOther names in the frame include Dua Lipa, Liam Gallagher and Lady Gaga, fresh from the success of her Oscar-winning turn in A Star Is Born. We\\'d like to hear what Lana Del Rey would make of a Bond theme... But the most brilliantly out-there suggestion is that producers could get Bond-in-waiting Idris Elba to crank out the theme song.\\n3) Kanye for President?\\n\"2020, I\\'ma run the whole election,\" bragged Kanye West on Facts, a track from his 2016 Life Of Pablo album.\\nThe mercurial musician has long expressed ambitions for the White House - but is he really prepared to go up against Donald Trump next year?\\nRecent indications suggest not. Releasing his new album, Jesus Is King, the rapper revised his political plans, saying he had his sights set on 2024, not 2020, and that he\\'d be running on a platform of job creation.\\n\"I\\'m not gonna run, I\\'m gonna walk,\" he declared, before informing fans he intended to \"legally change my name to \\'Christian Genius Billionaire Kanye West\\'\".\\n4) Glastonbury\\'s 50th anniversary\\nThe iconic festival celebrates its golden anniversary next year, with Taylor Swift, Paul McCartney and Diana Ross the first artists to be confirmed.\\nOrganiser Emily Eavis says the rest of the line-up will be equally impressive, with \"Pyramid-level\" headliners playing on the festival\\'s second stage.\\nShe\\'s also committed to getting \"as close as we can\" to 50/50 gender split on the bill; while there are \"fresh ideas\" afoot for the festival\\'s outré late-night arenas, Block9, Shangri La and Arcadia.\\nOther acts rumoured for the bill include Kendrick Lamar, Guns N\\' Roses, Ariana Grande, Arctic Monkeys, Madonna and The 1975. Elton John and Fleetwood Mac have been ruled out, however, and Queen won\\'t play because of a dispute over badgers.\\n5) Rihanna\\'s elusive reggae album\\nFor the last four years, Rihanna\\'s done everything except make an album: She\\'s built a fashion empire, launched a ground-breaking make-up range, and expanded her movie career.\\nThis means nothing to her fans, though, who constantly pester her for new music. In April, as Rihanna promoted a new beauty range on Instagram, one commenter complained: \"We want the album, sis.\"\\n\"Well, this is bronzer,\" the star replied.\\nOver the summer, she took trolling to a whole new level - sharing a video of herself in the studio with the caption: \"Behind the scenes of \\'Where\\'s the album?\\'\" Two weeks later, she released a range of T-shirts bearing the legend: \"No More Music\".\\nDespite that, we know there\\'s an album in the works. In November, she told Vogue it\\'d be \"reggae-infused\".\\n\"It\\'s in my blood,\" added the Bajan star. \"Even though I\\'ve explored other genres of music, it was time to go back to something that I haven\\'t really homed in on completely for a body of work.\"\\n6) The ones to watch\\nMedia caption\\nWatch clips from the BBC Sound Of 2020 nominees\\nA cottage industry has sprung up around predicting the new year\\'s best new music, with the Brits\\' Rising Star award, MTV Push and the BBC\\'s own Sound of 2020 all tipping fresh young artists for success.\\nTwo names crop up on all three lists: Celeste, who the NME called \"a once-in-a-generation talent\" and \"the finest British soul singer to emerge in years\"; and Joy Crookes, whose rich R&B grooves fuse her Irish-Bangladeshi heritage with a youthful London aesthetic.\\nIn Scotland, folk-pop artist Gerry Cinnamon is making big waves - selling out Glasgow\\'s 50,000-capacity Hampden Park stadium in a matter of hours, despite not having a record deal. That word-of-mouth success looks set to grow in 2020, as his unvarnished, boisterous songs find a bigger audience.\\nIt\\'s a similar story for Mancunian MC Aitch, who scored a top 10 hit earlier this year with the irresistibly cheeky Taste (Make It Shake). Since then he\\'s collaborated with Ed Sheeran and Stormzy, teeing up expectation for his debut album.\\nKeep an eye out, too, for singer-songwriter Maisie Peters, who knows her way around a confessional pop ballad; and Italian singer/rapper Beba, who released a handful of exhilarating singles in 2019.\\n7) The albums you\\'ll cherry-pick the best songs from, because no-one listens to albums any more\\nThe following musicians will declare their album is \"the ultimate expression of my artistic vision,\" even as we drag all their singles into curated playlist and consign the ropey ballad about their mum to the dustbin.\\nDua Lipa - Future Nostalgia. Inspired by disco and classic pop, Dua Lipa told the BBC her second album would sound \"like a dancercise class\".\\nThe 1975 - Notes On A Conditional Form. Singer Matty Healy told Radio 1 the record would be an ode to \"British nighttime culture\" including \"the beauty of the M25 and going to McDonald\\'s... in a Peugeot 206\".\\nKendrick Lamar - TBC. What do you do when your last album won a Pulitzer Prize? That\\'s the question Kendrick Lamar has to answer as he puts together the follow-up to 2017\\'s Damn. Although he\\'s been pretty quiet for the last 12 months, his appearance on several 2020 festival bills suggests King Kenny might have found the solution.\\nKesha - High Road. After the high drama of her Grammy-nominated Rainbow album, the Tik Tok singer says she\\'s \"reconnected to the unrestrained joy and wildness that\\'s always been a part of me\" for her fourth album.\\nPet Shop Boys - Hotspot. Recorded in Berlin, the album comes in advance of an extensive Greatest Hits tour in May.\\nLana Del Rey - White Hot Forever. When Lana\\'s latest album, Norman F_____ Rockwell came out in August, the star said she\\'d already started work on the follow-up. Enthusing about a new song called Let Me Love You Like A Woman, she said: \"I feel like it\\'s going to be really important\".\\nIdles - Toneland. Details are scarce, but the band announced work had begun on their third album earlier this year.\\nCardi B - TBC. The rapper posted an impressive freestyle on her Instagram in November, calling it, \"just a little something \\'til I finish up the album\". It could be a long wait, though - she\\'s currently in the middle of filming Fast & Furious 9.\\nThe Killers - Imploding The Mirage. After a triumphant headline set at Glastonbury, Las Vegas\\'s finest hit the studio to start work on their sixth album, with titles tentatively including Dying Breed and When Dreams Run Dry. It\\'s due in May, swiftly followed by a UK stadium tour.\\nOther artists with new music on the way include Halsey, The Weeknd, BTS, Haim, SZA, Childish Gambino, Selena Gomez, BlackPink, 21 Savage, The Avalanches, Kendrick Lamar, Bombay Bicycle Club and Justin Bieber.\\n8) The big tours\\n\\nMadonna\\'s Madame X Tour sees her playing smaller, more intimate venues\\nMadonna hits the UK at the end of January for a 14-night residency at the London Palladium.\\nIt\\'s a rare opportunity to see the Queen of Pop up-close and personal, and reviews of the US dates have called it \"a colourful riot\", a \"political spectacle\", and \"sacred yet sleazy - the ultimate Madonna combination\".\\nBe warned, though, the star\\'s been turning up hours late - frequently taking to the stage after 11pm.\\nStormzy\\'s made equally-ambitious plans, with a 55-date tour that includes three nights at London\\'s O2, as well as dates in Asia, Australia, New Zealand, North America and Africa.\\n2019\\'s breakout star Billie Eilish starts her ecologically-friendly Where Do We Go? tour in Miami next March, arriving in the UK for dates in Manchester, Birmingham and London; while Little Mix are heading out on a 21-date UK stadium tour over the summer, topped off with a headline slot at the BST festival in Hyde Park.\\nAlanis Morissette has announced she\\'ll be playing her debut album Jagged Little Pill in full to celebrate its 25th anniversary - although, at the time of writing, dates are restricted to the US.\\nYou\\'ll also have to jet to America if you want to catch the all-star 80s pop cruise: A week-long jaunt around the Caribbean in the company of The B-52\\'s, Berlin, Big Country, Bret Michaels, Bow Wow Wow and other artists whose names don\\'t begin with \"B\".\\nBack in the UK, The Eagles will play their iconic Hotel California album over two nights at Wembley Stadium; while 2020\\'s festival headliners include Kraftwerk (All Points East), Haim (Latitude), Lionel Richie (Isle of Wight), Liam Gallagher (Reading & Leeds) and Tyler, The Creator (Lovebox).\\n9) And don\\'t forget the musicals...\\nOK, so Cats wasn\\'t great - but the success of The Greatest Showman, Rocketman and Bohemian Rhapsody means there\\'s no shortage of movie musicals stampeding towards your local multiplex.\\nLeading the pack is Steven Spielberg\\'s new adaptation of West Side Story, starring Ansel Elgort and Rachel Zegler as Tony and Maria. Rita Moreno, who won an Oscar for her portrayal of Anita in the original film, also returns in a newly-created role.\\nAnother Oscar-winner, Jennifer Hudson, will play Aretha Franklin in a biopic of the Queen Of Soul (pictured above). \"I just hope I make her proud [and] do her justice,\" said the singer, who got the seal of approval from Franklin before her death.\\nFollowing the success of A Star Is Born, Bradley Cooper is directing and starring in a biopic of composer Leonard Bernstein; while Celine Dion is getting the biopic treatment in a French-language film called The Power of Love.\\nAs if that\\'s not enough, Film4 are adapting the hit West End show Everybody\\'s Talking About Jamie for the big screen; and the film version of Lin-Manuel Miranda\\'s kinetic Broadway show In The Heights will be released on 26 June... The same day as Top Gun 2.\\n10) Beethoven turns 250\\nLudwig van Beethoven was born in Germany in 1770 - although, for years, the composer believed he was born in 1772, a ploy by his father to make his prodigious talent seem even more incredible.\\nThere are dozens of events planned to mark the 250th anniversary of his birth, the centrepiece of which is a mammoth, 52-part documentary on Classic FM, presented by John Suchet every Saturday throughout 2020.\\nThe BBC also has major plans, kicking off on 11 January with the European premiere of David Lang\\'s new opera, Prisoner of the State, a dark, futuristic retelling of the story behind Beethoven\\'s opera Fidelio.\\nA week later, the BBC National Orchestra of Wales and WNO Orchestra will recreate the composer\\'s epic, four-hour 1808 benefit concert, where he premiered his 5th and 6th symphonies, as well as his 4th piano concerto, in which he was the soloist.\\nIn Beethoven\\'s native city, Bonn, the Beethoven museum has been renovated in time for the anniversary; while electronic music pioneers Kraftwerk will stage an open-air concert in his honour.\\nAnd, to cap it all off, a team of musicologists and scientists are using Artificial Intelligence to complete the 10th Symphony Beethoven was working on when he died in 1827. Sounds awful.\\nFollow us on Facebook, or on Twitter @BBCNewsEnts. If you have a story suggestion email entertainment.news@bbc.co.uk.',\n",
              "  array([-3.461355 , -1.003129 , -0.5076928], dtype=float32),\n",
              "  2),\n",
              " ('During The Mandalorian‘s eighth chapter, “Redemption,” Mando and his companions fight their way out of a bad situation in an action-packed 49 minutes during Taika Waititi’s Star Wars directorial debut.\\n\\nBut this episode was more about love than violence. When Baby Yoda hugs Mando in the final moments of the episode, the moment was warm enough to melt beskar. Their reunion capped off an episode of sacrifice, trust and camaraderie.\\n\\nWhen the episode begins, Mando is surrounded by a gang of stormtroopers and their ruthless leader, Moff Gideon. Escape seems imminent, however, after the arrival of the IG-11 unit — its modifications have made it the most deadly nurse droid in the galaxy. IG-11 is so dynamic that it warms Mando’s hatred for artificial intelligence.\\n\\nThe crew protects Baby Yoda in their escape on an underground lava river, though they lose IG-11, which sacrifices itself by self-destructing in the middle of a platoon of storm troopers. That sets up a final duel between Gideon, in a TIE-fighter, and Mando, using his newly acquired jet pack. In an impressive display of improvisation that would make Iron Man proud, Mando destroys Gideon’s ship and makes an escape.\\n\\nThis final chapter of season one set the foundation for the second season of Mandalorian, which is in production. Here’s what else we learned from chapter eight, “Redemption.”\\n\\n1. Moff Gideon’s backstory is just as important as Mando’s. And Gideon is equipped with a very special weapon.\\nWe’ve been eagerly awaiting more about Baby Yoda and Mando. Little did we know: Gideon is yet another character with compelling origins.\\n\\nThe episode unpacks the relationship between Gideon and The Empire’s attempts at eradicating Mandalorians. Gideon seems to have been a crucial leader during the siege of Mandalore, during which the Imperial forces killed most of the members of the Mandalorian order. And it seems that, in the process, Gideon acquired The Darksaber.\\n\\nThis is huge. And if you haven’t seen the animated series, you might have missed it. I, for one, thought Gideon was using a vibroblade to cut himself out of his crashed TIE. The episode ends with Gideon standing atop on the vessel with The Darksaber in-hand.\\n\\nFor Mandalorians, this is roughly the equivalent of Luke Skywalker’s lightsaber. The weapon has a long and notable history.\\n\\nHere’s an abridged version. The weapon came into existence when Tarre Vizsla became the first Mandalorian to join the Jedi Order. The Mandalorians protected the weapon after Vizla’s death, though The Darksaber fell into the hands of a number of non-Mandalorians. Ultimately, a Mandalorian leader, Bo-Katan Kryze, wielded it against The Empire during the final siege on Mandalore. Kryze likely died in The Empire’s invasion — along with almost all of the Mandalorians on Mandalore — and Gideon came into possession of the weapon. It makes him an even more powerful antagonist. (Turns out, he’s also deranged. He seems happy to kill his own stormtroopers, as we learned early in this episode.)\\n\\nBetween his possession of The Darksaber and his obsession with Baby Yoda, Gideon has a huge interest in The Force.\\n\\n2. We know Mando’s name: Din Djarren.\\nAs Mando comes to care for Baby Yoda — and stars acting like a sentient being instead of a droid — he has revealed more about himself. Most notably, Mando shared how he joined the Mandalorians. The Empire invaded his town, and during that invasion, droids killed his parents. A group of Mandalorians saved a young Djarren’s life.\\n\\nThat’s why he hates droids, and why it’s notable he allows IG-11 to remove Mando’s helmet. IG-11 saves Mando’s life by removing the lid and applying a healing spray. We get our first look at his face. (Unsurprisingly, it looks exactly like Pedro Pascal.) Our protagonist seems to be growing more comfortable with vulnerability, as Mando exposes his face to the world, even if its just the audience of a droid, for the first time in ages.\\n\\n3. Djarren needs to find Baby Yoda’s species.\\nThere’s an obvious direction for season two. The Mandalorian armorer spells it out for the viewer. Djarren must do one of two things: 1) train Baby Yoda to be a Mandalorian or 2) return Baby Yoda to his native species.\\n\\nBoth outcomes sound pretty dope. On one hand, we can imagine Baby Yoda, flying around with a jetpack and blaster in Beskar armor. On the other hand, we’ve got the prospect of Mando and Baby Yoda finding the Yodans (or the Yodanese or the Yodanians or whatever name you want to make up for that species). Whatever outcome, Mando is responsible for Baby Yoda.\\n\\n“You are as its father,” the Mandalorian armorer says.\\n\\nMay The Force be with him. And considering how bad of a dad Mando was in season one, he will need as much help from The Force as he can get.',\n",
              "  array([-3.445135  , -1.8132336 , -0.21694562], dtype=float32),\n",
              "  2),\n",
              " ('Blockchain is clearly not the only emerging technology that’s gained momentum in recent years. Advancements in artificial intelligence or the internet-of-things, for example, have also received significant attention. But there’s something quite disconcerting about the way in which blockchain hype is portrayed.\\xa0\\u2028\\nUgh, the hype\\nWe’ve often heard the line that “blockchain is a solution looking for a problem” and in a way, I have to say I agree. The notion of\\xa0decentralization\\xa0definitely seemed interesting in the aftermath of the 2008 financial crisis as consumers rightly lost\\xa0trust\\xa0in the banking system. But the\\xa0question\\xa0I’ve been asking myself for years is: what issues, or issues, is blockchain actually trying to solve? And why? Some of its purported\\xa0use cases\\xa0(voting,\\xa0payments,\\xa0digital\\xa0ID,\\xa0supply chain\\xa0management, etc) amount to little more than the willingness to add a distributed and encrypted ledger where one is not actually needed. It sometimes feels like\\xa0blockchain\\xa0is being thrown into the\\xa0mix\\xa0to complicate, rather than simplify, existing\\xa0processes\\xa0just because it’s trendy.\\nMeaningless\\nAlthough perhaps the real problem is that the term “blockchain” has become so widely used that it’s become somewhat redundant. Writing for the Verge, Adrienne Jeffries,\\xa0points out\\xa0that “the idea of a blockchain, the cryptographically enhanced digital ledger that underpins Bitcoin and most cryptocurrencies, is now being used to describe everything from a system for inter-bank transactions to a new supply chain database for Walmart. The term has become so widespread that it’s quickly losing meaning.” The lack of universal definition about what constitutes blockchain technology is likely a factor that’s also contributed to its lack of adoption. On a purely simplistic note, how can we apply a solution to a problem if we’re not quite sure about what the solution actually is?\\nDisillusionment\\nAccording to Gartner’s Hype Cycle, blockchain is still “sliding into the trough of disillusionment,” meaning the technology is struggling to live up to the expectations created by the hype around it. The Hype Cycle shows that most blockchain technologies are still five to 10 years away from having transformational impact, but if memory doesn’t fail me, this has been the case for as long as I’ve been covering the space. I can appreciate that it may take a while for some nascent technologies to mature and find their killer app, but surely the fact that it’s taken this long for blockchain to operate freely in the wild, is indicative of its potential future success — or lack thereof.\\nThe wrong approach\\nClearly not everyone agrees with me, though. Antoine Poirson, a partner at Antler, an early-stage venture capital firm and startup builder, still believes blockchain could make it. “If blockchain technology has been overhyped in the past, mainly because of the hype around cryptocurrencies which rely on this technology, it does have a lot of potential,” he said. “Allowing trust to be created in a distributed manner remains very powerful, and a lot of broader use cases have started to emerge. Blockchain technology is an enabler for business model innovation, however, until the business cases have been identified, the potential of the technology is not fully utilized,” Poirson added.Perhaps this is the crux of the issue: maybe blockchain technology hasn’t succeeded to date because the approach hasn’t been focused or specific enough.Desperate to improve their bottom lines, corporates have tried to leverage the technology in a bid to maximize efficiency and save costs. Banks, for example, have widely explored blockchain‘s ability to improve the\\xa0post-settlement and clearing process. On the other hand, corporates are still toying with the idea of using blockchain to track the provenance of goods or improve transparency.\\xa0\\nSomething bigger.\\nBlockchain’s purported promise is such that everyone is willingly taking a multi-faceted approach, not giving much thought to the possibility that its potential may, in fact, be limited. Or maybe blockchain is just the first iteration of something far more powerful, a base we can build on to restore our faith in decentralized systems.I’m aware that this article will not necessarily go down well with hardcore blockchain enthusiasts, and that’s fine. But, as we enter the new year, I urge you to take a moment and think about what lies ahead. How can we make the blockchain space different in 2020, and more importantly, what’s needed to enact this change?Earlier this year, I wrote a piece titled “Hype is killing blockchain technology.” At the time, you could argue it was my attempt to re-assess the industry following a three-year hiatus, but fast forward 10 months and not much has changed.',\n",
              "  array([-0.2804103, -3.2179952, -1.5872312], dtype=float32),\n",
              "  0),\n",
              " ('The holidays are here, and if you’re unwrapping a new Kindle (or a non-Amazon-branded e-reader, or just a device with an e-book app on it), you might be looking for some new books to read.\\nIf you need some recommendations, here’s a list of some of the best science fiction books released in 2019, which should be the perfect choices for a long plane ride home or a quiet vacation morning.\\nWe’ve rounded up our favorite and most-used games, apps, and entertainment. Check out our app picks for iPhones, Android phones, PCs and Macs; our favorite mobile games from Apple Arcade and Google Play Pass; and our top choices for gaming PCs, the PS4, Xbox One, Nintendo Switch and VR. We’ve also listed our favorite streaming shows on Disney+, Hulu, ESPN and Netflix, some great sci-fi books, and exciting new podcasts. (Note: pricing was accurate at the time of publishing, but may change.)\\nGIDEON THE NINTH BY TAMSYN MUIR\\nGideon the Ninth is probably the best thing I’ve read this year, a wildly fun blend of sci-fi and fantasy set in a mysterious, ancient castle. There’s necromancers, locked room mysteries, dueling cavaliers, warring political factions, and more that it would be a shame to spoil. If the blurb from Charles Stross describing it as “lesbian necromancers explore a haunted gothic palace in space!” can’t sell you, nothing will.\\nGideon the Ninth\\n$13.99 for Kindle from Amazon\\nBUY NOW\\nA MEMORY CALLED EMPIRE BY ARKADY MARTINE\\nMahit Dzmare is the newest ambassador to the massive, interstellar Teixcalaanli Empire. Her job goes south quickly when she discovers the previous ambassador is dead, leaving her to solve the mystery of his death and prevent her small station from being crushed by the massive political forces at work. It’s technically the first in a series, but it should hold up on its own, too.\\nA Memory Called Empire\\n$2.99 for Kindle from Amazon\\nBUY NOW\\nTHIS IS HOW YOU LOSE THE TIME WAR BY AMAL EL-MOHTAR AND MAX GLADSTONE\\nA mind-bending time travel story that’s also a love story, as warring time travelers duel across constantly shifting timelines to protect their future’s existence. Told in a series of alternating letters, it’s a short and beautifully written piece of science fiction.\\nThis Is How You Lose the Time War\\n$7.99 for Kindle from Amazon\\nBUY NOW\\nEXHALATION BY TED CHIANG\\nTed Chiang is best known for his short story Story of Your Life, which the movie Arrival was based on. His latest collection of stories, Exhalation, features nine original, thought-provoking pieces of fiction that deal with time, space, and humanity’s place in the universe.\\nExhalation\\n$13.99 for Kindle from amazon\\nBUY NOW\\nTHE CITY IN THE MIDDLE OF THE NIGHT BY CHARLIE JANE ANDERS\\nThe sophomore novel from Charlie Jane Anders is set on a far planet — half of it frozen, and half of it burned under the glare of the sun, with human settlers living in two cities in the (sort of) temperate zone in between. A new discovery of creatures that lived there first, though, changes everything.\\nThe City in the Middle of the Night\\n$13.99 for Kindle from Amazon\\nBUY NOW\\nFUTURE OF ANOTHER TIMELINE BY ANNALEE NEWITZ\\nAnnalee Newitz’s novel pictures a world where time travel is real, and a group of women use that technology to expand and protect female autonomy and rights, while fighting against a group of far-future misogynists determined to do the opposite. Also, plenty of punk rock.\\nFuture of Another Timeline\\n$13.99 for Kindle from Amazon\\nBUY NOW\\nEMPRESS OF FOREVER BY MAX GLADSTONE\\nVivian Liao is a genius inventor and entrepreneur — the next Steve Jobs, basically — until she’s flung to the far future where the entire galaxy is under the iron-fisted rule of the titular Empress of Forever. Needless to say, Vivian does not take to imperial rule well, in this fast-paced, galaxy-spanning adventure.\\nEmpress of Forever\\n$9.99 for Kindle from Amazon\\nBUY NOW\\nRECURSION BY BLAKE CROUCH\\nWhere Blake Crouch’s last book, Dark Matter, offered an action-packed take on alternate universes, Recursion (as the name might suggest) is a similarly action-packed time travel romp. The science here is a little fantastical, but if you’re looking for a lighter read, it’s certainly an entertaining adventure.\\nRecursion\\n$11.99 for Kindle from Amazon\\nBUY NOW\\nWANDERERS BY CHUCK WENDIG\\nChuck Wendig’s sprawling epic, Wanderers, is set in a near-future where wandering sleepwalkers start roaming the country due to an unknown pandemic. Mixing science fiction with political, environmental, and social commentary as the situation with the “wanderers” continues to escalate, Wendig’s book isn’t the easiest read of the year, but it’s perfect for anyone looking for a more serious sci-fi book.\\nWanderers\\n$13.99 for Kindle from Amazon\\nBUY NOW\\nANCESTRAL NIGHT BY ELIZABETH BEAR\\nIf you’re the kind of person who likes their science fiction with a bit more science in it, Ancestral Night is the book for you. With a far more realistic take on space travel than most books, Ancestral Night sees a pair of salvage operators uncover the haul of a lifetime — but that could also start a universe-spanning war.\\nAncestral Night\\n$7.99 for Kindle from Amazon\\nBUY NOW\\nYOU LOOK LIKE A THING AND I LOVE YOU BY JANELLE SHANE\\nScience fact, rather than science fiction, You Look Like a Thing and I Love You is an incredibly informative dive into how today’s artificial intelligence and machine learning algorithms work, and how they’ll shape the future of computers, technology, and our day-to-day lives.\\nVox Media has affiliate partnerships. These do not influence editorial content, though Vox Media may earn commissions for products purchased via affiliate links. For more information, see our ethics policy.',\n",
              "  array([-4.0331416 , -2.5404584 , -0.10153325], dtype=float32),\n",
              "  2),\n",
              " ('The BBC\\'s weekly The Boss series profiles different business leaders from around the world. This week we speak to Allen Lau, co-founder and chief executive of global storytelling website and app Wattpad.\\nIn the early days of his company, Allen Lau had a favourite evening pastime.\\nHe would lurk on the online storytelling community he had developed along with co-founder Ivan Yuen, seeking out new writers.\\nAllen would regularly discover 10, maybe 20 writers he\\'d never seen before who were sharing their original work on Wattpad.\\nThen one evening: \"I just kept on clicking, and it never ended. I kept bumping into new people,\" says the 51-year-old.\\nNot only were new writers flocking to the platform, but fans were interacting with their favourite authors. Allen knew something had clicked - a storytelling community was born.\\n\\nUsers have profiles where they can publish works and create their own reading lists\\nLaunched in 2006, Wattpad now has more than 80 million readers, and four million active writers on the site each month.\\nIt is a global platform backed by more than $117m (£88m) from international investors in Asia, the US, and Canada.\\nWith Wattpad\\'s revenue streams ranging from in-story advertising, to a paid-for premium version, and branded and sponsored content, tech site Crunchbase estimates that the company\\'s annual turnover is now more than $24m.\\nFrom the start, Allen and Ivan, 42, had big aspirations, but Wattpad\\'s story isn\\'t one of overnight success.\\nHong Kong-born Allen moved to Canada in the late 1980s aged 19. He studied electrical engineering, and was hired by Microsoft, a job that wasn\\'t the right fit for the young university graduate with an itch for entrepreneurship.\\nHe left to work at a Canadian software start-up, which he witnessed \"rocket ship\" from about 100 employees to more than 700 in about three years, before it was snapped up by an American firm. For Allen, it was an instructional experience about the possibilities of success.\\n\\nAllen Lau now wants to disrupt the wider entertainment industry\\nIvan, also an engineer, was an early hire at Allen\\'s first start-up, a mobile gaming company based in Toronto. Ivan later moved to Vancouver, but the pair kept in touch. One day Ivan sent Allen a message, wanting to show him a project he was tinkering with, a website where users could share original writing.\\nAllen, who has both a love of both reading and electronic gadgets, was already working on something similar, a way for people to read on the then popular Motorola Razr mobile phone.\\n\"It didn\\'t take long for us to decide \\'let\\'s do this together\\',\" says Allen. The name they picked is a twist on the term e-book, with \"watt\" for electricity combined with \"pad\" where people can jot down ideas.\\nThat was late 2006, the year Google bought video sharing site YouTube for $1.65bn.\\nAllen recalls thinking: \"If YouTube can generate 40 million users from zero in a year, perhaps we could generate a few million. But in the end, it was only about 1,000. Clearly, it wasn\\'t working.\"\\nThey had launched the platform with thousands of public domain books, classics like Pride and Prejudice, as content to draw in readers. It wasn\\'t the draw they hoped. Their Google ad revenue in 2007 was a meagre $2. In a word, early results were \"depressing\".\\n\\nThe company had just a handful of staff for the first few years\\nBut the stars aligned for Wattpad with Apple\\'s release of the iPhone, and the subsequent launch of its app store in 2008. Mobile text went from just about readable to user friendly almost overnight.\\n\"If I had a crystal ball I probably would have started the company a year later than we did,\" says Allen. \"But the good news is, it\\'s always better to be early than late.\"\\nIt was two years from Wattpad\\'s launch that a user finally posted an original work to the site - a Victorian-era vampire novel called Blind Truths. Within 18 months of that first, the company had overcome what Allen called the \"chicken and egg\" problem: \"Without content, we don\\'t have any readers.\\nMore The Boss features:\\nHow a university dropout built a toy empire\\nThe reigning queen of American reality TV\\nNo-one understood our idea, but now it\\'s worth over $1bn\\nHow a missing letter helped create a tech billionaire\\nThe map store boss who took the long route\\n\"Without any readers, no one would upload content. So we had to get the snowball rolling. But once the snowball was rolling, it just rolled faster and faster.\"\\nBoth the platform\\'s reach and community were a big draw. Creators can interact directly with fans and build an audience. And its support for multiple languages, from Vietnamese to Filipino, helped attract international users.\\nBy 2011, the company had a million registered users, and the audience has continued to grow. In years since, some Wattpad authors have achieved massive commercial, if not always critical, success.\\nAmerican Anna Todd became a literary phenomenon with her novel, After, written on her smartphone and released chapter by chapter.\\n\\nApple\\'s launch of the iPhone gave Wattpad a massive boost\\nIt has now been read by more than 600 million Wattpad users, while the ensuing series has a total 1.5 billion reads to date - and scored the first-time author a publishing deal. A subsequent film adaptation was released in April 2019, and won a People\\'s Choice Award in November.\\nMeanwhile, Beth Reekles, a British 17-year-old, had a runaway success with her rom-com novel The Kissing Booth. It was later published by Penguin Random House, and adapted for Netflix, becoming \"one of the most-watched movies\" of 2018, according to the streaming giant.\\nWattpad has now also moved into the broader entertainment space, collaborating with companies in North America, Europe, south-east Asia, and Latin America to bring the platform\\'s most popular works to televisions and movie screens. It has also launched its own print publishing division.\\nSays Allen: \"Once you have a million users there will be 100 ways to make money, once you have 100 million users there will be 10,000 ways to make money.\"\\n\\nAnna Todd was an early breakout author who found success on Wattpad\\nTo uncover emerging trends and discover which of the half a billion uploads that have been made to the platform could be best-sellers and blockbusters, Wattpad uses audience data and artificial intelligence algorithms, which deconstruct information as granular as sentence structure.\\nThe company estimates thousands of writers, like Ms Todd and Ms Reekles, have made money on the platform. Wattpad has various ways for writers to monetise their output, including a recently launched programme that allows readers to directly pay select writers for their work.\\nAdditionally, top writers have the opportunity to work on branded content, with firms like Coca-Cola and H&M interested in Wattpad\\'s predominantly millennial and generation Z users. Then there\\'s the possibility a writer could become the platform\\'s next Anna Todd.\\n\\nThe Wattpad platform has an audience of about 80 million people\\nLorraine Shanley, a publishing industry consultant, says \"publishers have been fascinated by Wattpad and impressed by what they have done\".\\n\"Wattpad offers its writers different ways of monetising their own work so it doesn\\'t look like they are taking advantage of their fans, it looks there is a courting and promoting of fans. That\\'s a tightrope and they\\'ve done it very well.\"\\nAllen says that watching Wattpad authors have success is \"very emotional\". \"We proved that we don\\'t have to do things in a traditional way, we don\\'t have to have a gatekeeper,\" he says.\\n\"We can democratise the process. At the end we can give a lot of the power back to the fans.\"',\n",
              "  array([-4.339742  , -3.3124385 , -0.05073248], dtype=float32),\n",
              "  2),\n",
              " (\"OPINION\\nKiller robots are not a fantasy. The world must reject and block these weapons.\\nMary Wareham Opinion contributor\\nPublished 2:21 PM EST Dec 30, 2019\\nAllowing machines to select and target humans sounds like something out of an apocalyptic sci-fi movie. But as we enter another decade, it is becoming increasingly obvious that we’re teetering on the edge of that dangerous threshold.\\nCountries including China, Israel, South Korea, Russia and the United States are already developing and deploying precursors to fully autonomous weapons, such as armed drones that are piloted remotely. These countries are investing heavily in military applications of artificial intelligence with the goal of gaining a technological advantage in next-generation preparedness for the battlefield.\\nThese killer robots, once activated, would select and engage targets without further human intervention. The United States and other countries developing them are trying to prevent progress toward an international treaty to ban them and retain meaningful human control over the use of force. They call efforts to regulate these weapons premature, and question concerns that deploying them will threaten the right to life and principles of human dignity.\\nKiller robots a top existential threat\\nRecognizing that, the momentum to prevent a future of killer robots intensifies. Killer robots are now seen as one of the top existential threats faced by the planet. A growing number of countries and some unlikely allies are now backing the drive for a new treaty to prohibit lethal autonomous weapons systems. As Nobel Peace Laureate Jody Williams warns, such weapons would cross “a moral and ethical Rubicon.”\\nIn September at the United Nations General Assembly, an “Alliance for Multilateralism” initiative led by France and Germany and including dozens of foreign ministers, identified killer robots as one of six “politically relevant” issues requiring an urgent multilateral response. (The others included climate change and gender equality in education.)\\nCampaign to Stop Killer Robots in London, on April 23, 2013.\\nOli Scarff/Getty Images\\nAt eight meetings on killer robots by the Convention on Certain Conventional Weapons since 2014, there has been widespread agreement among virtually all of the 80 participating countries on the need to retain some form of human control over the use of force. Thirty countries now vigorously promote a ban treaty as essential to stigmatize the removal of human control from weapons systems. \\nDon't ignore science fiction warnings: Who's going to stop the rise of the killer robots?\\nUnited Nations Secretary-General António Guterres expressed alarm last month that “killer robots could take the place of soldiers.” Deeming the prospect of machines with the power and discretion to take human life “politically unacceptable and morally despicable,” he has called for a new treaty to be negotiated and offered U.N. support toward that goal in his Agenda for Disarmament.\\nSocial biases could affect targets\\nLeading artificial intelligence experts, roboticists, scientists and technology workers at Google and other companies are also demanding regulation. Liz O'Sullivan of the International Committee for Robot Arms Control warned at a U.N. event last fall that algorithms are fed by data that inevitably reflect various social biases. If applied in weapons, she said, they could cause people with certain profiles to be targeted disproportionately. Killer robots would also be vulnerable to hacking and attacks in which minor modifications to data inputs could “trick them in ways no human would ever be fooled.”\\nWorld problems require international solutions: Trump takes verbal sledgehammer to United Nations foundation\\nThe concerns around killer robots are also beginning to impact their military acquisition and development. Increasingly jittery defense planners are becoming reluctant to budget millions of dollars for autonomous weapons systems that might be prohibited before they are even built. In 2019, for example, a major German industry association comprised of businesses and defense contractors, including Rheinmetall, called for the government to work for a new treaty to ban killer robots. \\nWith public pressure, calls for a new treaty to ban killer robots will soon become too overwhelming for military powers to contain the current diplomatic talks to the failing Convention on Certain Conventional Weapons.\\nThere’s increasing recognition that it’s time to chart a new path forward to create the treaty desired by so many. Only through new international law will it be possible to draw the line and determine what is acceptable when it comes to increasingly complex autonomous weapons systems.\\nMary Wareham is the arms division advocacy director at Human Rights Watch and coordinates the Campaign to Stop Killer Robots. Follow her on Twitter: @marywareham\\nPublished 2:21 PM EST Dec 30, 2019\",\n",
              "  array([-0.51274735, -2.0751574 , -1.2887495 ], dtype=float32),\n",
              "  0),\n",
              " ('Twenty-one years old this year, no one can dispute the dominance Google has enjoyed in the world of search. After all, it has a massive 81.5 percent market share worldwide. But, new players are beginning to chip away at its position in Europe and North America — particularly when it comes to users researching products — and they’re coming from different fields.\\n\\nIf you shift the focus to look at time spent online as a whole, the vast majority (96 percent) of activity already takes place outside search engines. Meanwhile, more time is being spent on social media, with the average user now spending 2 hours and 16 minutes on social each day — up by a minute from last year.\\n\\nAnd, as social platforms evolve to become less about conversation and more about entertainment, online consumer behavior is also shifting. The content focus of social networks is making them a prime destination for researching and buying new products; a behavior trend that responsive brands can now see as a big opportunity.\\n\\nProduct research shifts to social\\nHistorically, consumers have always headed to search engines to engage with products that interested them. As a result, search engine optimization (SEO) and pay-per-click advertising (PPC) have been key in allowing brands to get their products in front of users in that all-important research phase.\\n\\nHowever, users are now turning to social networks for product research. Recent studies from GlobalWebIndex and Facebook have found that over a quarter of global users between the ages of 16-64 have turned to social networks during their online product research. A sizeable 81 percent of respondents specified that Instagram, originally set up as a photo sharing platform, is now a consumer search go-to.\\n\\nThis trend’s prominence becomes more evident when focusing on younger consumers. According to GlobalWebIndex’s Social media report, social media has already surpassed search engines as the leading product research channel among 16-24 year olds.\\n\\nAnd it’s this trend that should stick in our minds, with insights suggesting that Gen Z will become 40 percent of all consumers by next year.\\n\\nPrioritize the Gen Z market\\nThe growing influence of Gen Z is clear. Fast-growing and demographically young markets such as Latin America, the Middle East, and Africa are already fully engaged in social search.\\n\\n57 percent of Latin American users are more inclined to start their product research on social media and 64 percent of users in Middle and East Africa will do the same. A sure sign that the trend is set to take hold globally.\\n\\nSo, for brands operating in European and North American markets, 2020 presents a huge opportunity — that’s if businesses are prepared to shift their strategy. Gen Z love images, so companies need to adapt their digital marketing strategies to prioritize image-based content, as well as investing in their social media departments, ensuring they have the right resources in place.\\n\\nBeauty matters on social\\nSocial product research is a completely different experience to the traditional and formulaic “search to ad” experience we’ve all been used to. Users expect to see products displayed in natural, realistic settings — and, most importantly, they expect it to be done beautifully. That means an end to generic ads that promote goods with a siloed, product-first focus.\\n\\nTo capture their audiences, companies need to wholeheartedly invest in design and creative. They need teams of people who can create the images and assets that will inspire and captivate their social user — and they need them quickly.\\n\\nBeyond 2020: visual search on the horizon\\nThis links with visual search which, although in its infancy, is already being led by the likes of Google Lens and Pinterest Lens. These platforms let users take photos, or use an existing photo, to find products related to it. The technology is primed and ready to take over search.\\n\\nThe CEO of Pinterest Ben Silbermann believes the future of search “will be about pictures rather than keywords” — implying they’re readying themselves to fully tackle the search space.\\n\\nEarlier this year, Facebook bought an artificial intelligence startup that created a visual search technology that allows people to take photos of a real-world object and use them to look for similar items online. They’ve also spoken about an upcoming visual search technology, similar to that of Google’s and Pinterest’s Lens technology. They claim it will let users take a photo of something they like, and then Marketplace will search across its listings to present them with similar items.\\n\\nMeanwhile, Instagram started to introduce alt text to photos in an effort to make the platform more accessible — and images more searchable. They’ve also launched in-app shopping, something that they’ll expand into other territories in the near future.\\n\\nCombined with the popularity of image based social networks such as Instagram and Pinterest, visual search technology could present further opportunity for social to take hold of product based searches. Accounting for the influence of Gen Z, this is certainly a trend digital marketers need to adapt their digital strategies and teams for.',\n",
              "  array([-4.2266326, -3.5127463, -0.045433 ], dtype=float32),\n",
              "  2),\n",
              " ('Racial bias in facial recognition software: What travelers should know as TSA, CBP expand programs\\nCurtis Tate USA TODAY\\nPublished 7:28 AM EST Dec 31, 2019\\nFederal government researchers found evidence of bias against minorities in facial recognition software as its use is set to expand at airport security checkpoints.  \\nThe Transportation Security Administration and U.S. Customs and Border Protection have been testing facial recognition technology at airports across the county,  expecting it will become the preferred method to verify a passenger\\'s identity.\\nThe National Institute of Standards and Technology reported this month that facial recognition software showed a higher rate of incorrect matches between two photos for Asian and black people relative to white people.\\nResearchers studied the performance of 189 algorithms from 99 manufacturers representing most of the industry. Some algorithms performed better than others, they concluded, meaning that it\\'s likely the industry can correct the problems.\\nThe institute found that U.S.-developed algorithms had the highest rate of incorrect matches, or false positives, for American Indians.\\nResearchers found a higher rate of false identifications of black women when matching their photos to an FBI mugshot database. Higher rates of mismatches increase the chance that a person could be falsely accused, the institute said.\\nControversy: Why some cities, states and lawmakers want to curb facial recognition technology\\nPatrick Grother, a computer scientist at the institute and the report\\'s primary author, said some mismatches can be resolved with a second attempt, such as checking a passport. However, he said, \"a false positive in a one-to-many search puts an incorrect match on a list of candidates that warrant further scrutiny.”\\nThe TSA has been testing facial recognition software at airport checkpoints in Los Angeles and Las Vegas, matching live images of passengers to the photos on their identity documents. Participation in those test runs was strictly voluntary.\\n\"TSA’s facial recognition system will be for passenger identification and to determine the appropriate level of screening only,\" spokesman Mark Howell said. \"TSA understands the variety of concerns related to facial recognition match performance and takes this issue seriously.\"\\nCBP uses the technology to screen arriving international passengers at 12 U.S. airports and exiting international passengers at 26 U.S. airports.\\nAdditionally, CBP uses it for entry at cruise ports in New York, New Jersey, Florida and Washington state.\\nThe future of airport security? What happens in Vegas might show up at a TSA checkpoint near you\\nBoth agencies plan to expand the use of the technology to screen passengers. It could speed up the process, allowing for shorter wait times for passengers and enabling security personnel to better focus their attention.\\n\"Facial recognition is going to replace that interaction with the TSA officer,\" said Brian Jackson, a security researcher at the Rand Corp., a policy research organization.\\nGrother\\'s team used an assortment of more than 18 million images of about 8.5 million people culled from the State Department, the Department of Homeland Security and the FBI. \\nThe research adds to concern about the accuracy of the technology, which has potential implications beyond airport security. \\nLast year, the American Civil Liberties Union used software developed by Amazon to match photos of all 535 members of Congress against a database of 25,000 publicly available arrest photos.\\nThe software incorrectly matched 28 lawmakers with photos of people who\\'d been arrested. Nearly 40% of those false matches were nonwhite lawmakers, though minorities make up only 20% of Congress.\\nPatricia Cogswell, the TSA\\'s acting deputy administrator, told House lawmakers in October that the agency wanted \"a very highly probable match\" in its developing biometric screening process.\\n\"We are not matching you against mugshots,\" she told a House Homeland Security subcommittee. \"If you don’t match, we go back to the regular process.\"\\nThough there may be lingering concerns about the accuracy of facial recognition technology, travelers may decide it\\'s worth the trade-off if they can save a few minutes.\\n\"Opting out will always have a time cost,\" Jackson said. \"There’s certainly a convenience benefit to it.\"\\nOpinion: Artificial intelligence, face recognition technology has a racial bias\\nPublished 7:28 AM EST Dec 31, 2019',\n",
              "  array([-2.2871854 , -3.6055644 , -0.13779646], dtype=float32),\n",
              "  2),\n",
              " (\"Turning yourself into a cyborg might sound like pure sci-fi, but recent progress in AI, neural implants, and wearable gadgets make it seem increasingly imaginable.\\n\\nThe weird and wonderful worlds of transhumanism and human enhancement are the subject of the 10th installment of the Sleepwalkers podcast. The final episode in the first season examines a subject that seems to resonate with techno-optimists in Silicon Valley but also raises some big questions: Where do we draw the line between humans and machines? Who should benefit from such technology? How do we retain control of our humanity?\\n\\nThrun, a prominent artificial intelligence expert who cofounded Google’s self-driving car project and helped develop the ill-fated wearable computer, Google Glass, argues that human beings are already a product of centuries of technological progress, so it would be foolish to forgo further enhancements. “The human I/O—the input/output, the ears and eyes and smell and so on, voice—are still very inefficient,” he says. “If I could accelerate the reading of all the books into my brain, oh my God, that would be so awesome.”\\n\\n\\nThe latest on artificial intelligence, from machine learning to computer vision and more\\nYuval Noah Harari, a historian who speculates about humanity’s future, is a leading figure in the burgeoning transhumanist movement. In his recent book, Homo Deus, Harari suggests that our ability to enhance ourselves with computers and bioengineering has already opened up a new era in human history. But he also fears this era—perhaps accelerated by AI—could pose an existential threat to our species. “We are really deciphering the underlying rules of the game of life, and are acquiring the ability to change these rules,” Harari warns.\\n\\nFor now though, using technology to alter our intelligence remains a distant dream, says Andy Schwartz, a neurobiologist at the University of Pittsburgh who works on brain-controlled computer interfaces for patients with physical disabilities. Although the technology is advancing, Schwartz says, it is a mistake to think it will become a pervasive consumer technology within the foreseeable future. “That's actually not true of a medically invasive procedure that involves putting implants on the surface of the brain,” he cautions.\\n\\nAs technology marches forward, there are many who stand to benefit from human enhancements. Noé Socha, an award-winning jazz guitarist with limited vision, is testing glasses that use video cameras and high-definition screens in front of the retina to restore some eyesight. Socha’s experience highlights the fact that simple enhancements could benefit those with disabilities most, potentially transforming their worlds.\\n\\nDiversity and inclusivity are also crucial issues for the technology of transhumanism, says Bryony Cole of the Future of Sex podcast and an organizer of sex-tech hackathons. She advocates for sex-related tech—from VR porn to sex toys—to be available to everyone, regardless of their physical ability or gender identity. “The core of our humanity, we want to connect, we want to belong.” she says. “We want to feel like we're part of something. That’s sort of the core part of that right down to our sexuality.”\\n\\nAccording to Harari, the historian, it will be just as important to guard against the capacity for new technolo\",\n",
              "  array([-3.0920794 , -3.3733983 , -0.08303431], dtype=float32),\n",
              "  2),\n",
              " ('Facebook has quietly started removing some misleading ads about HIV prevention medication, responding to a deluge of activists, health experts and government regulators who said the tech giant had created the conditions for a public-health crisis.\\nThe ads at issue — purchased by pages affiliated with personal-injury lawyers and seen millions of times — linked drugs designed to stop the spread of HIV with severe bone and kidney damage. Lesbian, gay, bisexual and transgender advocates long have said such claims are “false,” pointing to multiple studies showing the class of medication, known as PrEP, is safe.\\nAfter initially declining to disable the ads, Facebook began on Friday retroactively labeling some of them as rule violations in its archive, limiting their visibility. The company’s third-party fact-checkers concluded the ads were misleading and lacked context, according to a copy of an email sent by those fact-checkers to LGBT groups that was shared with The Washington Post, which first reported on the matter earlier this month.\\nAD\\nFacebook ads push misinformation about HIV prevention drugs, LGBT activists say, ‘harming public health’\\nThe change in course at Facebook drew praise from LGBT organizations that had worked since September to stop the spread of HIV misinformation on the social media platform. But many activists said they remain uneasy that it took so long to get Facebook’s attention in the first place — and worried the company’s policy on such ads in the future remains unclear.\\n“The removal of select ads is a strong first step given the findings of Facebook’s own fact-checking agency and the dozens of organizations that spoke out,\" said Sarah Kate Ellis, the leader of GLAAD. She added the “time is now for Facebook to take action on other very similar ads which target at-risk community members with misleading and inaccurate claims about PrEP and HIV prevention.”\\nFacebook spokeswoman Devon Kearns confirmed that the company had taken action against some of the ads. “After a review, our independent fact-checking partners have determined some of the ads in question mislead people about the effects of Truvada,\" she said, referring to the name of the drug. \"As a result we have rejected these ads and they can no longer run on Facebook.”\\nAD\\nThe incident illustrates Facebook’s persistent struggle to police its service, which reaches 2 billion people globally, and prevent the real-time spread of harmful posts, photos, ads and other troubling content.\\nFacebook has tapped thousands of human reviewers, invested in artificial intelligence and repeatedly revised its rules on what it allows and what it removes, particularly with an eye to preventing trouble during the 2020 presidential election.\\nBut its decisions often have left users, including regulators, deeply dissatisfied, feeling as though the company isn’t acting swiftly or aggressively enough to thwart online abuse, including misinformation that has the potential to result in real-world harm. In fact, many of the controversial ads about HIV prevention medication, purchased by Facebook pages associated with lawyers suing the drugs’ manufacturers on behalf of people who say they were harmed by the medication, had stopped running by the time Facebook took action against them last week.\\nAD\\nFacebook CEO Mark Zuckerberg says in interview he fears ‘erosion of truth’ but defends allowing politicians to lie in ads\\nLGBT activists, led by GLAAD, started trying to get Facebook’s attention about the issue more than three months ago. They formally asked one of its fact-checking partners, a non-for-profit called Science Feedback, to review the ads in November, according to Jessica P. Johnson, its science editor. Facebook asked for a review on Dec. 13, she added, four days after The Post published its initial story and LGBT groups went public with their concerns.\\nThe problem with the ads, LGBT advocates argued, is that they conflated the two applications of HIV-related drugs when, in fact, the preventive form, or PrEP, is safe. The advocates pointed to medical experts, including the Centers for Disease Control and Prevention, which publicly advises PrEP is “highly effective” and “recommended” for people at high risk. Absent swift action, organizations including GLAAD, the Human Rights Campaign and the Trevor Project said the ads threatened to scare patients away from a critical drug.\\nFacebook initially declined to take them down, signaling it would wait for a decision from its third-party fact checkers. Its handling of the issue drew sharp rebukes from top government officials, including New York Gov. Andrew M. Cuomo (D) and Sen. Elizabeth Warren (D-Mass.), who is seeking her party’s nomination for president. Warren, who also has been critical of Facebook’ policies on political ads, said Facebook’s inaction could “have serious public health consequences,” adding in a tweet: “Facebook needs to put the safety of its users above its own advertising profits.”\\nAD\\nZuckerberg promised to ‘fight’ efforts by Sen. Warren to break up Facebook in leaked comments\\nSen. Robert Menendez (D-N.J.) also penned a letter to the tech giant, demanding answers in response to The Post’s reporting. Menendez blasted the company in his Dec. 20 letter for exhibiting “complacency” and potentially putting “advertising profits over Americans’ lives.”\\nMonths after LGBT groups first sounded the alarm, Facebook’s third-party fact-checkers responded on Friday: They agreed the ads’ claim about PrEP was “misleading,” explaining in some cases it “overstates the risks for those who take Truvada as a preventive rather than as a treatment.” Science Feedback told LGBT activists that Facebook “has decided to begin marking this and similar ads as containing false or misleading information.”\\nIn response, Facebook disabled a series of ads, some of which had already ceased running on its service. But it did not ban all ads about drug-related lawsuits, or HIV prevention medication, nor did it issue any new policies around those who pay to reach users on those topics.\\nAD\\nSome ads linking bone and kidney damage to PrEP also remained fully visible in Facebook’s public ad archive, raising questions as to how exactly the company would apply its rules in the future. The company, for its part, says it’s still reviewing ads related to the medication.\\n“It’s gratifying to see one of Facebook’s fact-checkers back up the overwhelming consensus of AIDS, LGBTQ, and HIV medical groups that these ads are misleading,\" said Peter Staley, a co-founder of the PrEP4All Collaboration who previously raised some of the troubling ads with Facebook.\\n“But Facebook has put a warning on only one ad thus far, with many more unaffected,” he continued, adding: “If this is their official response, after ignoring us for months, then it’s a mess.”',\n",
              "  array([-0.5409566, -2.52695  , -1.0849859], dtype=float32),\n",
              "  0),\n",
              " ('China has one of the most tightly controlled internet environments in the world, but despite this its 854 million internet users yet again in 2019 found ways to challenge the government or talk about the issues they want to discuss.\\nThis year, China has seen a reignited #MeToo movement, young people have challenged unethical working hours, and the nation has united in concern against the aggressive rollout of AI technologies.\\nBut the government has also promoted its own interests related to the environment, the business sector, and of course, in Hong Kong.\\nHong Kong\\n\\nHong Kong\\'s demonstrations have dominated discussions among Chinese online\\nIn the last six months, this one topic has dominated news coverage on social media platforms both inside and outside mainland China.\\nIn fact the large-scale Hong Kong protests, which attracted international media attention in early June, led to the word \"Hong Kong\" initially becoming a censored search term on 9 June. When the protests first began, the Beijing government censored any reference to them, but after it became clear they wouldn\\'t go away, official media mounted a heavy media campaign to portray the demonstrations as violent with \"shades of terrorism\".\\nHashtags including #SupportTheHongKongPolice and #ProtectHongKong were aggressively rolled out by government media on the Chinese social media platform Sina Weibo.\\nIn contrast, on Twitter and Instagram, protestors used the hashtag #FightForFreedomStandWithHK and #GloryToHongKong - slogans that have subsequently become associated with the demonstrations.\\n\\'Civilised behaviours\\'\\n\\nA number of Chinese cities are moving to make exposed bellies an example of \\'uncivilised behaviour\\'\\nAhead of China rolling out a controversial social credit system in 2020, as a means of assessing citizens\\' economic and social reputation, one phrase has repeatedly cropped up: the need for more \"civilised behaviours\".\\nThe Beijing government has left it to regions to determine how they implement this, and as a result, a number of regulations encouraging citizens to be \"civilised\" have come into effect around the country, but have also left people scratching their heads.\\nIn July, the eastern Chinese city of Jinan banned topless men and the \"Beijing bikini\": the habit of men exposing their bellies by rolling up their shirt.\\nIn May, the capital targeted manspreading and eating on the subway, and eastern Nanjing has warned jaywalkers - pedestrians who cross the road at a red light - that their social credit could be impacted if they failed to wait for the little green man..\\nFacial recognition\\n\\nFacial recognition is entering more and more Chinese industries - but people aren\\'t impressed\\nChina\\'s development of artificial intelligence technologies has rocketed this year, but online topics related to the rise of facial recognition technologies have raised eyebrows and ignited a lot of concern online.\\nEarly in the year, payment service Alipay extensively worked with retail stores to enable consumers to buy products using facial recognition. But by July, it announced that it was adding beauty filters to facial scan payment devices, noting that the majority of consumers were not comfortable with the technology, and hated seeing their face to pay.\\nFacial recognition has been mocked for its imperfections. In May, a security camera wrongly identified a man scratching his face as taking a phone call.\\nAnd there have been a string of controversies related to platforms being unnecessarily intrusive when collecting consumers\\' facial data.\\nIn July, a vlogger was mocked after facial filters she had used revealed her to be a much older woman. In September, an app called ZAO was shut down, after it allowed users to insert their faces in place of film and TV characters, and sparked fraud and privacy fears.\\nAnd in November, a law professor sued a wildlife park, after it suddenly enforced facial recognition as a prerequisite for entry.\\n\\'996\\'\\n\\nJack Ma says his Alibaba business wouldn\\'t have been successful if he hadn\\'t worked extensive overtime\\nLate last year, the term \"996\" cropped up on a number of social media microblogs and forums, originally by workers in China\\'s tech industry as a subtle way to vent their frustrations at the excessive amount of work they were expected to do.\\nThe Chinese censors struggle to censor number sequences, given that they can often be innocuous. Consequently, Weibo users were able to use the term \"996\" to complain openly that their employer was violating China\\'s labour laws by making them work some 72 hours a week: from 9am to 9pm, six days a week.\\nBut the phrase has now seen expanded usage beyond the tech industry, especially among China\\'s young, who complained that overtime has become an epidemic.\\nHowever, some of the country\\'s best-known entrepreneurs have defended the \"996\" system, crediting it with enabling their businesses to surpass others. These include China\\'s richest man, Alibaba founder Jack Ma, and fellow tech entrepreneur Richard Liu.\\n985,211,996,251\\nAnother of China\\'s giant companies has been dragged into the number scandal too. The string of numbers 985,211,996,251 looks like a website address, but it\\'s actually being used to talk about a scandal involving a former employee of tech giant Huawei.\\nThe first and second set of numbers refer to China\\'s top universities, where many tech employees will come from - 985/221 in the country\\'s national university rankings; 996 is the number of hours they\\'re usually expected to work, and the 251 represents the number of days former Huawei employee Li Hongyuan spent in police custody after a disagreement with his employer.\\nLi was sent to prison for asking for severance pay having worked 13 years for the company. He was accused of extortion. Prosecutors freed him in August, after finding insufficient evidence to back Huawei\\'s claim.\\nThe case has tarnished the reputation of Huawei, despite the huge wave of domestic patriotism towards it following the arrest of its CFO Meng Wanzhou in Canada a year ago.\\nSexual assault\\n\\nBeauty vlogger Yuyamika encouraged others to break their silence on experiences of domestic violence\\nLast year, following international momentum of the #MeToo movement, China saw a number of women trying to use the hashtag to mention their own experiences of sexual abuse, but they were swiftly censored online.\\nThis year, however, women in China have made it difficult for government censors to filter their experiences of sexual assault, by sharing footage that they have covertly filmed of their own experiences.\\nIt has been increasingly common, and an almost daily occurrence in recent months, for videos to appear on Sina Weibo, which quickly go viral, of women being assaulted by their partners. The All-China Women\\'s Federation says that as many as 30% of China\\'s married women - some 90 million women - have suffered some form of domestic violence.\\nIn November, a beauty vlogger Yuyamika called on her million of followers to be \"no longer silent\" on domestic violence.\\nAnd there have also been calls among student communities to out people in positions of power for abusing women.\\nIn December, students at universities in Beijing and Shanghai united online to detail their experiences of sexual assault at the hands of their professors. This led to two university professors being sacked.\\nRecycling\\n\\nVR games went viral for showing Chinese people that recycling could be fun\\nThe environment has become a growing topic of international interest this year, and in July, the largest and most populous city in the world, Shanghai, took the bold move to enforce strict new recycling regulations.\\nIt demanded that people divide their waste into four different kinds, or else face hefty fines. Additionally restaurants and food delivery businesses have banned plastic cutlery and hotels have banned disposable goods.\\nThe same month, the hashtag #DividingRubbishChallenge went viral, with the government promoting ways that people could remember which product goes in which bin.\\nSocial media users across the country animatedly discussed viral songs, board games, and even a video of a man playing a virtual reality rubbish dividing game.\\nAnd there has been a widespread national consciousness to be more environmentally friendly. The official CGTN broadcaster says that 46 major Chinese cities will be following in Shanghai\\'s footsteps by the end of 2020.\\n\\'Boycott…\\'\\n\\nSome say they are boycotting the 2020 film Mulan after actress Liu Yifei posted support for Hong Kong\\'s police\\nThis year, there have been repeated calls - largely state-orchestrated - to boycott either individuals, products or franchises, if they are seen to be anti-China.\\nA number of international brands were slammed by Chinese media in August, because they referenced either Hong Kong, Macau or Taiwan as separate countries or regions: including luxury brands Versace, Coach and Givenchy.\\nChina went further this year, risking antagonising its international sports fans by indicating that it would also not be tolerant of huge sporting franchises. After basketball manager Daryl Morey tweeted his support for Hong Kong protestors in October, China outright banned NBA games on its state broadcaster.\\nIn December, a tweet by Arsenal football player Mesut Ozil also led to Chinese TV removing Arsenal games from its sports channels.\\nThere have also been media counter campaigns overseas in retaliation to China\\'s cancel culture.\\nIn March, Indian users used the hashtag #BoycottChineseProducts after China initially blocked a bid to designate Pakistan\\'s Masood Azhar, the leader of a group that carried out a Kashmir suicide bombing, a terrorist.\\nIn August, people on Twitter used the hashtag #BoycottMulan after Chinese actress Liu Yifei expressed her support for the Hong Kong police.\\nAdditional reporting by Yashan Zhao\\nBBC Monitoring reports and analyses news from TV, radio, web and print media around the world. You can follow BBC Monitoring on Twitter and Facebook.\\nMedia caption\\nJournalist Karoline Kan explains how Chinese social media is censored.',\n",
              "  array([-0.15794268, -2.0892305 , -3.8023205 ], dtype=float32),\n",
              "  0),\n",
              " ('As 2019 splutters to a close, it\\'s time for our annual lookback at our most-read tech stories, and to ask: \"What happened next?\".\\nFacebook and its family of apps dominates this year\\'s list with four entries - it probably won\\'t be a surprise that none of them were particularly brand-enhancing.\\nThe Chinese viral video app TikTok makes the cut for the first time. And many of the other \"big tech\" names are there too in one form or another.\\nBut there are a few notable exceptions. Neither Elon Musk nor Tesla made it, despite the window-smashing launch of the Cybertruck and plans to hack our brains. Google\\'s co-founders were originally on the list after deciding to give up day-to-day control of their empire, but were squeezed out just before publication.\\nVideo gaming also missed out, even though Prince Harry attracted lots of attention for suggesting Fortnite should be banned.\\nAnd both Huawei and Samsung are absent, even though the former\\'s loss of Google\\'s apps and the latter\\'s folding phone fiasco were two of the year\\'s standout developments.\\nIn any case, here\\'s what attracted most eyeballs in each month of the year:\\nJanuary: When three becomes one\\nA leak forced Facebook to reveal plans to merge the behind-the-scenes tech of messaging on WhatsApp, Messenger and Instagram. The effort was reported to be a pet project of chief executive Mark Zuckerberg.\\nHe later justified the move saying it would draw the three products closer together, making it easier for users to send posts between them. Furthermore, he said it would also help the firm expand its end-to-end encryption features, which help keep the messages secure.\\nMany observers noted, however, the action would also make it more difficult to split the company apart. And as the year went on that became a growing threat, with first Senator Elizabeth Warren and then other Democratic presidential candidates suggesting Facebook has too much power and influence.\\nBut it may not take a change of administration for Mr Zuckerberg\\'s ambitions to be thwarted. The Wall Street Journal recently reported that the Federal Trade Commission may intervene to prevent the apps being integrated.\\nFebruary: Don\\'t be scared\\nSocial media, the mainstream news and even the police all got in a tizzy over Momo for no good reason in February. It was claimed that youngsters\\' social media accounts were being \"hacked\" to show the bulging-eyed monster alongside \"challenges\" that would put their lives at risk.\\nOnline articles followed, linking more than 100 teenagers\\' deaths in Russia to the sensation. Except, of course, there was no evidence to back up any of this.\\nThis was not even the first time an image of the Japanese bird-woman sculpture had gone viral. There had been a similar smaller-scale scare in 2018 when the \"game\" had been linked to deaths in South America and India - again without any documented proof.\\nPundits described it as a \"panic [that] won\\'t go away\". Except it did.\\nThese days a search for Momo on Twitter turns up ads for masks of the ghoul, but little else.\\nAnd on TikTok the hashtag #momochallenge surfaces videos of people cooking and eating small dumplings that go by the same name in parts of Asia.\\nMarch: Turn it off and on again\\nFacebook\\'s family of apps experienced 14 hours of disruption, in what was billed as their \"most severe outage\" to date.\\nIn many cases, users were unable to access the services at all over the period. And it took the firm about another 10 hours to give itself the all-clear, at which point it tweeted that a \"server configuration change\" had been to blame.\\nThat allowed it to deny suggestions that it had been hacked, while remaining suitably vague about the actual cause.\\nApril: How did they get in?\\nCyber-security experts despaired after a study indicated that the most popular online password was \"123456\".\\nThe UK\\'s National Cyber Security Centre\\'s finding came with a warning that the string of digits is not only easy to guess, but would be one of the first codes tested by automated hacking tools.\\nThe public is advised to instead register a different complex login for each service they join, and use a password manager. But the hassle involved in having to copy and paste them in each time, has encouraged the adoption of biometric tests that automate the process if users pass a face, eyes or fingerprint ID check.\\nAnother alternative is to log in via another platform and let it do the heavy lifting. And in September, Apple joined the party when it allowed users to access third-party apps via a new Sign In With Apple button, mirroring earlier efforts by Google, Facebook and Twitter.\\nMay: WhatsApp cracked\\nWhen WhatsApp confirmed that a vulnerability in its app had been exploited to install surveillance software on victims\\' phones, one of the immediate questions was how widespread the attack had been.\\nIt took until October to get some clarification, at which point Facebook said it believed about 1,400 of its users had been directly compromised. It added that they included \"at least 100 human rights defenders, journalists and other members of civil society\" across at least 20 countries.\\nThe tech firm alleges NSO Group, an Israeli private security firm, is responsible and is currently suing it in the US courts. NSO disputes the claim and has said it will \"vigorously fight\" the case.\\nWhatever the outcome, the affair highlighted that if an attacker can load spyware onto a target\\'s phone or other device, end-to-end encryption and other security measures may be in vain.\\nJune: The death of Etika\\nBrooklyn-based Desmond \"Etika\" Amofah had a large online following, thanks to his quick wit and Nintendo video-game reaction videos on YouTube and Twitch. But in mid-June he caused concern when he posted a clip in which he discussed suicide. Days later the New York City Police Department confirmed he had killed himself.\\nSeveral of his friends and colleagues have since taken steps to memorialise him. An online store sells goods branded with his logo, and donates its profits to the National Alliance on Mental Illness. YouTuber PewDiePie also teamed up with actor Jack Black to raise further funds for the charity in Etika\\'s name.\\nOthers have marked the tragedy by getting themed tattoos. A still active Twitter account - @Etika - was created to keep his memory alive. And last month, a large mural was unveiled in Brooklyn featuring the gamer\\'s face alongside a pair of Nintendo Switch controllers.\\nReport\\nMost recently, YouTube faced complaints for not referencing the late creator in its Rewind recap of the year. \"No one should be surprised that YouTube still doesn\\'t understand its platform,\" posted one frustrated user.\\nIn any case, Etika\\'s claim in his final video that \"this world\\'s gonna forget me\" shows no sign of coming true any time soon.\\nIf you\\'ve been affected by a mental health issue, help and support is available. Visit BBC Action Line for more information about support services.\\nJuly: Photo glitches\\nFurther technical problems at Facebook HQ prevented users being able to upload new photos and videos to its apps, and in some cases prevented existing ones being viewable. The disruption lasted for about nine hours.\\nFacebook never really explained the cause, beyond saying it had been triggered by a maintenance operation.\\nOther smaller glitches persisted throughout the year, including intermittent outages in the US on Thanksgiving.\\nBut given that it now serves more than 2.4 billion users who log into at least one of its services once a month, it is a considerable feat of engineering to keep everything ticking along.\\nAugust: iPhone booby traps\\nStudies indicate that Apple\\'s mobile devices face fewer serious cyber-security threats than Android-powered equivalents. So when Google revealed that hackers were using booby-trapped websites to exploit previously unidentified flaws in iOS, potentially affecting \"thousands of visitors per week\", it was big news.\\nGoogle added that compromised handsets made it possible for the perpetrators to steal private messages, photos and location data in real-time.\\nFor days there was speculation about who might have been exposed. Apple eventually released a statement saying it believed that fewer than a dozen websites focused on \"content related to the Uighur community\" had been affected. Many took this to suggest that the Chinese state was involved. However, Apple did not explicitly draw this conclusion itself, which was unsurprising given its ties to the country.\\nThis was not Apple\\'s only Uighur-related controversy this year. The campaign group Sum of Us has repeatedly claimed that the firm\\'s willingness to comply with a Chinese ban on virtual private network (VPN) apps has made it harder for civil rights defenders to safely discuss claims of abuses against the ethnic minority. The organisation now plans to raise the matter at Apple\\'s next annual shareholders\\' meeting.\\nSeptember: More cameras\\nMedia caption\\nWATCH: Taking a slowfie with the iPhone 11\\nThe iPhone 11 range got more cameras, longer-lasting batteries and a new \"pro\" moniker for the top-of-the-range models. But there was no 5G - despite Samsung, Huawei and other rivals having already launched compatible smartphones. And whispers that Apple chief executive Tim Cook might be ready to unveil an augmented reality headset accessory, proved to be unfounded.\\nMarket watchers have since reported the iPhones sold better than they expected - particularly in the US and Western Europe.\\nAnd there is now talk of 2020 being the year of an \"iPhone supercycle\" thanks to an expected revamped design along with the introduction of 5G.\\nOctober: Jedi wars\\nAmazon - and many outsiders - thought it had the strongest bid for a high-profile contract to provide the Pentagon with cloud computing and artificial intelligence services. So there were shockwaves when Satya Nadella\\'s Microsoft clinched the so-called Jedi deal instead. It could be worth as much as $10bn (£7.7bn) over time.\\nNot only was this a big sum to miss out on, but Microsoft\\'s marketing team should also find it easier to pitch the firm\\'s Azure services to other government departments and private companies as a consequence. This could put Amazon Web Services\\' current status as the market leader under strain.\\nAmazon is challenging the award, claiming that President Trump pressured the Department of Defense into rejecting its bid because of a personal vendetta against its chief executive Jeff Bezos.\\nAll of this could all have ramifications for the 2020 presidential election. Regulation of big tech is already on the agenda, and Amazon could make a tempting target for Mr Trump during the campaign.\\nBut if Mr Bezos believes the Republican leader\\'s re-election could threaten his business, his status as one of the world\\'s richest men and the owner of the Washington Post could make him a formidable foe.\\nNovember: The eyes have it\\nMedia caption\\nWATCH: Feroza Aziz rejects TikTok\\'s explanations for blocking her from its app\\nAt the start of the year, TikTok was fairly obscure beyond its core teenage audience. These days it is one of the most talked about apps. It has launched one meme after another, and earned a reputation as being one of the most joyous places to be on the internet. But there are also concerns about it being Chinese-owned.\\nMatters came to a head last month when an American teenager posted a video that started off like an eyelash beauty tutorial. But creator Feroza Aziz quickly changed tack to criticise China\\'s treatment of the Uighurs.\\nHer clip went viral. Shortly afterwards, the 17-year-old discovered she had been blocked from posting new material. And soon after that, TikTok took the clip offline.\\nThen the social network reversed course. It put back the clip, blaming the removal on a \"human moderation error\". And it re-established Ms Aziz\\'s access, saying that she had been locked out because of unrelated past behaviour.\\nThe app insisted that there had been no attempt to suppress criticism of the Chinese government\\'s actions, but Ms Aziz was not convinced.\\nShe has continued to flag concern about the Uighurs. And in her latest \"skin care\" video also raises awareness about India\\'s controversial citizenship law, which offers illegal immigrants from nearby countries amnesty but only if they are non-Muslims - something she claims is \"immoral\". For whatever reason, the post has attracted far more views on Twitter and Instagram than the copy posted to TikTok.\\nMeanwhile, TikTok bosses are reportedly looking for a new global headquarters outside of China to help reinforce their claims to autonomy. But the app\\'s owner Bytedance has denied rumours that it might sell off the division to give it true independence.\\nDecember: Age of the Splinternet?\\nIt\\'s been a long while since the internet was a free-for-all, in which governments had little ability to restrict what their citizens did online. Even so, Russia\\'s announcement that it had successfully tested what it terms a \"sovereign internet\" still felt like a significant moment.\\nThe initiative involves forcing all web traffic through special nodes - a term for network connection points - where content can be filtered to remove what is deemed to be risky material. Furthermore, the intention is that in an \"emergency\" all data from outside the country could be blocked and the Runet - a term for the Russian internet - isolated.\\nThe achievement is described in the state media as a way to protect domestic companies and government bodies from cyber-attacks. But human rights campaigners warn that once the effort is up and running, the Kremlin may also use it to limit Russian people\\'s access to \"undesirable\" information.\\nIn doing so, the Russian government would be following the path of its counterparts in China, Saudi Arabia and Iran, which all censor dissenting voices.\\nAnd it would be following a wider trend. The US-based Freedom House digital rights group has warned that global internet freedom declined for a ninth consecutive year in 2019. Beyond Russia, it highlighted Kazakhstan, Sudan and Brazil as examples of places where digital surveillance, targeted cyber-attacks and/or online disinformation campaigns were cause for concern.\\nWe should hear more about Russia\\'s effort once President Putin has had a chance to examine the results of the tests, and decides how to proceed. For now, a Kremlin spokesman has denied it has any intention of \"cutting the internet\" up into separate parts.',\n",
              "  array([-4.31144   , -3.650007  , -0.04020252], dtype=float32),\n",
              "  2),\n",
              " ('By Noel Randewich\\n    SAN FRANCISCO, Dec 31 (Reuters) - What do cryptocurrency,\\nAlphabet and Walt Disney have in common?\\nThey\\'re all top investment picks for the next 10 years,\\naccording to Wall Street strategists surveyed by Reuters.\\n    As a decade dominated on the U.S. stock exchange by Netflix\\n, Apple, and other technology stocks comes to\\nan end, Reuters asked portfolio managers and other investment\\nexperts two questions: What is your top overall investment theme\\nfor the next 10 years, and what one stock would you buy and then\\nhold until 2030?\\n    Of 12 strategists and portfolio managers willing to venture\\nsuch long-term calls, Walt Disney and Google-owner Alphabet were\\npopular picks.\\n    Below are highlights of the recommendations:\\n    \\n Investor         Investment Theme         Top Stock Pick\\n Kim Caughey      5G wireless tech -       Microsoft          -\\n Forrest, Chief   \"Investing into 5G is    Says growth will be\\n Investment       something that we are    driven by customers\\n Officer of       doing, looking into a    continuing move to\\n Bokeh Capital    technology that will     its cloud platform.\\n Partners         take years to roll out   \\n                  and will change the way  \\n                  we live. It\\'s a          \\n                  world-wide technology    \\n                  event too.\"              \\n Jim Bianco,      Cryptocurrency - Says a  S&P 500 Index ETF -\\n head of Bianco   cryptocurrency could     \"It has been shown\\n Research         eventually challenge     over the past decade\\n                  the dollar as a reserve  that we are in the\\n                  currency, but such a     twilight of the\\n                  crypto does not yet      stock-picking era. I\\n                  exist.                   can buy the S&P and\\n                                           outperform all the\\n                                           high-paid fund\\n                                           managers.\"\\n Jim Besaw,       Carbon allowances - \"We  ARK Innovation ETF\\n chief            believe they could                - For\\n investment       outpace stocks and       exposure to\\n officer at       bonds, due to how they   disruptive tech\\n GenTrust         are regulated and the    including gene\\n                  expectation that supply  therapy, internet of\\n                  of these allowances may  things, robotics,\\n                  shrink dramatically      and cloud computing.\\n                  over the next decade to  \\n                  meet government carbon   \\n                  reduction mandates,      \\n                  increasing the           \\n                  likelihood of a price    \\n                  spike.\"                  \\n Tim Ghriskey,                             Alphabet - Points to\\n Chief                                     ongoing strength of\\n Investment                                advertising\\n Officer at                                business, plus\\n Inverness                                 opportunities in\\n Council                                   autonomous cars,\\n                                           healthcare and cloud\\n                                           computing.\\n Nancy Tengler,   Convertible securities   Palo Alto Networks\\n Chief            - \"With bond yields at            - Says its\\n Investment       historically low levels  dominant position in\\n Officer of       and on the heels of a    network security\\n Tengler Wealth   35-year bull market in   will drive growth as\\n Management       traditional bonds,       companies invest\\n                  convertible securities   more in cyber\\n                  provide an attractive    security.\\n                  income and risk          \\n                  adjusted return          \\n                  alternative.\"            \\n Phil Blancato,   Healthcare - \"We expect  Alphabet - Sees\\n CEO of           this sector to be        healthcare and\\n Ladenburg        somewhat sheltered from  artificial\\n Thalmann Asset   changes in               intelligence\\n Management       macroeconomic data and   becoming a major\\n                  more so driven by        revenue source.\\n                  secular and industry     \\n                  trends such as an aging  \\n                  population and a         \\n                  relatively inelastic     \\n                  demand for healthcare    \\n                  services.\"               \\n Jake                                      Walt Disney - \"If\\n Dollarhide, CEO                           Netflix can be one\\n of Longbow                                of the most\\n Asset                                     successful stocks of\\n Management                                this decade with\\n                                           streaming, then\\n                                           Disney, already with\\n                                           theme parks, ESPN,\\n                                           box office movies\\n                                           and standard Disney\\n                                           everything else, can\\n                                           certainly dominate\\n                                           the next decade with\\n                                           Disney+, with\\n                                           Marvel, Star Wars,\\n                                           Pixar and all of the\\n                                           classic Disney\\n                                           content.\"\\n Dan Morgan,      Gold - Likes it as       Salesforce.com\\n portfolio        stability during                 - \"The\\n manager at       economic strife and as   enterprise shift to\\n Synovus Trust    a long-term inflation    software-as-a-servic\\n                  hedge.                   e (SaaS) computing\\n                                           is in its early\\n                                           days.\"\\n Peter Tuz,                                Walt Disney - \"The\\n President of                              way its products are\\n Chase                                     delivered will\\n Investment                                certainly change\\n Counsel                                   over the next decade\\n                                           but the content that\\n                                           the company creates\\n                                           has proved to be of\\n                                           timeless value.\"\\n Robert Phipps,   International equities,  SPDR S&P Biotech ETF\\n Director at Per  particularly emerging            - “80 to 85\\n Stirling         markets - \"Less trade    percent of those\\n Capital          friction and a           companies are likely\\n Management       weakening dollar will    takeout targets for\\n                  be very friendly to      big biotech\\n                  emerging markets over    companies. It’s an\\n                  the next decade or so,   area where there\\n                  and valuations are very  will be tremendous\\n                  cheap.\"                  change and dramatic\\n                                           discoveries over the\\n                                           next ten years.\"\\n Gary Bradshaw,                            Nvidia          -\\n portfolio                                 Sees as a play on \\n manager at                                artificial\\n Hodges Capital                            intelligence, plus\\n Management                                strong gaming and\\n                                           data center\\n                                           business.\\n Shannon          Capitalizing on U.S.     Alphabet - \"It will\\n Saccocia, Chief  residents moving from    continue to be on\\n Investment       coastal cities to        the cutting edge of\\n Officer at       middle America, like     technology.\"\\n Boston Private   tech infrastructure and  \\n                  housing - \"Residential   \\n                  (real estate), whether   \\n                  single-family or         \\n                  multifamily, outside of  \\n                  core markets is one of   \\n                  the biggest              \\n                  opportunities over the   \\n                  next decade.\"            \\n \\n    \\n    \\n\\n (Additional reporting by April Joyner, Sinead Carew and\\nCaroline Valetkevitch in New York, Editing by Rosalba O\\'Brien)\\n  \\nOur Standards:The Thomson Reuters Trust Principles',\n",
              "  array([-3.3011966 , -1.4575207 , -0.31423384], dtype=float32),\n",
              "  2),\n",
              " ('Go, go gadgets has long been the attitude in my house. Perhaps yours, too: A smartphone made it easier to stay in touch. A smart TV streamed a zillion more shows. A smart speaker let you talk to a smart thermostat without getting out of bed. That’s progress, right?\\nNow I’ve got a new attitude: It’s not just what I can get out of technology — I want to know what the technology gets out of me.\\nFor the past year, I’ve been on the trail of the secret life of our data. What happens when you put your iPhone to sleep at night? Does Amazon’s Alexa eavesdrop on your family? Who gets to know where you drive — and where you swipe your credit card?\\nTrying to get straight answers has been, literally, a full-time job. I’ve digested the legal word salad of privacy policies, interrogated a hundred companies and even hacked into a car dashboard to grab my data back. There are lots of stories about online threats, but it feels different watching your personal information streaming out of devices you take for granted. This year I learned there is no such thing as “incognito.” Just stepping out for an errand, I discovered, lets my car record where I shop, what I listen to and even how much I weigh.\\nAD\\nHelp Desk: Ask our tech columnist a question\\nLearning how everyday things spy on us made me, at times, feel paranoid. Mostly, my privacy project left me angry. Our cultural reference points \\xad— Big Brother and tinfoil hats — don’t quite capture the sickness of an era when we gleefully carry surveillance machines in our pockets and install them in our homes.\\nWith each discovery, I’ve looked for ways to change my own relationship with technology. I’ve stopped installing new smart-home devices that let corporations or police log what’s happening at my house. I switched Web browsers and credit cards. When possible, I use a pseudonym or a throwaway email address.\\nStill, I’m going to level with you. After a year of wrestling my data from corporate America, I hardly feel in control. Being paranoid isn’t enough to save us in the age of surveillance.\\nAD\\nBut no, privacy isn’t dead. A path to reclaiming it — fuzzy and almost too late — is starting to emerge. We just have to be angry enough to demand it.\\nData is power\\nWhile we’re busy living increasingly online lives, it’s hard to know what’s at stake in our data.\\nMost of the headlines focus on leaks and the unintended consequences of data collection, like hackers stealing credit card numbers. You hear about creepy but vague violations, like when Apple and Amazon hired people to review recordings taken from their voice assistants. In a world where so many others are collecting our personal data, it’s legitimate to worry whether they’re doing enough to protect it.\\nBut there’s a more fundamental problem: Why is so much of our information being collected in the first place?\\nWhen I began my privacy project, I learned something about the now-ubiquitous Alexa I hadn’t quite understood when I first brought home an Echo speaker. Every time Amazon’s artificial intelligence activates, it keeps a recording. Amazon had four years of my family’s conversations.\\nAD\\nWe picked the 10 most-influential technologies of the decade. It isn’t all bad.\\nThere’s more: Amazon also keeps reports on appliances you connect to Alexa — in my smart home, every flip of a light switch or adjustment on the thermostat. Last week, Amazon reported that Alexa users received “millions” of doorbell and motion announcements during the 2019 holiday season, “from carolers to delivery drivers and holiday guests.” Surveilling that many homes is a thing the company brags about. (Amazon CEO Jeff Bezos owns The Washington Post, but I review all technology with the same critical eye.)\\nAmazon isn’t building its dossier on you just to be creepy. It wants your voice and your data to train its AI, the technology it hopes will rule our future economy.\\nWhile we’ve been wondering at the new capabilities of connected apps and devices, many of them have been quietly turning our private experiences into their raw materials. These companies act like the data belongs to them, rather than us. Largely unhindered by law, a hidden economy of data brokers gobbles every data morsel it can. Author Shoshana Zuboff gave this data grab a sharp name that I hope sticks: “surveillance capitalism.”\\nAD\\nThere are lots of ways your data can, and will, be used against you. Governments frequently compel companies to hand over what they know. Tracking your credit card lets retailers know how much you’re willing to pay. Tracking what you watch on TV lets politicians micro-target your fears. Tracking your Web surfing lets marketers glimpse your desires — to get you to buy things you may not really need.\\nThese corporations understand that data is a form of power. It’s time to take ours back.\\nThe arms race\\nOpting out is more easily said than done.\\nI tried putting my Alexa speaker on mute, but that defeated the purpose of having a voice-operated assistant in my house. Turning it back on, Amazon would let me delete its recordings of my voice and smart-home activity — but only after the fact, and if I remembered.\\nAD\\nAround every corner in my connected life lay one of these traps. Data-collecting companies, especially when they’re trotted in front of lawmakers, like to say they give us “control.” But often it’s a false choice between forgoing some new capability vs. letting them mine your life. That’s not how technology has to work.\\nIn my privacy project, I found that every swipe or tap of a credit card lets as many as a half-dozen kinds of companies grab information about what, where and how much we spend. Since I can’t live without a credit card, I switched most of my purchases to the new Apple Card, which restricts its bank, Goldman Sachs, from selling customer data.\\nThat’s good, but Apple didn’t do anything to stop data collection by the Mastercard network its card runs on, or by retailers and point-of-sale system operators. Sometimes companies say they protect our privacy, but I find they often use a narrow definition of privacy. Same for your smartphone: Apple brags, “What happens on your iPhone stays on your iPhone,” but doesn’t stop app makers from sending your personal information to third-party tracking companies.\\nAD\\nFacebook, Google and lots of other data-collecting companies offer privacy control panels that hardly anybody ever uses. I don’t blame anyone for keeping away: I’ve tried adjusting the terrible default settings for Google, Facebook and Amazon, but the companies keep changing the controls and the types of information they collect. Using a virtual private network, or VPN, doesn’t do much to stop them from grabbing data from a device you use while logged in to one of their services.\\nHands off my data! 15 default privacy settings you should change right now.\\nThe arms race is exhausting. After I discovered how much Google’s Chrome let tracking cookies ride shotgun while I browsed the Web, I switched to Mozilla’s Firefox, which has default cookie-tracking protection. But even it struggles to defeat a newer, more pernicious form of tracking called fingerprinting, already used on a third of the most-popular sites.\\nThe truth is, most of us don’t have the time or expertise to defend ourselves from the smartest minds in Silicon Valley, many of whom say they want to improve the world but hooked their own financial success onto grabbing as much data as possible.\\nAD\\nData co-pilots\\nWe won’t regain our privacy if we leave it up to individuals. If we’re going to survive the age of surveillance, we’re going to need help.\\nThat starts with laws. Privacy isn’t just an individual right. It’s a public good that, when done right, keeps everyone safe, whether they’re paying attention or not. This ought to be obvious: Our data shouldn’t have a secret life.\\nAmerica doesn’t have a broad privacy law, like Europe’s General Data Protection Regulation, or GDPR. But after years of U.S. lawmakers just talking about data, we’re starting to see some action. So far, that has come mostly in the form of regulatory fines. We should demand laws that not only require companies to come clean about what they’re taking but also place some limits on it.\\nThe new Apple upgrade rule: Why the iPhone 11 is — or isn’t — for you\\nStarting in January, California will bring us closer to a general data law with its new California Consumer Privacy Act, or CCPA. It treats our data like we own it, and gives California residents new powers to demand that companies show us what they’ve collected and who they share it with. It might force some (but not all) of the companies I investigated in my privacy project to open up.\\nAD\\nTransparency means that vigilant citizens \\xad— and pushy journalists — can hold companies accountable through public debate about what sorts of data collection are acceptable. Transparency is also good for business: It helps consumers trust what’s happening behind the digital curtains.\\nBut better seeing our data gets us only so far. My inbox is already flooded with updated privacy policies and data disclosures from companies rushing to comply with the CCPA. Managing all the data I generate is more than even I can handle.\\nWhen we’re sick, we go to a doctor. To keep our computers safe, we install anti-virus software. We rely on professionals to help out with lots of complex aspects of modern life: Why not have professionals help with data, too? Call them your privacy co-pilot.\\nA fledgling privacy service called Jumbo shows what’s possible. From your phone, it logs into Google, Facebook, Amazon and others and spruces up your privacy on your behalf. In clear language and colorful illustrations, it explains the real choices we have and makes recommendations like you’d get from a really clued-in friend. It’s my favorite app of the year.\\nThe first time I used Jumbo, I was shocked that it identified a half-dozen privacy settings for Facebook and Google that even I had missed. Now the app goes in on a regular basis and deletes my Alexa recordings, Google data and Twitter posts, reducing the data trail I leave behind me.\\nRight now Jumbo is tiny and faces an uphill battle when it adds a paid version in the coming months. But the privacy co-pilot market is burgeoning with new ideas, joining the likes of password managers and security-focused WiFi routers. California’s new law smartly carves out protection for third parties to manage our data for us. Now we need clear professional rules for how these companies represent us and prove their trustworthiness.\\nI don’t know exactly how this will evolve. But we’re more likely to win when there are laws that stop data collection from being a secret — and when we have companies fighting to protect our privacy, not just exploit it.\\nRead more from our Secret Life of Your Data series:\\nAlexa has been eavesdropping on you this whole time\\nIt’s the middle of the night. Do you know who your iPhone is talking to?\\nThe spy in your wallet: Credit cards have a privacy problem\\nGoodbye, Chrome: Google’s Web browser has become spy software\\nI found your data. It’s for sale.\\nYou watch TV. Your TV watches back.\\nThink you’re anonymous online? A third of popular websites are ‘fingerprinting’ you.\\nWhat does your car know about you? We hacked a Chevy to find out.',\n",
              "  array([-4.3350825, -3.4229112, -0.0467962], dtype=float32),\n",
              "  2),\n",
              " ('InsightFinder, a startup from North Carolina based on 15 years of academic research, wants to bring machine learning to system monitoring to automatically identify and fix common issues. Today, the company announced a $2 million seed round.\\nIDEA Fund Partners, a VC out of Durham, N.C., led the round, with participation from Eight Roads Ventures and Acadia Woods Partners. The company was founded by North Carolina State University professor Helen Gu, who spent 15 years researching this problem before launching the startup in 2015.\\nGu also announced that she had brought on former Distil Networks co-founder and CEO Rami Essaid to be chief operating officer. Essaid, who sold his company earlier this year, says his new company focuses on taking a proactive approach to application and infrastructure monitoring.\\n“We found that these problems happen to be repeatable, and the signals are there. We use artificial intelligence to predict and get out ahead of these issues,” he said. He adds that it’s about using technology to be proactive, and he says that today the software can prevent about half of the issues before they even become problems.\\nIf you’re thinking that this sounds a lot like what Splunk, New Relic and Datadog are doing, you wouldn’t be wrong, but Essaid says that these products take a siloed look at one part of the company technology stack, whereas InsightFinder can act as a layer on top of these solutions to help companies reduce alert noise, track a problem when there are multiple alerts flashing and completely automate issue resolution when possible.\\n“It’s the only company that can actually take a lot of signals and use them to predict when something’s going to go bad. It doesn’t just help you reduce the alerts and help you find the problem faster, it actually takes all of that data and can crunch it using artificial intelligence to predict and prevent [problems], which nobody else right now is able to do,” Essaid said.\\nFor now, the software is installed on-prem at its current set of customers, but the startup plans to create a SaaS version of the product in 2020 to make it accessible to more customers.\\nThe company launched in 2015, and has been building out the product using a couple of National Science Foundation grants before this investment. Essaid says the product is in use today in 10 large companies (which he can’t name yet), but it doesn’t have any true go-to-market motion. The startup intends to use this investment to begin to develop that in 2020.',\n",
              "  array([-4.714202  , -3.9879122 , -0.02789077], dtype=float32),\n",
              "  2),\n",
              " (\"Beth Galetti, Senior Vice President of Human Resources at Amazon, speaks during the Wall Street Journal CEO Council, in Washington, U.S., December 10, 2019. REUTERS/Al Drago\\nNEW YORK (Reuters Breakingviews) - Chief executives from carmakers to consumer giants sound surprisingly similar when talking about the future. Artificial intelligence, machine learning and automation crop up, as does the challenge of finding staff qualified to carry out such grand strategies. That gives an unlikely back-office function the power to make or break tech-driven ambitions in 2020: human resources.\\nIt’s usually a department with limited scope, performing the grunt-work of hiring, setting workplace-conduct policies and policing unacceptable behavior. At a more senior level, it also involves devising compensation packages to encourage and reward service – sometimes to excess.\\nThe focus is shifting. Over four-fifths of corporate executives and HR bosses surveyed by Randstad Sourceright expect artificial intelligence and robotics to create employment opportunities. Mentions of AI rose almost fivefold between 2012 and 2017 on the earnings calls of non-tech New York-listed companies, reckons Stanford University’s AI Index.\\nYet hiring the best tech talent pits, for example, Walmart, whose jobs website lists almost 700 vacancies for software developers, against Alphabet and Amazon. The median annual salary for a U.S. software developer with five years’ experience is over $100,000, according to Stack Overflow. CEOs outside of finance, who are not used to paying handsomely for junior staff, have to get comfortable with higher wage bills.\\nCompanies also face frequent battles to stop employees decamping elsewhere. Software firms have higher staff turnover than those in other sectors, LinkedIn found. CEOs hoping to attract and keep coders may therefore have to offer more appealing benefits, like Dropbox’s unlimited vacation, or help paying off student loans, or better parental leave. Appealing to millennials’ much-hyped sense of social responsibility might also work. Ford Motor could pitch that working on self-driving cars is more beneficial to society than building new phone software. Procter & Gamble can talk up the kudos of creating environmentally friendly shampoo.\\nYet CEOs will also have to accept that tech-savvy staff increasingly want multiple changes of job, rather than long stints at one firm. Specialty chemicals group Covestro, for example, set up a tech hub away from its German headquarters in Leverkusen, and assumes those who join will remain for 18 months at best. That means the hunt for good coders may be endless, and that hiring the right hirers will become the most crucial job in 2020.\\nThis is a Breakingviews prediction for 2020. To see more of our predictions, click here: bit.ly/2Qz9lz9\\nBREAKINGVIEWS\\nReuters Breakingviews is the world's leading source of agenda-setting financial insight. As the Reuters brand for financial commentary, we dissect the big business and economic stories as they break around the world every day. A global team of about 30 correspondents in New York, London, Hong Kong and other major cities provides expert analysis in real time.\\n\\nSign up for a free trial of our full service at https://www.breakingviews.com/trial and follow us on Twitter @Breakingviews and at www.breakingviews.com. All opinions expressed are those of the authors\",\n",
              "  array([-2.1435225, -0.3762342, -1.6280228], dtype=float32),\n",
              "  1),\n",
              " (\"OPINION\\nAs Iraq violence flares, how do we stop wars from happening in the 2020s?\\nLionel Beehner Opinion columnist\\nPublished 2:21 PM EST Dec 31, 2019\\nThe sign along Manhattan’s West Side Highway is unmistakable: C-3PO glares down, next to him the Star Wars logo reimagined as a stop sign that says, “Stop Wars.”\\nAs the 2020s get underway, are we any closer to ending wars?\\nThe sad fact is that there are more conflicts raging than when we rang in the last decade. Cities are ablaze in anti-government protests. Sure, violence is down from previous centuries, a point heralded by folks like Harvard’s Steven Pinker, but conflict is as prolific as ever.\\nEven the United States' highly secure embassy in Baghdad was breached by Iraqis protesting US airstrikes that targeted Iranian proxies in Iraq.\\nWhy? Why hasn’t man figured out the futility of solving problems with guns?\\nThe answer is too complicated to unpack but a few pieces of evidence are worth unraveling.\\nU.S. troops in every corner of the planet\\nFirst, the US military is virtually everywhere in the globe. If that weren’t the case, one can imagine that war might even be more frequent and more intense. Ditto for UN peacekeepers, operating across 13 missions in the world’s worst troublespots.\\nThe flipside to this argument is that US forces — and to some degree UN peacekeepers — do not always keep the peace, and in some ways contribute to conflict, either by arming unsavory actors (Saudis, etc.) or by creating perverse moral hazards — why train my own forces if Uncle Sam can protect me? — and leaving security vacuums (Europe, Asia-Pacific, etc.). With the US about to pull its forces out of Africa, there is also concern that it will leave behind security vacuums filled by Chinese workers or Russian mercenaries like the Wagner Group.\\nProtesters in front of the U.S. Embassy compound in Baghdad, Iraq, on Dec. 31, 2019.\\nKhalid Mohammed/AP\\nHere’s a gloomy prediction: The US will not “win” another war, at least not in my lifetime.\\nThe US has to get off its early-20th century high-horse that it can somehow still win conflicts. The wars of tomorrow are unwinnable. At best all we can hope to do is manage them.\\nThat simple fact has huge ramifications for our ballooning defense budget, which is still oriented to fighting and winning conventional wars against “near-peer competitors,” which is US military-speak for Russia, China, North Korea, and Iran. But all we need is enough firepower to deter these potential foes and to extend deterrence among our allies. There is no need to spend $738 billion on expensive fighter jets or space programs that will not make us any safer. The second half of the 20th century is littered with money pits and military pipedreams — remember the Future Combat Systems?\\nAfghanistan peace process: 2020 presidential hopefuls owe Afghan women their support\\nWe now sit at the edge of a precipice in modern war: with artificial intelligence and social media reshaping the character of warfare, there is a real risk that the US government will dump untold billions into unproven technologies, simply to keep up with the Russians and Chinese, without partnering with the private sector or enlisting the best and brightest in these fields.\\nDefense dollars would be more wisely spent on fixing our schools, improving our K-12 education, and making college more affordable. We are losing to these countries when it comes to teaching the next generation of scientists, engineers, military planners and policymakers.\\nOur expectations of victory must be more realistic\\nMoney lavished on the Pentagon could also have gone toward revitalizing our broken diplomatic corps. To avoid war requires dedicated civilian diplomats on the ground and properly staffed embassies to take preventive action. A case in point is Ethiopia, one of Africa’s rare success stories. Yet, as Robert Malley of the International Crisis Group points out, the country faces a real risk of splintering along ethnic lines, a war that would spread.\\nStopping war is a noble goal but untenable option. Ignoring conflicts like those in Syria, Yemen, or Ukraine is not an option. Nor is intervening militarily in every brushfire. There are those who think that a well-armed United States surrounded by high walls is the only way to fully secure the homeland and protect our allies.\\nThis is a recipe for failure, bankruptcy, and free-riding behavior. The US has tinkered with a grand strategy that ultimately intervenes where US vital national security interests are present — such as the Middle East or Indo-Pacific region.\\nYet, as Harvard’s Joseph Nye and others have pointed out, our main ace in the hole is not our military strength but our American values of inclusivity, pluralism, and liberty — sometimes referred to as “soft power.” Boots on the ground are not what’s needed.\\nUSA TODAY Editorial Board: Donald Trump wants to end endless wars. Flirting with isolationism won't get him there.\\nWe are too unimaginative when it comes to predicting tomorrow’s conflicts and taking preventive action. A recent survey by the Council on Foreign Relations queried experts yet found a list of conflict zones my 7-year-old son could have conjured up: North Korea, the Middle East, Venezuela.\\nThe world remains a dangerous place. It does not need a tougher sheriff, issuing empty threats by Twitter or arming itself with the biggest weapon systems. It needs leadership, backed by a cadre of experienced diplomats, that can manage festering crises before they escalate.\\nA more realistic sign for C-3PO in the year 2020 would be one that read: “Manage Wars.”\\nLionel Beehner, a member of USA TODAY's Board of Contributors, is an assistant professor and director of research at West Point's Modern War Institute.\\nPublished 2:21 PM EST Dec 31, 2019\",\n",
              "  array([-1.6505027, -0.9864249, -0.8320894], dtype=float32),\n",
              "  2),\n",
              " ('Is your new year\\'s resolution to find true love? If so, you may well be considering a dating app for your phone. Thanks to trailblazers like Tinder, online dating has gone mainstream. And mature daters want in on the action too.\\nLorna is 62 and lives in Edinburgh where she works as a PA. She is a divorcee with children and grandchildren and is still looking for love.\\n\"You come home from work, you want to be talking to someone about your day, it\\'s good for your mental health,\" she explains. \"It\\'s the reason why I\\'m still dating at my age.\"\\nBut it becomes harder to meet potential partners as you get older, she says. \"For women of my generation you can\\'t just walk into a pub by yourself, because that sends a certain message.\"\\nShe became an early adopter of dating apps like Tinder, drawn to the way you can build up a rapport privately through messaging, before committing to meet in person.\\nIt was exciting rather than frightening, publishing her profile for the first time, she remembers.\\nShe has met some lovely people, but the experience also left her feeling uncomfortable.\\n\"I was often approached by people much younger than me, people that are the age of my children. I found that creepy and unsettling. I didn\\'t even like the thought that they\\'ve looked at my profile.\"\\nSo the idea of a dating app that was age-restricted appealed to her. She tried signing up to one called Lumen, which is for over-50s. It\\'s just one player in a growing market for mature daters.\\nOnline dating is a billion dollar global industry that has enjoyed huge growth in recent years thanks to mobile phones. The industry is expected to grow by as much as 30% over the next two to three years, according to research group eMarketer.\\nTinder brought online dating into the mainstream after it launched in 2012, helping to remove any stigma around it. In the USA the number of people using dating apps shot up from 15 to 25 million between 2015 and 2018.\\nMatchmaking companies used to cater primarily for people in their late 20s to early 40s, who were looking to get married or meet a long-term partner. The big growth now is at either end of the spectrum: the 18-24 market and those over 50.\\nIn the 2013-15 period, when Tinder was taking off, online dating tripled for that younger demographic; and in the same period it doubled for those aged 55-64, according to the Pew research centre.\\nLumen stands out because, like Tinder it only exists as an app. It is designed first and foremost for the mobile phone.\\n\"If you look on social media sites like Facebook, the audience is getting older, so it\\'s a natural extension that they would meet people online for dating,\" says Charly Lester, founder of Lumen.\\nShe founded it in London in 2018, believing she had spotted a gap in the market. She knew lots of people in their 50s and 60s who were just as phone-fixated as the younger generation.\\nHer app has attracted more than 1.5 million users in less than a year.\\nIt\\'s free to join, but the company makes money by charging users for \"premium\" features. These include the ability to search potential partners with \"advanced filters\" like \"height, children, smoking, politics and star sign\" and the chance to see who has saved you as a \"favourite\".\\nMore established companies also cater for mature daters, like OurTime and Silver Singles. They began as website platforms but have now developed apps. OurTime says it has seen a 146% increase in active users in the past two years, with 63% of joiners registering by mobile.\\nMedia caption\\nCan a 30-something launch a dating app for over-50s?\\nSome dating apps wouldn\\'t let you sign up if you were older than 50, so people lied about their age, says Ms Lester. \"Another reason we started the app is that a lot of women were complaining that men their own age didn\\'t want to date them any more, they wanted to date significantly younger.\"\\nYou do find men in their eighties chasing the \"youngsters\" in their 50s, she says, but you can use age-range filters to set who can see your profile.\\nSo do mature daters behave differently to younger ones?\\nThere is a divergence in what men and women want in later life, says Nicola Fox Hamilton, who lectures in cyber-psychology at Dublin\\'s Institute of Art, Design and Technology.\\n\\nNicola Fox Hamilton researches online dating\\nWomen are looking for companionship but without a caring role, because often they are just escaping from that and value freedom, she says.\\nMen are more focused on committed relationships at this age, she thinks. And they are often struggling with more narrow social networks in real life, so online dating is a lifeline for them.\\nIt\\'s also a myth that people are not interested in sex at this age, she says, with women placing only a slightly lesser value on it than men.\\nScams\\nHowever, this age group is more prone to \"catfishing\" and romance scamming, she warns.\\nCatfishing involves people creating fake profiles to lure people into a relationship. When this is a scam, the goal is to extract money from the unsuspecting person.\\nPeople have lost anything from £50-£800,000, with many losing more than £5,000, says Ms Fox Hamilton.\\nA typical scam account might involve an attractive man serving in the military - or any kind of scenario that explains why they struggle to meet in person.\\nMany victims don\\'t believe it has happened until the police tell them, she says, and even then they struggle to take it in. Many who do realise it has happened don\\'t report it out of shame.\\nLumen acknowledges that scamming is an issue in this market. It combats it with artificial intelligence software, and a \"selfie\" registration system which makes users take a photograph of themselves when they register, and compares it to the profile photos they then upload to ensure they are genuine.\\nA team of just 15 look after a site where 1.5 million are looking for love, and human intervention is sometimes required to spot people joining in bad faith.\\nBack in Edinburgh, Lorna says she is always careful to follow the sensible safety precautions recommended by dating sites - she thinks would-be mature daters shouldn\\'t be deterred.\\nMany of her friends have found successful relationships through online dating, including marriage. She hopes she will be next and in the meantime she enjoys the excitement, she says.\\n\"Each time someone sends a message, my heart lurches,\" she confesses. \"It\\'s like when you see someone across the dance floor.\"\\nYou can follow Dougal on Twitter: @dougalshawbbc',\n",
              "  array([-4.516921  , -3.6856134 , -0.03666852], dtype=float32),\n",
              "  2),\n",
              " (\"How to stop AI from perpetuating harmful biases\\nby THE CONVERSATION — 20 days ago in SYNDICATION\\n62\\nSHARES\\nArtificial Intelligence (AI) is already re-configuring the world in conspicuous ways. Data drives our global digital ecosystem, and AI technologies reveal patterns in data. Smartphones, smart homes, and smart cities influence how we live and interact, and AI systems are increasingly involved in recruitment decisions, medical diagnoses, and judicial verdicts. Whether this scenario is utopian or dystopian depends on your perspective.\\nThe potential risks of AI are enumerated repeatedly. Killer robots and mass unemployment are common concerns, while some people even fear human extinction. More optimistic predictions claim that AI will add US$15 trillion to the world economy by 2030, and eventually lead us to some kind of social nirvana.\\nWhat's new with the iPhone 11?\\nVolume 0%\\n*chirp chirp*\\nWho’s there? It’s early bird tickets to TNW2020\\nCOME IN\\nWe certainly need to consider the impact that such technologies are having on our societies. One important concern is that AI systems reinforce existing social biases – to damaging effect. Several notorious examples of this phenomenon have received widespread attention: state-of-the-art automated machine translation systems which produce sexist outputs, and image recognition systems which classify black people as gorillas.\\nThese problems arise because such systems use mathematical models (such as neural networks) to identify patterns in large sets of training data. If that data is badly skewed in various ways, then its inherent biases will inevitably be learnt and reproduced by the trained systems. Biased autonomous technologies are problematic since they can potentially marginalize groups such as women, ethnic minorities, or the elderly, thereby compounding existing social imbalances.\\nIf AI systems are trained on police arrests data, for example, then any conscious or unconscious biases manifest in the existing patterns of arrests would be replicated by a “predictive policing” AI system trained on that data. Recognizing the serious implications of this, various authoritative organizations have recently advised that all AI systems should be trained on unbiased data. Ethical guidelines published earlier in 2019 by the European Commission offered the following recommendation:\\nWhen data is gathered, it may contain socially constructed biases, inaccuracies, errors and mistakes. This needs to be addressed prior to training with any given data set.\\nDealing with biased data\\nThis all sounds sensible enough. But unfortunately, it is sometimes simply impossible to ensure that certain data sets are unbiased prior to training. A concrete example should clarify this.\\nAll state-of-the-art machine translation systems (such as Google Translate) are trained on sentence pairs. An English-French system uses data that associates English sentences (“she is tall”) with equivalent French sentences (“elle est grande”). There may be 500m such pairings in a given set of training data, and therefore one billion separate sentences in total. All gender-related biases would need to be removed from a data set of this kind if we wanted to prevent the resulting system from producing sexist outputs such as the following:\\nInput: The women started the meeting. They worked efficiently.\\nOutput: Les femmes ont commencé la réunion. Ils ont travaillé efficacement.\\nThe French translation was generated using Google Translate on October 11 2019, and it is incorrect: “Ils” is the masculine plural subject pronoun in French, and it appears here despite the context indicating clearly that women are being referred to. This is a classic example of the masculine default being preferred by the automated system due to biases in the training data.\\nIn general, 70 percent of the gendered pronouns in translation data sets are masculine, while 30% are feminine. This is because the texts used for such purposes tend to refer to men more than women. To prevent translation systems replicating these existing biases, specific sentence pairs would have to be removed from the data, so that the masculine and feminine pronouns occurred 50 percent/50 percent on both the English and French sides. This would prevent the system assigning higher probabilities to masculine pronouns.\\nNouns and adjectives would need to be balanced 50 percent /50 percent too, of course, since these can indicate gender in both languages (“actor,” “actress;” “neuf,” “neuve”) – and so on. But this drastic down-sampling would necessarily reduce the available training data considerably, thereby decreasing the quality of the translations produced.\\nAnd even if the resulting data subset were entirely gender balanced, it would still be skewed in all sorts of other ways (such as ethnicity or age). In truth, it would be difficult to remove all these biases completely. If one person devoted just five seconds to reading each of the one billion sentences in the training data, it would take 159 years to check them all – and that’s assuming a willingness to work all day and night, without lunch breaks.\\nAn alternative?\\nSo it’s unrealistic to require all training data sets to be unbiased before AI systems are built. Such high-level requirements usually assume that “AI” denotes a homogeneous cluster of mathematical models and algorithmic approaches.\\nIn reality, different AI tasks require very different types of systems. And downplaying the full extent of this diversity disguises the real problems posed by (say) profoundly skewed training data. This is regrettable, since it means that other solutions to the data bias problem are neglected.\\nFor instance, the biases in a trained machine translation system can be substantially reduced if the system is adapted after it has been trained on the larger, inevitably biased, data set. This can be done using a vastly smaller, less skewed, data set. The majority of the data might be strongly biased, therefore, but the system trained on it need not be. Unfortunately, these techniques are rarely discussed by those tasked with developing guidelines and legislative frameworks for AI research.\\nIf AI systems simply reinforce existing social imbalances, then they obstruct rather than facilitate positive social change. If the AI technologies we use increasingly on a daily basis were far less biased than we are, then they could help us recognize and confront our own lurking prejudices.\\nSurely this is what we should be working towards. And so AI developers need to think far more carefully about the social consequences of the systems they build, while those who write about AI need to understand in more detail how AI systems are actually designed and built. Because if we are indeed approaching either a technological idyll or apocalypse, the former would be preferable.\",\n",
              "  array([-0.15166889, -2.4305756 , -2.9423616 ], dtype=float32),\n",
              "  0),\n",
              " ('(WEST PALM BEACH, Fla.) — The first phase of a U.S.-China trade agreement will be inked at the White House in mid-January, President Donald Trump announced Tuesday, adding that he will visit Beijing at a later date to open another round of talks aimed at resolving other sticking points in the relationship.\\nThe so-called “Phase One” agreement is smaller than the comprehensive deal Trump had hoped for and leaves many of the thorniest issues between the two countries for future talks. Few economists expect any resolution of “Phase Two” before the presidential election in 2020.\\nAnd the two sides have yet to release detailed documentation of the pact, making it difficult to evaluate.\\nTrump said high-level Chinese government officials will attend the signing on Jan. 15 of “our very large and comprehensive Phase One Trade Deal with China.”\\nChicken Feet Could Herald a New Era for U.S.-China Trade\\nChina has agreed to boost its U.S. goods imports by $200 billion over two years, the U.S. Trade Representative said Dec. 13 when the deal was announced. That includes increased purchases of soybeans and other farm goods that would reach $40 billion a year.\\nChina has also agreed to stop forcing U.S. companies to hand over technology and trade secrets as a condition for gaining access to China’s vast market, demands that had frustrated many U.S. businesses.\\nIn return, the Trump administration dropped plans to impose tariffs on $160 billion of Chinese goods, including many consumer items such as smartphones, toys and clothes. The U.S. also cut tariffs on another $112 billion of Chinese goods from 15% to 7.5%.\\nMany analysts argue that the results are fairly limited given the costs of the administration’s 17-month trade war against China. U.S. farm exports to China fell in 2018 to about one-third of the peak reached six years earlier, though they have since started to recover.\\nImport taxes remain on about half of what the U.S. buys from China, or about $250 billion of imports. Those tariffs have raised the cost of chemicals, electrical components and other inputs for U.S. companies. American firms have cut back on investment in machinery and other equipment, slowing the economy’s growth this year.\\nA study last week by economists at the Federal Reserve found that all of the Trump administration’s tariffs, including those on steel and aluminum as well as on Chinese imports, have cost manufacturers jobs and raised their costs. That’s mostly because of retaliatory tariffs imposed by China and other trading partners.\\nMany experts in both the U.S. and China are skeptical that U.S. farm exports can reach $40 billion. The most the U.S. has ever exported to China before has been $26 billion. China has not confirmed the $40 billion figure.',\n",
              "  array([-2.767519 , -0.2475624, -1.8548214], dtype=float32),\n",
              "  1),\n",
              " ('OPINION\\n2020 resolution for Trump, Democrats and us: Let go of the past, adapt to the future\\nPaul Brandus Opinion columnist\\nPublished 8:40 AM EST Jan 1, 2020\\nI often think of some of the last words spoken by President John F. Kennedy. “We would like to live as we once lived,” he remarked at a breakfast hours before his assassination, “but history will not permit it.” \\nNor will history permit us to live today as we once did. Events intrude, as they did for JFK that day. But this hasn’t stopped politicians — on both sides — from trying to turn back the clock. For example, here we are with one-fifth of the 21st century behind us, and Democrats running for president (most of them anyway) talk of labor-intensive manufacturing and unions, while President Donald Trump praises coal and tries to bring back incandescent light bulb technology dating to 1879. Why not bring back the rotary phone and black-and-white TV while we’re at it? Is that the best they can do? \\nTheir nostalgia is understandable. Technology is moving faster than mainstream comprehension; it threatens vast disruption to what we know. This, combined with the very human desire for simple solutions and an avoidance of sacrifice, gives political fuel to those who maintain that answers to the future can be found in the past. \\nEdison would be first to look forward\\nBut life marches on whether we like it or not, and adaptability, not clinging to yesterday, is required.\\nFor example, Democrats should talk more about how to respond to the principal destroyers of jobs today: robotics and artificial intelligence, which are beginning to chew up even once safe white-collar jobs (to be fair, at least one Democrat, Andrew Yang, is talking about this).\\nAnd Trump should acknowledge that bankruptcies in the coal industry have accelerated on his watch while the cost of LED bulbs and renewable energy continues to fall. The future is clear, whether he can admit it or not. \\nSurely, no one would be more contemptuous of these desires to turn back time than the very inventor of incandescent lighting himself: Thomas Edison. Never one to be nostalgic about the past, the relentless inventor was always, always looking ahead. \"There\\'s a way to do it better,” the wizard of Menlo Park said. “Find it.”\\nWashington, D.C.\\nJulio Cortez/AP\\nAt the turn of the 21st century, 69% of Americans were satisfied “with the way things are going in the United States.” Today, it’s half that. According to Gallup data, satisfaction began to rebound during the Obama era and has strengthened during the Trump years, but even so: We have fallen far during this century.\\nWe must indeed do things better, as Edison said, and we must find a way to do them together. If you’re trying to get somewhere, it’s best to keep your eyes on the road ahead, not the rearview mirror. Accordingly, as we begin a new and perhaps perilous decade, I think we should acknowledge some problems that have become all too apparent and need addressing.  \\nOur personal liberties are being challenged and various studies — from Freedom House, the Cato Institute and the Heritage Foundation, to name three — note that numerous other countries are freer then we are. Freedom House says that “in recent years (America’s) democratic institutions have suffered erosion,” while Heritage says that based on a dozen categories, the United States is 76.8% economically free — below 11 other countries. \\nWe can do better.  \\nDebt, oil, space: Three things that will be important over the next decade\\nOur politicians brag about how great things are and how we\\'ve never had it better. Yet life expectancy in the United States has fallen three years in a row. \\nWe can do better. \\nWe love to whine about the state of things, but only 61% of eligible voters bothered to cast a ballot in 2016.\\nWe can do better. \\nWe demand good roads and schools, clean water, health care and all the rest but complain that taxes are too high (fact check: not so).\\nWe can do better. \\nHelp make things better in 2020\\nWe have become meaner and less civil. We associate only with our own kind while shunning, belittling, even dehumanizing others who are different. In doing so, we demean our individual character, weaken our communities and tear at our national fabric.   \\nWe can do better. \\nWe call ourselves patriots and claim to love the Constitution — though most of us likely can only describe its First or Second Amendments. \\nWe can do better. \\nWe consider ourselves knowledgeable and informed, yet surveys repeatedly show vast ignorance about things that matter. Only about one-third of Americans, for example, can name all three branches of government. Another study found that “more Americans could identify Michael Jackson as the composer of ‘Beat It’ and ‘Billie Jean’ than could identify the Bill of Rights as a body of amendments to the U.S. Constitution.” \\nAnd everyone seems to have an opinion on Ukraine. Too bad hardly anyone can find it on a map. \\nAn unfit president: From hush money to Trump impeachment, 2019 was a dizzying year of corruption and scandal\\nPerhaps we should acknowledge that we\\'re not as smart as we presume, and that this collective ignorance makes it harder to solve problems. \\nWe can do better.\\nThere\\'s more, but you get the point. We’re dealing with deep rooted, often intertwined problems. And while electing a president who conveys civility and good old-fashioned American decency can make a huge difference, we can also be part of the change ourselves. To begin, we can acknowledge our own shortcomings, stop blaming others for our problems and simply resolve to do better by others. As Edison said, we must find a way.\\nWe’re all in this together. I wish you a wonderful 2020 and beyond. \\nPaul Brandus, founder and White House bureau chief of West Wing Reports, is the author of \"Under This Roof: The White House and the Presidency\" and is a member of USA TODAY\\'s Board of Contributors. Follow him on Twitter: @WestWingReport\\nPublished 8:40 AM EST Jan 1, 2020',\n",
              "  array([-0.20149636, -1.9620013 , -3.1720724 ], dtype=float32),\n",
              "  0),\n",
              " ('CHICAGO (Reuters) - A Google artificial intelligence system proved as good as expert radiologists at detecting which women had breast cancer based on screening mammograms and showed promise at reducing errors, researchers in the United States and Britain reported.\\nThe study, published in the journal Nature on Wednesday, is the latest to show that artificial intelligence (AI) has the potential to improve the accuracy of screening for breast cancer, which affects one in eight women globally.\\nRadiologists miss about 20% of breast cancers in mammograms, the American Cancer Society says, and half of all women who get the screenings over a 10-year period have a false positive result.\\nThe findings of the study, developed with Alphabet Inc’s (GOOGL.O) DeepMind AI unit, which merged with Google Health in September, represent a major advance in the potential for the early detection of breast cancer, Mozziyar Etemadi, one of its co-authors from Northwestern Medicine in Chicago, said.\\nThe team, which included researchers at Imperial College London and Britain’s National Health Service, trained the system to identify breast cancers on tens of thousands of mammograms.\\nThey then compared the system’s performance with the actual results from a set of 25,856 mammograms in the United Kingdom and 3,097 from the United States.\\nThe study showed the AI system could identify cancers with a similar degree of accuracy to expert radiologists, while reducing the number of false positive results by 5.7% in the U.S.-based group and by 1.2% in the British-based group.\\nIt also cut the number of false negatives, where tests are wrongly classified as normal, by 9.4% in the U.S. group, and by 2.7% in the British group.\\nThese differences reflect the ways in which mammograms are read. In the United States, only one radiologist reads the results and the tests are done every one to two years. In Britain, the tests are done every three years, and each is read by two radiologists. When they disagree, a third is consulted.\\n‘SUBTLE CUES’\\nIn a separate test, the group pitted the AI system against six radiologists and found it outperformed them at accurately detecting breast cancers.\\nConnie Lehman, chief of the breast imaging department at Harvard’s Massachusetts General Hospital, said the results are in line with findings from several groups using AI to improve cancer detection in mammograms, including her own work.\\nThe notion of using computers to improve cancer diagnostics is decades old, and computer-aided detection (CAD) systems are commonplace in mammography clinics, yet CAD programs have not improved performance in clinical practice.\\nThe issue, Lehman said, is that current CAD programs were trained to identify things human radiologists can see, whereas with AI, computers learn to spot cancers based on the actual results of thousands of mammograms.\\nThis has the potential to “exceed human capacity to identify subtle cues that the human eye and brain aren’t able to perceive,” Lehman added.\\nAlthough computers have not been “super helpful” so far, “what we’ve shown at least in tens of thousands of mammograms is the tool can actually make a very well-informed decision,” Etemadi said.\\nA yellow box indicates where an artificial intelligence (AI) system found cancer hiding inside breast tissue, in an undated photo released by Northwestern University in Chicago January 1, 2020. Northwestern University/Handout via REUTERS\\nThe study has some limitations. Most of the tests were done using the same type of imaging equipment, and the U.S. group contained a lot of patients with confirmed breast cancers.\\nCrucially, the team has yet to show the tool improves patient care, said Dr Lisa Watanabe, chief medical officer of CureMetrix, whose AI mammogram program won U.S. approval last year.\\n“AI software is only helpful if it actually moves the dial for the radiologist,” she said.\\nEtemadi agreed that those studies are needed, as is regulatory approval, a process that could take several years.',\n",
              "  array([-4.3628526 , -4.4618716 , -0.02458245], dtype=float32),\n",
              "  2),\n",
              " (\"Google is developing artificial intelligence to help doctors identify breast cancer, according to a research paper published in Nature today. The model, which scans X-ray images known as mammograms, reduces the number of false negatives by 9.4 percent—a hopeful leap forward for a test that currently misses 20 percent of breast cancers, as reported by The New York Times.\\nToday, breast cancer is the second leading cause of death in women, beat out only by lung cancer in its deadliness and overall prevalence. Early detection is the best defense most people have in identifying and treating the disease. Yet while mammograms are the most common detection tool, they miss a large number of cases. “Mammograms are very effective but there’s still a significant problem with false negatives and false positives,” Shravya Shetty, a researcher at Google who co-authored the paper, tells The Verge.\\n“MAMMOGRAMS ARE VERY EFFECTIVE BUT THERE’S STILL A SIGNIFICANT PROBLEM WITH FALSE NEGATIVES AND FALSE POSITIVES”\\nIn the study, which Google funded, researchers used anonymized mammograms from more than 25,000 women in the UK and 3,000 women in the US. “We tried to follow the same principles radiologists might follow,” Shetty says. According to Google’s blog post, the team first trained AI to scan X-ray images, then looked for signs of breast cancer by identifying changes in the breasts of the 28,000 women. They then checked the computer’s guesses against the women's’ actual medical outcomes.\\nUltimately, they were able to reduce false negatives by 9.4 percent and cut down false positives by 5.7 percent for women in the US. In the UK, where two radiologists typically double-check the results, the model cut down false negatives by 2.7 percent and reduced false positives by 1.2 percent. “The model performs better than an individual radiologist in both the UK and the US,” Christopher Kelly, a scientist at Google who co-authored the paper, tells Wired.\\nThe system was not perfect. While researchers found that AI outperformed doctors in identifying breast cancer in most cases, there were also instances where doctors flagged cancer that the model originally missed. “Sometimes, all six U.S. readers caught a cancer that slipped past the AI, and vice versa,” Mozziyar Etemadi, a researcher at Northwestern University and another co-author of the paper, tells The Wall Street Journal.\\nRESEARCHERS FOUND THAT AI OUTPERFORMED DOCTORS IN IDENTIFYING BREAST CANCER IN MOST CASES\\nStill, Google says it’s hopeful the system can eventually be used in clinical settings. “We’re very excited and encouraged by these results,” says Daniel Tse, a product manager at Google who also co-authored the paper. He tells The Verge that the team is currently working to ensure the findings can be generalized across populations. “There’s obviously quite a bit of nuance when you put this into clinical practice,” he adds.\\nGoogle has been careful to frame this project as one that will help radiologists, not replace them. “They each bring their own strength, it’s complementary,” says Shetty. “There are a number of cases where the radiologists catch something that the model misses, and vice versa. Bringing the two together could strengthen the overall results.”\\nThe project is part of Google’s ongoing efforts to expand into the field of healthcare. Earlier this year, the tech giant partnered with Ascension to gain access to the health records of millions of American citizens. That project came under fire after a whistleblower alleged the health records were not being anonymized. For the breast cancer study, Google partnered with clinical researchers in the US and UK, and used data that had already been de-identified.\",\n",
              "  array([-4.4494405 , -4.3876405 , -0.02441063], dtype=float32),\n",
              "  2),\n",
              " ('Artificial intelligence is more accurate than doctors in diagnosing breast cancer from mammograms, a study in the journal Nature suggests.\\nAn international team, including researchers from Google Health and Imperial College London, designed and trained a computer model on X-ray images from nearly 29,000 women.\\nThe algorithm outperformed six radiologists in reading mammograms.\\nAI was still as good as two doctors working together.\\nUnlike humans, AI is tireless. Experts say it could improve detection.\\nHow good is it?\\nThe current system in the NHS uses two radiologists to analyse each woman\\'s X-rays. In rare cases where they disagree, a third doctor assesses the images.\\nIn the research study, an AI model was given anonymised images, so that the women could not be identified.\\nUnlike the human experts, who had access to the patient\\'s history, AI had only the mammograms to go on.\\nThe results showed that the AI model was as good as the current double-reading system of two doctors.\\nAnd it was actually superior at spotting cancer than a single doctor.\\nCompared to one radiologist, there was a reduction of 1.2% in false positives, when a mammogram is incorrectly diagnosed as abnormal.\\nThere was also a reduction of 2.7% in false negatives, where a cancer is missed.\\nDominic King from Google Health said: \"Our team is really proud of these research findings, which suggest that we are on our way to developing a tool that can help clinicians spot breast cancer with greater accuracy.\"\\nMost of the mammograms came from Cancer Research UK\\'s OPTIMAM dataset collected from St George\\'s Hospital London, the Jarvis Breast Centre in Guildford and Addenbrooke\\'s Hospital, Cambridge.\\nIt takes over a decade of training as a doctor and specialist to become a radiologist, capable of interpreting mammograms.\\nReading X-rays is vital but time-consuming work, and there is an estimated shortage of more than 1,000 radiologists across the UK.\\nWill AI take over from humans?\\nNo. It took humans to design and train the artificial intelligence model.\\nThis was a research study, and as yet the AI system has not been let loose in the clinic.\\nEven when it is, at least one radiologist would remain in charge of diagnosis.\\nBut AI could largely do away with the need for dual reading of mammograms by two doctors, easing pressure on their workload, say researchers.\\nProf Ara Darzi, report co-author and director of the Cancer Research UK (CRUK) Imperial Centre, told the BBC: \"This went far beyond my expectations. It will have a significant impact on improving the quality of reporting, and also free up radiologists to do even more important things.\"\\nWomen aged between 50 and 70 are invited for NHS breast screening every three years - those who are older can ask to be screened.\\nThe use of AI could eventually speed up diagnosis, as images can be analysed within seconds by the computer algorithm.\\nSara Hiom, director of cancer intelligence and early diagnosis at CRUK, told the BBC: \"This is promising early research which suggests that in future it may be possible to make screening more accurate and efficient, which means less waiting and worrying for patients, and better outcomes.\"\\n\\nHelen Edwards had breast cancer in her 40s\\nHelen Edwards, from Surrey, was diagnosed with breast cancer at the age of 44, before she was eligible for screening.\\nShe required surgery, chemotherapy and radiotherapy, but has been cancer-free for more than a decade.\\nShe was a patient representative on the CRUK panel which had to decide whether to grant Google Health permission to use the anonymised breast cancer data.\\nHelen told the BBC: \"Initially I was a bit concerned about what Google might do with the data, but it is stripped of all identifiers.\\n\"In the long term this can only benefit women.\\n\"Artificial intelligence machines don\\'t get tired... they can work 24/7 whereas a human being can\\'t do that, so to combine the two is a great idea.\"\\nFollow Fergus on Twitter.',\n",
              "  array([-3.9925635 , -4.2820797 , -0.03279833], dtype=float32),\n",
              "  2),\n",
              " (\"Jan 2 (Reuters) - The following are the top stories in the Wall Street Journal. Reuters has not verified these stories and does not vouch for their accuracy.\\n- U.S. Food and Drug Administration plans to ban the sale of fruity flavors in cartridge-based e-cigarettes, but the restriction won't apply to tank vaping systems commonly found at vape shops, according to people familiar with the matter. on.wsj.com/2QQs9u3\\n- California bankruptcy court has sided with electric utility PG&E Corp in its fight with bondholders over the interest rate that it must pay on its debts while under bankruptcy court protection. on.wsj.com/2u62NjS\\n- An attempt by supporters of Iran-backed militias to storm the U.S. Embassy at Baghdad ended on Wednesday, as protesters withdrew from the area after their leadership ordered the suspension of a violent challenge to American troop presence in Iraq. on.wsj.com/2MPtIqJ\\n- Don Larsen, the journeyman pitcher who reached the heights of baseball glory in 1956 for the New York Yankees when he threw a perfect game and the only no-hitter in World Series history, died Wednesday night. on.wsj.com/2u6qDfj\\n- Google's health research unit said it has developed an artificial-intelligence system that can match or outperform radiologists at detecting breast cancer, according to new research. on.wsj.com/2QI1KhE\\n- Pete Buttigieg raised $24.7 million in the final three months of the year, his campaign said Wednesday, bringing the former South Bend, Indiana mayor's total last year to about $76 million, making him one of the best fundraisers in the Democratic presidential field. on.wsj.com/2MNHyKp (Compiled by Bengaluru newsroom)\\nOur Standards:The Thomson Reuters Trust Principles\",\n",
              "  array([-0.413249 , -1.4197257, -2.3359132], dtype=float32),\n",
              "  0),\n",
              " (\"FILE PHOTO: The logo of Xiaomi is seen inside the company's office in Bengaluru, India, January 18, 2018. REUTERS/Abhishek N. Chinnappa/File Photo\\nBEIJING (Reuters) - Chinese smartphone maker Xiaomi Corp will invest more than 50 billion yuan ($7.18 billion) in artificial intelligence and fifth generation internet technologies over the next five years, as competition in the sector grows.\\nXiaomi Chief Executive Lei Jun made the announcement in a letter posted on the company’s social media account on Thursday, but did not provide specific investment details.\\n“We need to turn our continuous advantage we have in AIoT and intelligent life into absolute victory in intelligent full scene, and completely cement our king status in the smart era,” Lei said, using the acronym for artificial intelligence of things, a reference to a combination of AI and internet technologies.\\nThe announced investment increases Xiaomi’s pledge made last year to invest 10 billion yuan over five years in an “All in AIoT” strategy.\\nThe Beijing-based company started as an affordable cellphone maker but its internet-enabled products now include smart TVs and rice-cookers.\\nHowever, Xiaomi also faces intense competition in its home market from rival Huawei Technologies, which captured a record 42% of China’s smartphone market in the third quarter at the expense of other local manufacturers and Apple.\\nLei announced in October the company planned to launch more than 10 5G phone models in 2020.\\n($1 = 6.9629 Chinese yuan)\",\n",
              "  array([-4.9743257, -3.227065 , -0.047707 ], dtype=float32),\n",
              "  2),\n",
              " ('Xiaomi logos are seen during a news conference in Hong Kong, China June 23, 2018.  REUTERS/Bobby Yip/File Photo\\nBEIJING (Reuters) - Chinese smartphone maker Xiaomi Corp will invest more than 50 billion yuan ($7.18 billion) in artificial intelligence and fifth generation internet technologies over the next five years, as competition in the sector grows.\\nXiaomi Chief Executive Lei Jun made the announcement in a letter posted on the company’s social media account on Thursday, but did not provide specific investment details.\\n“We need to turn our continuous advantage we have in AIoT and intelligent life into absolute victory in intelligent full scene, and completely cement our king status in the smart era,” Lei said, using the acronym for artificial intelligence of things, a reference to a combination of AI and internet technologies.\\nThe announced investment increases Xiaomi’s pledge made last year to invest 10 billion yuan over five years in an “All in AIoT” strategy.\\nThe Beijing-based company started as an affordable cellphone maker but its internet-enabled products now include smart TVs and rice-cookers.\\nHowever, Xiaomi also faces intense competition in its home market from rival Huawei Technologies, which captured a record 42% of China’s smartphone market in the third quarter at the expense of other local manufacturers and Apple.\\nLei announced in October the company planned to launch more than 10 5G phone models in 2020.\\n($1 = 6.9629 Chinese yuan)',\n",
              "  array([-4.737741  , -2.8669105 , -0.06788573], dtype=float32),\n",
              "  2),\n",
              " ('ABUJA/LAGOS (Reuters) - A steady “thwack” rings out at the driving range just outside Nigeria’s capital Abuja. A girl clad in pink shoes, pink skirt and pink vest wields a club with a power that belies her slight frame, grimacing at poor shots and smiling happily at others.\\nIyene Essien’s golf medals already outnumber her 13 years of age, and she has competed on three continents. She is the top junior player in Nigeria, and now wants to deliver her country’s first gold medal for golf at the 2022 Summer Youth Olympics.\\n“In this game you compete against yourself and not other players, which is very exciting,” she told Reuters, adding that anything that goes wrong “is still your mistake, you are the one playing so you cannot blame your caddy or anyone for the mistake you made.”\\nHer journey began when she saw a young white boy playing at the IBB International Golf and Country Club in Abuja. Her father saw her excitement and quickly arranged to get her on the course.\\n“He asked me if I wanted to play and I said yes, so he bought me clubs and got me a golf professional to train me,” she said — the same man who was training the white boy.\\nHer first medal came quickly, at the age of five. She now has 17 medals and has represented Nigeria 11 times at tournaments in Africa, Europe and the United States.\\nPoised and precocious, Essien was the only teenager among 177 golfers at the 2019 Nigeria Ladies Golf Open Championship. She took 10th place, shyly high-fiving the other competitors as the crowd cheered.\\nIyeneobong Essien,13-year-old golf prodigy points at her trophies as she poses for a photograph at home in Abuja, Nigeria October 27, 2019. Picture taken October 27, 2019. REUTERS/Abraham Achirga\\nHer father, consultant economist Eyo Essien, says she is just getting started.\\n“She still has some room for growth and I think as I said by the age of 15 she will be ripe, you know, to take the golf world by storm,” he said.\\nBut Essien said she is committed to balancing school and golf, and intends to study artificial intelligence and robotics at university.\\n“In the next 10 years with a degree in my hand, I will play as a professional golfer,” she said.\\nWriting by Libby George; Editing by Hugh Lawson\\nOur Standards:The Thomson Reuters Trust Principles',\n",
              "  array([-4.2573214 , -3.3961575 , -0.04883519], dtype=float32),\n",
              "  2),\n",
              " (\"Since September, commuters using Shanghai’s Number 9 bus route have had a new way of catching a ride. Rather than stand at a designated stop, they open a smartphone app and book a ride to wherever they're going. The service, provided by Alibaba, takes those reservations into account and calculates where the bus should go, using the company’s artificial intelligence to customize the route. The idea is to boost ridership—and curb traffic—by making public transit more convenient.\\n\\nShanghai is just the latest city to give this sort of scheme a try. From Helsinki, Finland, to Sydney, cities around the world have spent the past few years trying to implement AI-fueled, on-demand bus services. Few have succeeded.\\n\\nEarlier this year, Singapore decided against renewing a pilot for on-demand buses. In Germany, microtransit company CleverShuttle—which bills itself as more of a ride-pooling service than a bus—pulled out of three of the eight cities it was operating in, citing economic and bureaucratic hurdles. In a pilot project with shared rides company Via, bringing underserved residents to public transit nodes, Los Angeles Metro is spending $14.50 per trip—twice what it spends on a regular bus trip.\\n\\nOn-demand buses have been a thing for decades. Public transit agencies often call them demand-responsive buses, and deploy them to serve users who lack easy access to standard routes because they live especially far away, or may have special needs. Because they reach relatively few people, they’re expensive to operate. They’re inefficient too, often making riders wait undetermined amounts of time for a ride. So cities must strike a balance between making public transit accessible to the largest number of residents, and meeting their budget goals.\\n\\n“Elite projection: the mistake of not realizing that you are in the minority by virtue of being elite, which means that you may be in love with something that doesn't actually scale to the whole population.”\\n\\nJARRETT WALKER, PUBLIC TRANSIT CONSULTANT\\n\\nProjects like the one in Shanghai represent a new kind of effort. The new, tech-powered services—sometimes called microtransit, because they use small vehicles—claim to make those routes cost-effective and attractive by pairing transit data with the convenience of a smartphone app. The goal is to help transit agencies reach currently underserved populations, such as people who need a ride home from the train station, night riders, or urbanites who’d like to ditch the car but don’t want to use public transit.\\n\\nAccording to the tech companies pushing this solution, making on-demand busing work is a matter of crunching vast amounts of transit data, now made available by location tracking, and using algorithms to create custom shared routes. Data will help agencies reroute buses in real time based on factors like user demand and congestion, says Amos Haggiag, CEO of Optibus, whose software helps cities plan and manage bus routes, both on-demand and fixed. “I do see mass transit, even the large buses, as much more dynamic.” Many of those companies, including Uber, think all buses, not just those in low-ridership areas, should run on demand.\\n\\nReality, though, adds complications. Not everyone who needs to get around has access to an app. Smartphone ownership remains vastly unequal among countries, and between income and age groups. The cost of data is still cited as a major barrier to smartphone use around the world. And even those who do have phones may not want to rely on them to get to work. When I point out that my smartphone shuts down when the weather gets too cold in winter, Haggiag says my situation is “extreme.” I live in Montreal, along with 1.75 million other people.\\n\\nKeep Reading\\nillustration of a head\\nThe latest on artificial intelligence, from machine learning to computer vision and more\\nTech companies and planners often make decisions without considering the needs of people who are not like them. A pilot project in St. Petersburg, Florida, that let residents use Uber to connect to bus stops faced low adoption rates. The local transit authority realized residents, many of whom were low-income, didn’t know how to use Uber. They needed help on how to use the app, a planner told WIRED in 2017. Elsewhere, “smart city” initiatives have been called out for their lack of inclusivity.\\n\\nThe problem is “elite projection,” according to public transit consultant Jarrett Walker—“the mistake of not realizing that you are in the minority by virtue of being elite, which means that you may be in love with something that doesn't actually scale to the whole population.” That helps explain why Uber, which has undoubtedly improved the quality of life for those who can afford it, is used by many microtransit companies as a baseline for what a transit experience should look like: available on demand and ultra-convenient. “All of these companies have enormous amounts of venture capital, and therefore an enormous ability to shape the conversation in a way that serves their interests,” Walker says.\\n\\n\\nIf you want to know what happens when transit becomes fully Uberized, look to Innisfil. The small city in Ontario didn’t have enough cash to set up a fully fledged transit system, so it partnered with Uber to offer subsidized rides. The scheme has proven so popular that the municipality has had to cap rides and increase fares to keep it going. “They've had too much demand,” says Moaz Ahmad, a Toronto-based public transit consultant. “When anything grows, it will find plateaus, it will find barriers that have to be surmounted.”\\n\\nIf the success of a transit system depends on the number of people who use it, then successful public transit is everything but on-demand transportation. Cities have been able to boost bus ridership by making improvements not on their ability to reach users where they live, but by increasing frequency, speed, and capacity on the busiest routes. They’ve done this by removing stops or spacing them out, building dedicated bus lanes, and decreasing the time it takes for people to get on and off the bus, for instance by letting people board through rear doors. Ridership and on-time performance soared on Manhattan’s 14th Street bus line after efficiency measures were introduced starting in 2018, while travel times dropped 38 percent.\\n\\nNew tech could deliver real value, though, on a transit system’s most important routes. “People are always talking about first mile, last mile, but no one is dealing with all the miles in between,” says Optibus’ Haggiag. Where many transit authorities still rely on spreadsheets and analog planning tools to design bus routes, the Optibus software can input unlimited data sources to calculate optimal routes and schedules. The city of Herzliya, Israel, saw its bus ridership double after redesigning its bus map and increasing frequency, Haggiag adds.\\n\\nTime and again, cities have found that making those main corridors more reliable increases incentives for people to find a way to reach bus stops, whether that’s by riding an Uber, a scooter, or a bike. Most often, though, that’s by walking, which is easier said than done. Denver has drafted a plan to improve walkability after finding that 39 percent of sidewalks within a mile of bus stops are either nonexistent or too narrow. Portland, Oregon, is improving access to transit stops as part of its Vision Zero plan to eliminate traffic deaths, because 32 percent of stops have been found to be unsafe for pedestrians. Sometimes the best way to get people on the bus involves no technology at all.\\n\\nIt may be that on-demand buses or microtransit may be the best solution for a city’s needs, but in most cases the answer starts with good governance. “When you are responding to a marketing pitch, you are not thinking,” Walker says. “Thinking begins by having a moral conversation about your goals, about what kind of city you want, and about what you want life in your city to be.”\",\n",
              "  array([-5.14992   , -4.318068  , -0.01931069], dtype=float32),\n",
              "  2),\n",
              " (\"A study by Google Health says artificial intelligence can spot breast cancer more accurately than doctors. Here's how the development could help patients. (Source: QuickTake)\",\n",
              "  array([-3.6857002, -4.3138375, -0.0392209], dtype=float32),\n",
              "  2),\n",
              " ('AI in 2020 and beyond: create a digital replica of your aging parent or yourself\\nEdward C. Baig USA TODAY\\nPublished 10:38 AM EST Jan 2, 2020\\nBig letters AI made up of circuits over smaller \"ARTIFICIAL INTELLIGENCE\" with a view of Earth with lighted points that are connected\\nGetty Images\\nYou’re racing to the airport, unaware there’s a wreck on the highway ahead. \\nFortunately, an artificial intelligence-driven system in your vehicle is looking after you. The system automatically checks on your flight – still on schedule – and determines your chances of making it to the gate on time are slim. With your permission, it can proactively book an alternate flight. \\n“That’s the true virtual assistant in the future,” says Gartner vice president and fellow David Cearley. “Rather than having conversational interfaces respond to discrete things, it understands the context and can respond to (your) intent.” \\nMuch has been said and written about the future of AI, and the role it will play – good and potentially bad – in practically everything consumers and businesses engage in. What pretty much everyone agrees on is that AI will make a profound difference through the next decade and beyond, during which we may see a further blurring between human and machine.\\nAbout a year ago, researchers at Pew Research Center and Elon University’s Imagining the Internet Center asked the following: “By 2030, do you think it is most likely that advancing AI and related technology systems will enhance human capacities and empower them?”\\nOf the nearly 1,000 technologists who weighed in, about two-thirds predicted most of us will be better off, with a third thinking otherwise. And most expressed at least some concern over the long-term impact of AI on the “essential elements of being human.”\\nAlmost no one disputes the fact that AI will continue to get smarter.\\nYour smart TV is spying on you: Here are step-by-step instructions to stop it\\n$3 gas a thing of the past?: Gas prices likely to stay steady in 2020\\nSriram Raghavan, who heads IBM Research AI, predicts that in 2020 by combining learning with logic, AI will start to develop a “common-sense” reasoning system, to help businesses deploy more conversational automated customer care and technical support tools.\\nFor his part, Jeff Loucks, executive director of Deloitte\\'s Center for Technology, Media & Telecommunications, believes that within 10 years, AI-powered robots may help aging people remain in their homes. And AI embedded in more smart devices will help all of us monitor our health and wellness.\\nThe dark side\\nThose who worry about the dark side, however, fear that AI will result in data abuse, loss of jobs and an erosion in our ability to think for ourselves.\\nAnd AI systems must be trained without prejudice and bias. An NYU study from last year pointed out that the people building out such systems are too white and too male.\\nAlarm bells have been sounded by some of the most famous names in tech.\\nTesla and SpaceX chief Elon Musk has said AI is far more dangerous than nuclear weapons. The late scientist Stephen Hawking warned AI could serve as the \"worst event in the history of our civilization\" unless humanity is prepared for its possible risks.\\nFor some, “deepfakes” are the immediate concern, especially with the 2020 U.S. presidential election coming up. These rather sophisticated “doctored” videos can make it look like a politician said something outrageous, controversial or out of character. \\nThere\\'s even a market for deepfake porn.\\n“We are used to Photoshopped photos by now and are sadly also somewhat used to fake news. But we are really not used to being fooled by our own eyes and ears if we see something on a very crisp and clear video,” says Lars Buttler, CEO of the AI Foundation, an organization that has developed forensics technology to help identify such fakes. \\nA virtual AI you\\nSeparate from all that, AI Foundation is developing “personal AIs,” kind of avatars of famous people, starting with a digital version of author and spiritual adviser Deepak Chopra. On a phone, tablet or computer, this virtual Chopra can recognize you, respond to your questions and even meditate with you, Buttler says. \\n11/28/18 5:45:33 PM -- New York, NY, U.S.A -- Deepak Chopra -- Photo by Robert Deutsch, USA TODAY Staff\\nRobert Deutsch, USAT\\nBy late in the year, though, Buttler believes you’ll be able to create your own personal AIs – perhaps of people close to you like your 5-year-old kid or an elderly parent or grandparent. And you\\'ll be able to create a digital replica of yourself that looks, talks and is trained by you.\\n“A photo tells us what somebody looks like and (is) frozen in time. With a video we also add the elements of the tone of their voice, their mannerisms. But with your own AI, you can literally go back to that point in time and talk to them (or your younger self),” Buttler says. \\nSuch personal AIs might also be used to entertain, teach or, Buttler suggests, become the future AI equivalent of YouTube stars. \\nMeanwhile, Snapchat is applying deepfakery into a new feature called Cameo. It will let you edit your own face into a customizable video loop or GIF.\\nWe may make use of virtual assistants and personas in all sorts of ways.\\nGartner’s Cearley envisions a scenario where you’re in your kitchen cooking a roast, assisted perhaps by none-other-than late celebrity chef Julia Child. You’re not just following some step-by-step video recipe. A virtual Child (or someone like her) is effectively cooking with you, thanks to AI, sensors, and the fact that your oven and other appliances are connected and can talk to one another.\\nAt a critical moment when there\\'s the risk of overcooking the meat, Child might pipe in: “Oh my goodness, it’s time to pull it out,’” she might say. \\nIt is an example of how AI can drive “radical simplification,” Cearley says. “The computer is not something that sits on my desk. The computer is my home, my car and the environment that I’m working through.”\\nEmail: ebaig@usatoday.com; Follow @edbaig on Twitter\\nPublished 10:38 AM EST Jan 2, 2020',\n",
              "  array([-4.021161  , -3.9436796 , -0.03802271], dtype=float32),\n",
              "  2),\n",
              " (\"2010 – 2019: The rise of deep learning\\nby TRISTAN GREENE — 18 days ago in ARTIFICIAL INTELLIGENCE\\nCredit: Nicole Gray\\n87\\nSHARES\\nNo other technology was more important over the past decade than artificial intelligence. Stanford’s Andrew Ng called it the new electricity, and both Microsoft and Google changed their business strategies to become “AI-first” companies. In the next decade, all technology will be considered “AI technology.” And we can thank deep learning for that.\\nDeep learning is a friendly facet of machine learning that lets AI sort through data and information in a manner that emulates the human brain’s neural network. Rather than simply running algorithms to completion, deep learning lets us tweak the parameters of a learning system until it outputs the results we desire.\\nWhat's new with the iPhone 11?\\nVolume 0%\\nBirds are cool\\nEarly birds are even cooler. Get cheap tickets to TNW2020 right now\\nYEAH\\nThe 2019 Turing Award, given for excellence in artificial intelligence research, was awarded to three of deep learning‘s most influential architects, Facebook’s Yann LeCun, Google’s Geoffrey Hinton, and University of Montreal’s Yoshua Bengio. This trio, along with many others over the past decade, developed the algorithms, systems, and techniques responsible for the onslaught of AI-powered products and services that are probably dominating your holiday shopping lists.\\nCredit: CS231N\\nDeep learning powers your phone’s face unlock feature and it’s the reason Alexa and Siri understand your voice. It’s what makes Microsoft Translator and Google Maps work. If it weren’t for deep learning, Spotify and Netflix would have no clue what you want to hear or watch next.\\nHow does it work? It’s actually simpler than you might think. The machine uses algorithms to shake out answers like a series of sifters. You put a bunch of data in one side, it falls through sifters (abstraction layers) that pull specific information from it, and the machine outputs what’s basically a curated insight. A lot of this happens in what’s called the “black box,” a place where the algorithm crunches numbers in a way that we can’t explain with simple math. But since the results can be tuned to our liking, it usually doesn’t matter whether we can “show our work” or not when it comes to deep learning.\\nDeep learning, like all artificial intelligence technology, isn’t new. The term was brought to prominence in the 1980s by computer scientists. And by 1986 a team of researchers including Geoffrey Hinton managed to come up with a back propagation-based training method that tickled at the beginnings of an unsupervised artificial neural network. Scant a few years later a young Yann LeCun would train an AI to recognize handwritten letters using similar techniques.\\nCredit: Harvard Magazine\\nBut, as those of us over 30 can attest, Siri and Alexa weren’t around in the late 1980s and we didn’t have Google Photos there to touch up our 35mm Kodak prints. Deep learning, in the useful sense we know it now, was still a long ways off. Eventually though, the next generation of AI superstars came along and put their mark on the field.\\nIn 2009, the beginning of the modern deep learning era, Stanford’s Fei-Fei Li created ImageNet. This massive training dataset made it easier than ever for researchers to develop computer vision algorithms and directly lead to similar paradigms for natural language processing and other bedrock AI technologies that we take for granted now. This lead to an age of friendly competition that saw teams around the globe competing to see which could train the most accurate AI.\\nThe fire was lit. By 2010 there were thousands of AI startups focused on deep learning and every big tech company from Amazon to Intel was completely dug in on the future. AI had finally arrived. Young academics with notable ideas were propelled from campus libraries to seven and eight figure jobs at Google and Apple. Deep learning was well on its way to becoming a backbone technology for all sorts of big data problems.\\nAnd then 2014 came and Apple’s Ian Goodfellow (then at Google) invented the generative adverserial network (GAN). This is a type of deep learning artificial neural network that plays cat-and-mouse with itself in order create an output that appears to be a continuation of its input.\\nCredit: Obvious\\nWhen you hear about an AI painting a picture, the machine in question is probably running a GAN that takes thousands or millions of images of real paintings and then tries to imitate them all at once. A developer tunes the GAN to be more like one style or another – so that it doesn’t spit out blurry gibberish – and then the AI tries to fool itself. It’ll make a painting and then compare the painting to all the “real” paintings in its dataset, if it can’t tell the difference then the painting passes. But if the AI “discriminator” can tell its own fake, it scraps that one and starts over. It’s a bit more complex than that, but the technology is useful in myriad circumstances.\\nRather than just spitting out paintings, Goodfellow’s GANs are also directly behind DeepFakes and just about any other AI tech that seeks to blur the line between human-generated and AI-made.\\nIn the five years since the GAN was invented, we’ve seen the field of AI rise from parlor tricks to producing machines capable of full-fledged superhuman feats. Thanks to deep learning, Boston Dynamics has developed robots capable of traversing rugged terrain autonomously, to include an impressive amount of gymnastics. And Skydio developed the world’s first consumer drone capable of truly autonomous navigation. We’re in the “safety testing” phase of truly useful robots, and driverless cars feel like they’re just around the corner.\\nFurthermore, deep learning is at the heart of current efforts to produce general artificial intelligence (GAI) – otherwise known as human-level AI. As most of us dream of living in a world where robot butlers, maids, and chefs attend to our every need, AI researchers and developers across the globe are adapting deep learning techniques to develop machines that can think. While it’s clear we’ll need more than just deep learning to achieve GAI, we wouldn’t be on the cusp of the golden age of AI if it weren’t for deep learning and the dedicated superheroes of machine learning responsible for its explosion over the past decade.\\nAI defined the 2010s and deep learning was at the core of its influence. Sure, big data companies have used algorithms and AI for decades to rule the world, but the hearts and minds of the consumer class – the rest of us – was captivated more by the disembodied voices that are our Google Assistant, Siri, and Alexa virtual assistants than any other AI technology. Deep learning may be a bit of a dinosaur, on its own, at this point. But we’d be lost without it.\\nThe next ten years will likely see the rise of a new class of algorithm, one that’s better suited for use at the edge and, perhaps, one that harnesses the power of quantum computing. But you can be sure we’ll still be using deep learning in 2029 and for the foreseeable future.\\nRead next: Dell's XPS 13 gets even better with thinners bezels and a bigger keyboard\\nTECHMICROSOFTGOOGLEDEEP LEARNINGMACHINE LEARNINGDATALEARNINGAMAZON ALEXADATA SET\\nSHARE ON FACEBOOK (2)\\nSHARE ON TWITTER (57\",\n",
              "  array([-4.2176695 , -4.072255  , -0.03228749], dtype=float32),\n",
              "  2),\n",
              " ('Wall Street’s focus on Silicon Valley has helped lift shares of Citigroup Inc., Bank of America Corp. and JPMorgan Chase & Co. to new highs and is likely to keep fueling gains for the biggest bank stocks, according to analyst Mike Mayo.\\n\\n\\n“I’m living in my hoodie” as long as tech keeps boosting banks’ revenue and trimming their expenses, said Mayo, who covers the sector at Wells Fargo, in a phone interview. In October, Mayo shed his suit jacket on Bloomberg TV to show his commitment to transforming himself into a “techie” from a bank analyst.\\n\\n\\n\\nMayo reaffirmed Citigroup, BofA and JPMorgan as his top picks as the sector continues to focus on fintech opportunities such as digital banking, electronic payments and the use of artificial intelligence. The three were the best performers among the top 40 banks, by assets, in 2019, with Citigroup soaring about 53% while BofA and JPMorgan both gained 43%. Those gains outpaced the KBW Bank Index, which rallied 32% last year, and the S&P 500, which rose 29%.\\n\\n\\n\\nThe three banks extended gains in Thursday trading, with Citigroup touching the highest since January 2018 and JPMorgan at one point rallying to a record.\\n\\n\\nIn September, Mayo said the 2020s would be the “decade of technology for banks,” citing a shift to digital delivery, via computers and software, from physical delivery, via branches and people.\\nMayo\\xa0noted\\xa0that bank stocks had a great year in 2019 even though “traditional” metrics, like net interest margins and lending growth, were less than stellar, and even as capital markets were “sluggish.”\\nWhile some of the bank stocks’ gains were “catch-up” after 2018’s underperformance, he said the three largest banks “trouncing” the rest of the sector reflected the “revamping of banking with technology.”\\n\\n\\nThe third quarter was a “turning point,” he added, as revenue grew faster than expenses. He expects revenue growth will keep outpacing costs in 2020, “enabled by the tech transformation.” JPMorgan and Citigroup are due to report fourth-quarter earnings on Jan. 14.',\n",
              "  array([-3.5389745 , -2.81922   , -0.09287819], dtype=float32),\n",
              "  2),\n",
              " ('A former high-level Google executive says he was pushed out after promoting human rights at the company, as leadership pushed plans for a censored Chinese search engine.\\nRoss LaJeunesse says company executives were dead set on entering China with some kind of censored search engine, despite comments from Google saying the project was always an experiment. “They are determined to do this,” he says he realized as executives made decisions about the project without him and other colleagues. “They don’t want to hear what I had to say.”\\nLaJeunesse spent 11 years at Google and served as its head of international relations, making him likely the highest-ranking former executive to dissent from the company. In his role, he oversaw human rights issues during a period of massive growth and controversy, much of it centering on Google’s work on “Project Dragonfly,” a censored Chinese search engine.\\nGOOGLE “REALLY SIDELINED ME”\\nWhile Google stopped offering its search product in China in 2010 amid censorship concerns, LaJeunesse says other executives at the company continued to press to enter the market. In 2017, LaJeunesse writes in a blog post released today, he learned about Project Dragonfly. He says he was immediately alarmed, both by the product and by the company’s failure to keep him involved.\\nAs Google pushed for deals in authoritarian Saudi Arabia and launched the Google Center for Artificial Intelligence in Beijing, LaJeunesse says, he pushed for a company-wide human rights program that would bring new oversight to product launches. But Google rebuffed the idea, and eventually brought in a colleague to oversee policy issues related to Dragonfly.\\n“Just when Google needed to double down on a commitment to human rights,” LaJeunesse writes in the blog post, “it decided to instead chase bigger profits and an even higher stock price.”\\nThe issues extended to the broader culture within the company as well, according to LaJeunesse. He says, at one point, during an all-hands meeting, his boss at the company suggested Asian employees “don’t like to ask questions.”\\nDuring a company “diversity exercise,” he says employees were asked to sort themselves by identity and were asked to write down stereotypes. He says he kept an “internal narrative” about Google’s commitment to diversity and thought the idea was “a little edgy” but that the company would act responsibly.\\nInstead, he joined a group of “homos” and had stereotypes shouted at him. He stopped the exercise in his room and spoke to other employees who were upset. After he complained to HR, he says he was accidentally copied on a human resources email that chided him for bringing up the issues and suggested HR “do some digging” on him.\\n“Their response to that was I’m the problem,” he says. “I still kind of can’t believe it myself.”\\nEventually, LaJeunesse says he was reassigned and was offered what he saw as a demotion. He says he was “re-orged out of a job” and left the company in April, and he has since launched a campaign for the US Senate representing Maine. “I came back and realized that things have not gotten better since I was a kid,” he says. “It’s actually worse for working families.”\\n“RE-ORGED OUT OF A JOB”\\nIn a statement, a Google spokesperson said the company has an “unwavering commitment” to human rights and that LaJeunesse was reassigned as part of a reorganization that affected many others. “As part of this reorganization, Ross was offered a new position at the exact same level and compensation, which he declined to accept,” the spokesperson said. “We wish Ross all the best with his political ambitions.”\\nLaJeunesse’s statement follows another year of tumult at Google, where several employees involved in internal advocacy at the company have alleged that they were terminated for their activism. Last month, a total of five Google employees said they were fired for rallying colleagues around issues like protesting Project Dragonfly, and they have filed formal labor charges against the company.\\nApart from the firings, Google leadership has shown other signs of weariness over the internal battles. In one notable example, the company said it would scale back its all-hands TGIF meetings to focus directly on business issues. But LaJeunesse’s stance against the company’s policies may raise the stakes even higher.\\n“I was one of the few old-timers around who remembered what Google used to be,” he says. “The company changed around me.”',\n",
              "  array([-0.10333376, -2.5230627 , -4.019579  ], dtype=float32),\n",
              "  0),\n",
              " ('IDEAS\\nAndrew Burt is chief legal officer at Immuta and a former official in the FBI Cyber Division.\\nJames C. Trainor is senior vice president at Aon and a former Assistant Director of the FBI Cyber Division.\\nWho’s responsible for protecting the 2020 presidential elections against cyber attacks?\\nNobody really knows, either inside or outside the U.S. government. To be sure, many agencies are hard at work combating cyber threats, but when it comes to fighting increasingly urgent threats in cyberspace – from attacks on our elections to hacks into the data stores of our largest companies – there is simply no one steering the ship. Instead, our government is confronting cyber threats through a largely incidental blend of overlapping agencies and authorities.\\nCongress and the Administration can fix this by creating a standalone agency, with the requisite mix of law-enforcement and intelligence authorities, to serve as the single source of threat information and investigations into network intrusions directed against the U.S. This would inject structure, coherence and accountability into our government’s approach to the cyber domain.\\nJust how disorganized is the current approach? Take as an example a partial list of who’s responsible for what in cyberspace: The FBI, whose Cyber Division one of us directed in the lead-up to the 2016 presidential elections, and the Secret Service investigate malicious cyber activities taking place in the U.S., while the National Security Agency, along with other elements of the intelligence community, collect intelligence on cyber activities overseas. The Defense Department, meanwhile, disrupts malicious activities when a military response is required. And the Department of Homeland Security, acting through the newly renamed Cybersecurity and Infrastructure Security Agency, serves as the nation’s “risk advisor.” That’s not even taking into account the roughly half-dozen standalone “intelligence centers” – like the CTIIC or the DC3, to name just two – tasked with tracking all these different activities.\\nFormer GOP Congressman Handed 26 Month Prison Sentence Over Insider Trading\\nThis approach is costly, not just in diminished effectiveness but also in real terms. Taken together, for example, the federal government requested a total of $11 billion in 2020 for all these cyber activities. That sum is about $2 billion more than the entire annual budget of the FBI, which, as the nation’s largest law enforcement agency, is tasked with investigating all federal crimes.\\nSo what would a single cyber agency look like in practice?\\nTo start, both cyber investigations and intelligence operations would be the sole domain of this new agency. The 2014 attack on Sony, for example, would have been handled entirely by the new agency, as would the 2016 efforts to interfere with our elections and the other hacks that have dominated the news cycle before fading into our collective memory. The agency would also be responsible for cyber operations designed to collect intelligence on or influence the behavior of our adversaries overseas. The agency would be relatively slim and mission-focused, bearing more similarities to the FBI or CIA than bureaucratic behemoths like DHS.\\nWhere cyber capabilities are not central to the crime or the operation, the new agency would serve as supporting experts to other agencies – outside of activities like network intrusions, the creation of malware or data theft, for example, computers still do play an enabling role in numerous other crimes. And because network intrusions are all, at root, crimes, the new agency would report directly into the Department of Justice, helping to ease the transition when criminal cases are handed off for prosecution – similar to the Drug Enforcement Agency, the Bureau of Alcohol, Tobacco, Firearms and Explosives and other agencies focused on domain-specific activities.\\nBy creating a new cyber agency, Congress would also be admitting another critical truth: The James Bonds and Jack Bauers of the information age don’t need guns and bullets. What they require instead is deep technical expertise – they must understand the intricacies of software, the insights that can be culled from data and the adversaries who use these powers to cause us harm. It is for many of these reasons that Israel, a leader in the field of cybersecurity, unified all of its cyber capabilities into one single government agency last summer.',\n",
              "  array([-0.50685745, -0.9909408 , -3.6348815 ], dtype=float32),\n",
              "  0),\n",
              " ('A Google artificial intelligence system proved as good as expert radiologists at predicting which women had breast cancer based on screening mammograms and showed promise at reducing errors, researchers in the United States and Britain reported. Gavino Garay reports.',\n",
              "  array([-3.4903193 , -4.075149  , -0.04864501], dtype=float32),\n",
              "  2),\n",
              " ('A former Google executive has raised concerns about the tech giant\\'s human rights policies as it eyes expansion in China and elsewhere.\\nRoss LaJeunesse, the firm\\'s former head of global international relations, said he was \"sidelined\" after he pushed the company to take a stronger stance.\\nGoogle defended its record in a statement, saying it has an \"unwavering commitment\" to human rights.\\nMr LaJeunesse is now campaigning for a seat in the US senate.\\nHe said his experience at Google convinced him of the need for tougher tech regulations.\\n\"No longer can massive tech companies like Google be permitted to operate relatively free from government oversight,\" he wrote in a post on Medium.\\nTech entrepreneurs call for more government regulation\\nGoogle\\'s Project Dragonfly \\'terminated\\' in China\\nGoogle\\'s main search business quit China in 2010 in protest of the country\\'s censorship laws and alleged government hacks.\\nBut it has since explored ways to return to the country, a major market, stirring controversy.\\nIn July, the company said it had cancelled its efforts to develop a censored search engine in China. The \"Dragonfly\" programme had generated concerns about enabling state control among US politicians and some employees, including Mr LaJeunesse.\\n\\'Executives came up with an excuse to say no\\'\\nMr LaJeunesse said Google rebuffed his efforts to formalise a company-wide programme for human rights review, even as it worked to expand in countries such as China and Saudi Arabia.\\n\"Each time I recommended a Human Rights Program, senior executives came up with an excuse to say no,\" he wrote.\\n\"I then realized that the company had never intended to incorporate human rights principles into its business and product decisions. Just when Google needed to double down on a commitment to human rights, it decided to instead chase bigger profits and an even higher stock price.\"\\n\\nGETTY IMAGES\\n\\nGoogle boss Sundar Pichai cancelled the controversial \\'Dragonfly\\' programme\\nMr LaJeunesse started at Google in 2008 and worked there until last May. He said he also raised concerns about treatment of women and minorities - only to find himself flagged as a troublesome employee.\\nFor example, he said his boss said at a staff meeting: \"Now you Asians come to the microphone too. I know you don\\'t like to ask questions.\"\\nGoogle said it \"rigorously\" investigates claims of inappropriate conduct and has worked to improve the reporting process.\\nIt said it conducts human rights assessments for its services and does not believe the more centralised approach recommended by Mr LeJeunesse was best, given its different products.\\n\"We have an unwavering commitment to supporting human rights organizations and efforts,\" a spokeswoman said in a statement.\\n\"We wish Ross all the best with his political ambitions,\" she added.\\nMr LaJeunesse is now running for the Senate as a Democrat in Maine, for a seat currently held by Republican Susan Collins.\\nHis attack comes as concerns about the practices of Silicon Valley giants grow in the US, with politicians as different as Democrat Elizabeth Warren and President Donald Trump criticising tech giants.\\nMr LaJeunesse said Google has changed as its business expanded into new areas, such as cloud computing, and its original leaders became \"disengaged\".\\nLarry Page and Sergey Brin, who co-founded the business while in graduate school at Stanford University, officially stepped down from top roles at the company last month.',\n",
              "  array([-0.20799112, -1.7580703 , -4.1728296 ], dtype=float32),\n",
              "  0),\n",
              " ('Get ready for a smart fridge showdown at CES 2020, because Samsung and LG will both be unveiling fridges with added artificial intelligence capabilities this year. Samsung’s latest edition of its Family Hub refrigerator and LG’s second-generation InstaView ThinQ fridge both tout AI-equipped cameras that can identify food. The idea is that the cameras can scan what’s inside and let users know what items they’re short on, even making meal suggestions based on the ingredients they still have.\\nSamsung’s Family Hub smart fridge was first unveiled at CES 2016, and since then, the company has been rolling out updated iterations with Bixby support, SmartThings integration, and AKG speakers. The latest edition adds software upgrades to enable AI image recognition in its View Inside cameras.\\nWHICH WILL BE THE LAST FRIDGE STANDING?\\nBefore, the cameras let users see what’s in their fridges from their smartphones, a useful feature if you happen to be out grocery shopping and can’t remember what you need to stock up on. With the AI-enabled updates, Family Hub will supposedly make these recommendations for you on its own, identifying which ingredients you’re low on. Though it’s to be determined how well the image recognition will work — for example, how will it deal with ingredients stored in tubs of Tupperware?\\nThe software upgrades also include improved meal planning with the help of Whisk, a food tech startup Samsung acquired last year. Whisk lets users plan meals for up to a week and then creates smart shopping lists using ingredients that apply to multiple recipes.\\nFinally, the huge built-in touchscreen that can be used as a virtual bulletin board can now support video clips, as well as mirror content from Samsung TVs and phones. That means you can watch vertical videos like IGTV on your Samsung fridge, as God intended.\\nSamsung’s Family Hub fridge comes in silver (pictured here) and black. Image: Samsung\\nLG is showing off two models of its InstaView fridges, both of which feature a 22-inch display that can turn transparent to let users see what’s inside without opening the door and letting the cold air out. There’s the AI-equipped InstaView ThinQ and the InstaView with Craft Ice, which makes fancy, two-inch spherical ice balls. Those are supposed to melt slower than regular ice, if that’s a problem that you have. The InstaView with Craft Ice was released in the US last year, but will now be available in more markets.\\nThere’s no pricing information yet, but based on the prices for LG and Samsung’s previous fridge models, customers can expect prices to range from $4,500 to $6,000. Samsung says its Family Hub updates will be available in the spring.\\nI’m not opposed to the idea of a huge Wi-Fi-connected touchscreen on a fridge — in fact, it seems like a genuinely useful way to look up recipes or display cute photos and videos. I’m skeptical how well the AI will identify different ingredients, and whether using a computer to see what items you’re low on is really better than just taking a look for yourself.',\n",
              "  array([-4.672248  , -4.1150174 , -0.02601232], dtype=float32),\n",
              "  2),\n",
              " ('How Silicon Valley wants to fuck with our brains\\nby FIONA J MCEVOY — in SYNDICATION\\n78\\nSHARES\\nIntroducing his students to the study of the human brain Jeff Lichtman, a Harvard Professor of Molecular and Cellular Biology, once asked: “If understanding everything you need to know about the brain was a mile, how far have we walked?” He received answers like ‘three-quarters of a mile’, ‘half a mile’, and ‘a quarter of a mile’.\\nThe professor’s response? “I think about three inches.”\\nEarly birds get all the good stuff\\nLike a great price on TNW2020 tickets\\nGIMME\\nLast month, Lichtman’s quip made it into the pages of a new report by the Royal Society which examines the prospects for neural (or “brain-computer”) interfaces, a hot research area that has seen billions of dollars of funding plunged into it over the last few years, and not without cause. It’s projected that the worldwide market for neurotech products — defined as “the application of electronics and engineering to the human nervous system” — will reach as much as $13.3 billion by 2022.\\nSo, despite our admitted lack of understanding, it seems the brain is a new and significant frontier for tech-pioneers looking to reinvent — and perhaps irreversibly influence — the way we interact with the world.\\nThe Royal Society report speculates:\\nMental health conditions could be treated by using interfaces to target relevant parts of the brain, bringing relief to the hundreds of millions worldwide who have depression. Even Alzheimer’s disease, which has proved resistant to conventional therapies, might be halted or reversed.\\nOutside of medical use:\\nPeople could undergo ‘whole brain diagnosis’ to identify their unique talents and challenges. Today’s ‘brain training’ computer games, whose impact is debated, might give way to demonstrably effective ‘brain cleaning’ or ‘mind gym’ sessions to keep minds sharp and creative.\\nNeural interfaces offer myriad possibilities to enhance everyday life. We could use our minds to open doors, turn on lights, play games, operate equipment or type on computers.\\nThen there are opportunities to enhance or supercharge the brain itself. Implants, helmets, headbands or other devices could help us remember more, learn faster, make better decisions more quickly and solve problems, free from biases…\\nMood, knowledge and memory could be securely and confidentially backed up or uploaded to a digital cloud.\\nI know, it’s a lot. And I’ve omitted the references to telepathy, the potential merging of humans with artificial intelligence, and the option to hook your neural interface up to that of another animal, like a bird.\\nTo a sci-fi nut, this must all sound like manna from heaven. To the rest of us, it’s likely to be a little bewildering (to say the least). So, is this a real proposition? Or just the (fairly creepy) wishlist of some over-ambitious Silicon Valley nerds?\\nThe truth is that it’s difficult to tell what the long-term trajectory for brain-computer interfaces will be but, to a degree, they are already here. Though still fairly elementary, we currently have drones and artificial limbs that can be controlled using the brain alone, as well as headsets that boost concentration and memory. Some of these technologies are invasive, but many are not. Some record and react to brain activity, some stimulate it, and some do both.\\nReassuringly, it’s non-invasive technologies that look to be headed for commercial distribution. Most of these are re-imaginings of the electroencephalogram (EEG), a system that monitors and records electrical impulses in the brain. One of the leaders in the commercial space, CTRL-Labs, specifically focuses on what it calls ‘intention capture’. Their product is a electromyogram (EMG)-based wristband, which can respond to electrical signals as they activate in a user’s arm muscle. At the moment, the company’s demo has a player controlling a simple game using only this impulse detection and no physical movement (take a look).\\nIf you’re cynical about how far this could go, you should know that Facebook acquired CTRL-Labs last month, and just a couple of weeks ago leaked transcripts from Mark Zuckerberg’s internal meetings reinforced the firm’s keen interest in brain-computer interfaces.\\nGiving his thoughts on Elon Musk’s Neuralink project, Zuck says:\\nI am very excited about the brain-computer interfaces for non-invasive. What we hope to be able to do is just be able to pick up even a couple of bits. So you could do something like, you’re looking at something in AR, and you can click with your brain. That’s exciting… Or a dialogue comes up, and you don’t have to use your hands, you can just say yes or no. That’s a bit of input. If you get to two bits, you can start controlling a menu, right, where basically you can scroll through a menu and tap. You get to a bunch more bits, you can start typing with your brain without having to use your hands or eyes or anything like that. And I think that’s pretty exciting. So I think as part of AR and VR, we’ll end up having hand interfaces, we’ll end up having voice, and I think we’ll have a little bit of just direct brain.\\nIf a little bit of “direct brain” doesn’t bother you, it’s worth looking ahead to the possibilities that extend beyond basic control of an elementary system.\\nFor example, we already have neural systems that can read moods and emotions. Last year, The South China Morning Post reported that this kind of technology had been deployed by Chinese firms looking to monitor employees for signs of anger, anxiety or depression using devices built into headwear and hats. And perhaps even more impressively (or disturbingly), researchers at Kyoto University in Japan have been able to use a deep neural network to convert brain signals from an fMRI scan (used to map neural activity) into an image that contains many of the shape and color characteristics as one viewed by the subject of the scan.\\nThis is all just to say that these types of systems are unlikely to cease development once they provide the capabilities to click or scroll in Mark Zuckerberg’s AR hellscape.\\nThe Royal Society report makes sure to flag some early concerns. Most rational-thinking people won’t be too far behind them: What would it mean if an external company or government could gain access to our moods, or even our thoughts? How might human privacy — and indeed autonomy — be protected in if these technologies became ubiquitous? How can we ensure that they wouldn’t be weaponized by bad actors or governments to influence and control entire populations? (And is it okay if they only want to subliminally coax us to eat more healthily or respect the rules…?)\\nIt’s not hard to think of governments that will be watching the progression of this technology very keenly.\\nThough it’s only fair to weigh risks against benefits before eagerly ringing the alarm bell, even here there is ambiguity. The benefits of commercializing this technology seem extremely limited, at least on the face of it. Gameplay? Fitness? Hands-free navigation of augmented or virtual reality environment? None of these feel like strong arguments for selling access to our brains.\\nBut what about neural interfaces that could improve memory or concentration, making us super productive in life and work? Presumably, one could make the case that this is a worthwhile trade? Well, incidentally, completely separate research released just after the Royal Society report should urge caution around attempts to enhance such functions.\\nA new journal in Science published findings that appear to affirm the long-held theory that there is an active “forgetting mechanism” which kicks in while we sleep. The study found that when researchers suppressed neurons that produce the naturally occurring hypothalamic melanin-concentrating hormone (MCH) in mice, their memory performance actually increased. In other words, without this unnatural suppression these hormones act very deliberately to impair — or “modulate” — our memories.\\nThis is a biological addition, not some kind of “lack” that we must compensate for with technology. We might safely assume that it serves some worthwhile evolutionary purpose.\\nIndeed, there is good reason to believe that if we didn’t forget we would live in a perpetual state of confusion, our brains awash with confusing superfluous information. One curious story that speaks to the chaos of the ever-remembering mind is that of the man who became known as subject S; a young Moscow-based journalist (later identified as Solomon Shereshevsky) who approached neuropsychologist Dr. Alexander Luria in 1929 with a very peculiar problem: he could not forget.\\nAccording to Luria’s reports, subject S. was able to remember foreign poems, scientific formulas, and enormously long strings of words and numbers decades after he had been told them. He recited them to perfection every time Luria tested him.\\nGreat asset, eh? To never forget a name at a cocktail party, miss a birthday, fail a test on a fact or formula you already learned? To remember your own human life with crystal clarity rather than with the foggy haze that tends to wash over even our dearest memories?\\nNot so. According to the New York Times:\\nS.’s ability to remember was also a hindrance in everyday life. He had a hard time understanding abstract concepts or figurative language, and he was terrible at recognizing faces because he had memorized them at an exact point in time, with specific facial expressions and features. The ability to forget, scientists eventually came to realize, was just as vital as the ability to remember.\\nWho knows what psychological or neural confusion could eventually be brought on by using brain-computer interfaces to optimize evolutionary facets…\\nBut we probably shouldn’t run screaming for the hills just yet. These systems are in their infancy, and there have been incredible breakthroughs in the research that should yield great benefits for people with mental and physical impairments. Nevertheless, The Royal Society are right to get ahead of the ethical and moral dilemmas that will accompany the commercialization of this type of technology. It is unfamiliar terrain, and allowing a system to intervene on our physical and mental capacities is an unprecedented encroachment that could easily turn sour. Certainly if we are to judge by the ways technological intelligence and surveillance have been wielded so far.\\nFor now we should keep a close watching brief on how this technology develops, as well as any-and-all proposals for its use. One thing seems to be true, if we thought society had already reached its technological saturation point, we “ain’t seen nothin’ yet.”',\n",
              "  array([-2.4073775 , -2.9810755 , -0.15174122], dtype=float32),\n",
              "  2),\n",
              " ('What to expect at CES 2020: Ivanka Trump, flying cars, sex toys and 8K TVs – oh my!\\nEdward C. Baig USA TODAY\\nPublished 4:25 PM EST Jan 4, 2020\\nFlying cars, sex toys, 8K TV, even Ivanka Trump. That\\'s some of what to expect from the tech industry\\'s annual pilgrimage to the desert. \\nCES, the mammoth tech trade show organized by the Consumer Technology Association (CTA), will draw some 170,000 people from around the world to Las Vegas to launch products and services – but also to make deals and schmooze with one another.\\nFor the consumer watching from afar (since the show isn\\'t open to the public), the best part of CES often can be the range of what\\'s there, from the weird to the wonderous to the stuff that makes us all ask just \"why.\" \\nOver the years, it has transitioned from just the gadgets and gizmos consumers can\\'t wait to get their hands on, featuring many companies the average consumer might do a double take over. \\n“Every company is becoming a tech company,” says Gary Shapiro, CEO of the CTA.\\nWho\\'s going to CES\\nAmong the who\\'s who on the speaker lineup this year are Salesforce chairman and co-CEO Marc Benioff, Delta CEO Ed Bastian, Samsung’s consumer electronics CEO Hyun-Suk Kim, and Daimler AG chairman Ola Källenius.\\nIvanka Trump, the daughter and adviser of President Donald Trump, listens as Trump talks to the media on his way to the Marine One helicopter, Nov. 20, 2019, as they leave the White House in Washington, en route to Texas.\\nPatrick Semansky, AP\\nIn a controversial turn for some, Ivanka Trump will also take the stage, to advocate on administration plans for employer-led strategies that invest in reskilling workers, creating apprenticeships and developing K-12 STEM education programs.\\nAs Rachel Sklar tweeted: \"This is a terrible choice on so many levels but also – what an insult to the YEARS AND YEARS  of protesting how few women were invited to keynote & being told it was a pipeline problem while similarly-situated men were elevated. There are so many great, qualified women. Shame.\"\\nBut what\\'s really come out of CES?\\nTo be honest, it\\'s been a while since any single blockbuster product or service was introduced at CES. One main reason: The big tech companies – Amazon, Apple, Facebook, Google, Microsoft and Samsung – reserve the big launches for their own events. \\nSpying TVs: Your smart TV is spying on you. Here are step-by-step instructions to stop it\\nShapiro concedes that no single trade show producer can possibly meet all of a company’s needs. But “the relevancy is the fact that we’re sold out.”\\nStill, those big tech companies will be attending CES in one capacity or another, with Amazon and Google, in particular, trying again to convince the public that products that embrace their respective Alexa and Google Assistant virtual assistants and smart home platforms are superior to their rivals.\\nApple\\'s back at CES – sort of\\nApple’s rather public appearance at this year’s CES is rare. It won’t be exhibiting in a booth – that hasn’t happened forever. But Apple’s senior director for global privacy, Jane Horvath, will be on a panel with counterparts from Facebook, Procter & Gamble, and Wing Venture Capital, as well as Federal Trade Commission Commissioner Rebecca Slaughter.\\nWhat to expect from CES 2020\\nWhat you’ll also hear plenty about are 5G wireless networks, artificial intelligence and tech that will enable entire smart cities. \\nSure, there’ll be the usual slew of strange, cool and whacky new products as well, mostly from companies on the show floor you never have heard of and, sadly for them, likely never will. \\nYou can expect a wide range of product categories to be represented: robots, headphones, cameras, computers, and though not an over-the-top wireless show, smartphones, too, maybe even more of the emerging foldable-type handsets. \\nHere’s some of what else you\\'ll hear about:\\nStreaming stampede continues\\nDespite all the recent launches, with Apple and Disney joining the congested streaming landscape late last year, the stampede for new entertainment services will continue well into 2020. \\nAt CES, Quibi (pronounced “kwibee”) CEO Meg Whitman and founder Jeffrey Katzenberg will showcase the new streaming service. Slated to launch on April 6, Quibi is built around bite-size content, essentially five- to 10-minute episodes from A-listers like Lorne Michaels, Jennifer Lopez, Sophie Turner, Zac Efron, Dwayne \"The Rock\" Johnson, and Steven Spielberg.  \\nSeparately, execs from Comcast’s NBCUniversal are expected to spill further details on the company’s own new streaming service Peacock, also set for an April debut.\\nDigital health – and sex\\nFrom hearing aids that leverage AI and machine learning to, yes, sex tech, CES is embracing all aspects of health and wellness. \\nConsider it an about-face from last year. That’s when the CTA stepped into controversy after giving out an innovation award to a sex tech startup Lora DiCarlo for an “adult toy” called Osé, only to subsequently rescind the award. Following outrage over what was viewed as a sexist stance toward a woman-focused product, the CTA apologized and reversed its reversal. Looking to avoid a similar brouhaha this time around, CTA is including pleasure tech in the wellness category. \\nCES will also be hosting a digital health summit for the 11th time. And Shapiro says doctors can even earn continuing education credits by attending health care-related sessions. “We’ve seen a huge interest from insurance companies and others basically saying, \\'how can technology solve consumer health problems?\\'” he says. “What is it we can do using technology that deals with the fact we have an aging population?”\\nFlyings cars and transporters \\nSegway will be showcasing the new S-Pod \"personal transporter\"  that aims to enhance the way people get around at malls, theme parks and airports, especially for those folks with mobility challenges. The company says it is part of a bigger smart city mission to bring new transportation options to metro areas.\\nSegway S-Pod personal transporter.\\nSegway\\nMeanwhile, though CES is not considered a dedicated car show, electric vehicles and self-driving cars are all part of the mix, as are entertainment systems within vehicles. \\nAnd Hyundai Motor plans to unveil its concept “Personal Air Vehicle.” Translation: flying car. Maybe the future isn’t as far off as you might think.\\nGinormous TVs\\nCES wouldn’t be CES without the latest in TV tech.\\nNow that you’ve purchased a sweet, large-screen 4K television to anchor your home theater – and got it relatively cheaply – the challenge for TV makers, led by usual suspects LG, Samsung, and Sony, will be trying to convince you that what you have now isn’t quite good enough. So, CES this time around will be a showcase for 8K sets – never mind that there won’t be much to watch in the format just yet, and many average viewers can’t even tell the difference between an HD and 4K broadcast. The push for 8K isn’t surprising, given how low 4K TV prices have gotten, putting a strain on retailer margins. \\nThose who take the plunge, will “buy 8K for the future…so they know in three or four years they’re not obsolete,” says Tom Campbell, chief technologist at the Southern California based Video & Audio Center chain. \\nIf you want to look even further CES attendees will also get to peer at 16K. Even if it’ll be years before you’d even think about buying one.\\nEmail: ebaig@usatoday.com; Follow @edbaig on Twitter\\nPublished 4:25 PM EST Jan 4, 2020',\n",
              "  array([-4.8458824 , -3.4695902 , -0.03977095], dtype=float32),\n",
              "  2),\n",
              " ('Google AI system can surpass human experts in spotting breast cancer, study finds\\nJazmin Goodwin USA TODAY\\nPublished 11:31 AM EST Jan 3, 2020\\nGoogle’s artificial intelligence system can identify breast cancer more accurately than radiologists, according to a study published in Nature on Wednesday.\\nThe model, created in collaboration with cancer researchers and Google Health, was trained on X-ray images, known as digital mammography, from women in the U.K. and U.S. to spot signs of breast cancer in the scans. Researchers used mammograms from 76,000 women in the U.K. and more than 15,000 women in the U.S., according to Google Health.  \\nShravya Shetty, technical lead at Google Health who co-authored the study, said the results \"exceeded expectations\" and revealed possibilities that the AI could assist in workload reduction by being employed as a second reader with breast cancer screenings.\\nResearchers noted that the model received less information than human experts, only processing the most recent of an anonymized mammogram. With this, the model was able to surpass radiologists in spotting breast cancer and reducing false positives and negatives.\\nThe system produced a 1.2% reduction of false positives in the U.K. and a 5.7% reduction in the U.S. It was able to reduce false negatives by 2.7% in the U.K. and 9.4% in the U.S. \\nBreast cancer signified by an Xray with red cells emanating from one of the breasts\\nGetty Images\\nMammograms are the lead method for screening breast cancer but fail to be 100% accurate in showing if a woman has breast cancer. Screening mammograms do not find about 1 in 5 breast cancers, according to the American Cancer Society.\\nFalse-positive mammograms can cause anxiety for patients as results show abnormalities even though no cancer is present. While false-negative mammograms can cause a false sense of security with results looking normal even though breast cancer actually is present.\\n“There are some promising signs that the model could potentially increase the accuracy and efficiency of screening programs, as well as reduce wait times and stress for patients,” said Google Health in a blog post.\\nBreast cancer surgery: What women need to know\\nFDA proposal: FDA proposes mammogram changes for first time in 20 years to identify breast cancer early\\nThis is the latest in AI research from Google to assist with issues in healthcare. Last year, the tech giant released a study reporting its developments in using AI to predict lung cancer in patients. Another study revealed how AI can improve eye care and make ophthalmologists more effective. \\nThe tech giant made its first entry into the $3.5-trillion healthcare market, with its planned acquisition of Fitbit, the wearable exercise, heart rate, and sleep tracking device.\\nDaniel Tse, product manager at Google Health and research co-author, says next steps for the AI include working with even more groups around the world. \\n\"We\\'d love to continue to work with our existing research partners, find new ones, as well as engaging in conversations with patients, providers and regulatory groups,\" said Tse. \"We can begin to get a sense of telling them the story of how we got to this point, learn from their experience and build tools that will ultimately help each of these groups.\"\\nBreast cancer remains the second leading cause of cancer death in women in the U.S., with nearly 40,000 women dying each year, according to the CDC. About 1 in 8 women in the U.S. will develop breast cancer in their lifetime.\\nPublished 11:31 AM EST Jan 3, 2020',\n",
              "  array([-4.5339775 , -4.3546586 , -0.02386732], dtype=float32),\n",
              "  2),\n",
              " ('The biggest tech news yesterday is that the former Google human rights chief says he was “sidelined” over the proposed, censored Chinese search engine known as “Dragonfly.” Ross LaJeunesse, the executive, knew how to ensure his story would make an impact. He spoke with The Verge’s Colin Lecher and many other media outlets, published a Medium post with frankly shocking details, and dominated tech news all day. Good. His story deserves attention.\\nAn idea that I’ve been kicking around as we prepare for season two of the Land of the Giants podcast (about Google, naturally) is that until very recently, Google was a special kind of naive. It is a powerful, massive company that used to see itself as a utopian collective which just so happened to make oodles of cash.\\nIf you get annoyed that Google has pivoted its way through launching and killing a dozen messaging apps, that open, freewheeling culture is why.\\nYou are reading Processor, a newsletter about computers by Dieter Bohn. Dieter writes about consumer tech, software, and the most important news of the day from The Verge. This newsletter delivers about four times a week, at least a couple of which include longer essays. You can subscribe to Processor and learn more about it here. Processor is also a YouTube series with the same goal: providing smart and surprising analysis with a bit of humor. Subscribe to all of The Verge’s great videos here!\\n\\n\\nBy subscribing, you are agreeing to receive a daily newsletter from The Verge that highlights top stories of the day, as well as occasional messages from sponsors and / or partners of The Verge.\\nThat kind of naiveté would be endearing if it wasn’t also so dangerous. An organization as powerful as Google that isn’t willing to admit its size, influence, and power is bound to stumble into problems. I think Dragonfly was one result of that disconnect.\\nEven if you could believe that Google could have made a moral case for Dragonfly (and I’m leaving that judgment for another time), the telling thing is that Google didn’t try — it was not openly discussed with employees like so many other Google products.\\nHere’s an important paragraph from Colin Lecher’s story on LaJeunesse:\\nAs Google pushed for deals in authoritarian Saudi Arabia and launched the Google Center for Artificial Intelligence in Beijing, LaJeunesse says, he pushed for a company-wide human rights program that would bring new oversight to product launches. But Google rebuffed the idea, and eventually brought in a colleague to oversee policy issues related to Dragonfly.\\nAssuming LaJeunesse’s account is accurate, there are any number of motivations you could ascribe to these decisions. But I want to home in on just one: I think that dealing forthrightly with the Dragonfly decision in a more traditional, open, “Googley” way would have forced the entire company to contend with the uncomfortable truth that it is a massive, geopolitical entity. It would have popped the bubble of Google’s self-image.\\nWell, it popped anyway. Which means that Google is a company without a clear identity anymore. And the truth is that it was never as defined as it should have been in the first place. The old one — “Don’t be Evil” — didn’t scale, to borrow a classic Silicon Valley phrase.\\nThe operative verb in “Don’t be Evil” is “to be.” You can’t live up to “Don’t be Evil” if you don’t know what you are in the first place.\\nI don’t think that the massive size of Google fully accounts for the things that LaJeunesse experienced, but I do think it’s an important factor. Almost exactly a month ago I published an essay I titled “Google’s Third Era,” pegged to the news that Google CEO Sundar Pichai was also becoming Alphabet’s CEO as Larry Page and Sergey Brin stepped back. Here’s what I wrote then:\\nIf the first era of Google was developing the technology, and the second era was growing to a massive scale, the third era is contending with the effects of that scale. That reckoning isn’t happening because the founders formalized their already reduced roles by handing over the CEO title. It’s happening because both internally and externally, we don’t know how to deal with a company as big and powerful as Google.\\nIt’s troubling to think that as a society we don’t know how to deal with institutions as large and powerful as Google (or Apple, Amazon, Facebook, and Microsoft). It’s even more troubling to think that nobody inside Google knows how to contend with Google’s size, either.\\nGoogle’s old mantra was about defining itself by saying what it it wouldn’t be. Now, Google has to figure out what it will be.\\nCES IS COMING AND THAT RIGHT SOON\\n└ Here’s what’s next for gadgets in 2020\\nYesterday’s newsletter had the intro to this essay, but not all the category-specific predictions. Here they are: this is both a CES preview and a larger look at some of the trends coming to gadgets in 2020. CES begins this weekend and I’ll be there with many other Verge reporters and video directors.\\n└ Samsung and LG go head to head with AI-powered fridges that recognize food\\nI am so here for an arms race of wacky features on smart fridges. Using AI to determine what’s needed from the grocery store is so wildly unnecessary and so likely to work badly that I just want to sit back and watch. Samsung and LG are duking it out on the battlefield of features nobody asked for and it is so excessive you kind of just have to marvel at it. So yes, I’m here for it. Not here enough to actually buy one or ever recommend you do, but here to watch this low-stakes fight.\\n└ LG’s latest rollable TV descends from the ceiling like a projector screen\\nThese are fun too look at, but don’t wait to buy one, they’re not coming to a big box store near you anytime soon. Jon Porter has the details:\\nDespite promising that its last prototype from CES 2019 would be going on sale that year, LG subsequently failed to make that release date. LG Display’s press release doesn’t mention if or when it expects the new rollable TV to go on sale, which suggests that a release isn’t exactly imminent.\\n└ Dell’s latest XPS 13 has a new design with a bigger display and Ice Lake chips\\nDell arguably updates the XPS 13 too often. There’s this model and the 2-in-1 model, and both have changed often enough to be confusing. But every change has been for the better, and this one is no exception. I encourage you to click though and just LOOK at this thing: it’s beautiful and svelte. Hopefully build quality is pretty good -- Dells can feel a little plasticky sometimes.\\nBut what’s undeniable is that all that iteration has taken Dell to a place where it’s making laptops that make even the most recent MacBooks look dowdy and old.\\nTrue story: I was typing on my MacBook Pro over the holidays and there was somebody who works far way from the tech industry hunting around the rooms until he found me. He heard me typing and thought it was some sort of insect chattering in the house.\\n└ Samsung’s Galaxy Book Flex α aims to be a cheaper QLED 2-in-1 laptop\\nI don’t want to pre-judge this, but right now Samsung is definitely in a trust-but-verify place when it comes to experimental laptops. It has yet to ship the Galaxy Book S ARM-based laptop it announced in August, for one thing. For another, while I’m generally bullish on OLED becoming great on laptops, I’m not sure I’d take a chance on this one just yet. Wait not just for a release but also for reviews before you get too excited.\\n└ GE’s new smart switches and dimmers can be installed in almost any home\\nIf you looked at getting smart switches for your home and threw up your hands when it came time to figuring out if your wiring would work, these may be worth a look. Dan Seifert explains:\\nThe vast majority of smart lighting switches and dimmers on the market require an extra wire in the junction box called a neutral wire, which is lacking on many older homes. This wire is used to control and regulate voltage and is necessary for many dimmers to work properly. Prior to GE’s new products, the only other smart switch and dimmers that didn’t need a neutral wire were from Lutron’s Caséta line of products. Unlike the Caséta products, though, GE’s new switches and dimmers do not require a hub and can connect directly to a Wi-Fi network.\\nMORE FROM THE VERGE\\n└ Google disables Xiaomi integrations for all its devices after a Nest showed weird images\\nReddit user Dio-V, who said via email to The Verge that they’re based in The Netherlands, said they saw images of an enclosed porch, a sleeping man in a chair, and a sleeping baby in a crib. Dio-V told The Verge he has been contacted by Google about the post, but so far had not heard from Xiaomi.\\n└ A pop-up YouTube account might have locked down Rolling Stones rarities for decades\\nWhat a weird story! Copyright leads to so many strange outcomes. Who gets paid, how, why, and when is getting ever more complicated as antiquated laws buckle under the stress of the internet’s new distribution models. There’s nobody better to explain it than Dani Deahl:\\nAll the recordings turned 50 years old in 2019, meaning they were slated to become public domain in the European Union unless they were published in some form before the end of the year. But it’s unclear if fleeting YouTube uploads are enough to satisfy the EU’s publishing requirements, according to Zvi S. Rosen, lecturer at the George Washington University School of Law. “It’s really kind of pushing the edge of what’s possible under the law,” Rosen tells The Verge.\\n└ The IRS is done letting TurboTax easily steer you away from filing taxes for free\\nShame on Intuit for its lobbying behavior. Shame on congress and the IRS for not pushing to make free tax filing readily available. Thank Pro Publica for its dogged reporting on this, which will make a genuine difference in people’s lives this tax season.\\n└ Brydge’s iPad keyboard with trackpad is coming next month for $200\\nI know people want this to work and I know there are many people who actually have managed to get working mouse support integrated into their iPad workflows but I ...just don’t see it. Mouse support is built-in as an accessibility feature on the iPad, but it is foreign to the whole experience of iPadOS.\\nI’m not saying the iPad should never have full-fledged mouse support, but I am saying it’s not good enough yet to justify this keyboard. Then again, I am going to try it and find out if I’m wrong. (I don’t expect I will be.)\\nTrackpad support on the iPad is still very limited, and the experience isn’t as fluid as you might expect coming from a Mac. But interest still seems to be high: the Libra keyboard received more than $313,000 through crowdfunding and preorders.\\n└ Dell will soon let you interact with your iPhone apps from your PC\\nI am absolutely old enough to regale you with stories of PalmOS HotSync over Serial ports, but you don’t want that. I will say that it seems like we will be forever cursed to reinvent the ways mobile devices will talk to laptops for all eternity. Here’s Dell’s latest take on it, which is unique in that it works with the iPhone.\\n└ FDA announces new crackdown on flavored vaping products\\nVape tank e-cigs predate Juul and other cartridge-based e-cigs by a lot. They’re less convenient and harder to hide, but not exactly hard for teenagers to acquire. Assuming this not-quite-a-ban stays as it is, I would not be surprised to see some tank-based product rush in to fill the gap. Maybe? This does seem like a half measure, in any case. Nicole Wetsman has the story:\\nBy focusing on flavored cartridges (which are popular with teenagers), and not including other methods of vaping (like vape tanks), the FDA said it keeps flavors accessible to adult users who may be making a switch from traditional cigarettes. Juul announced in October that it would stop selling fruit-flavored pods.',\n",
              "  array([-0.6444804, -0.79346  , -3.7815397], dtype=float32),\n",
              "  0),\n",
              " ('TikTok parent company ByteDance has built technology to let you insert your face into videos starring someone else. TechCrunch has learned that ByteDance has developed an unreleased feature using life-like deepfakes technology that the app’s code refers to as Face Swap. Code in both TikTok and its Chinese sister app Douyin asks users to take a multi-angle biometric scan of their face, then choose from a selection of videos they want to add their face to and share.\\nWith ByteDance’s new Face Swap feature, users scan themselves, pick a video and have their face overlaid on the body of someone in the clip\\nThe deepfakes feature, if launched in Douyin and TikTok, could create a more controlled environment where face swapping technology plus a limited selection of source videos can be used for fun instead of spreading misinformation. It might also raise awareness of the technology so more people are aware that they shouldn’t believe everything they see online. But it’s also likely to heighten fears about what ByteDance could do with such sensitive biometric data — similar to what’s used to set up Face ID on iPhones.\\nSeveral other tech companies have recently tried to consumerize watered-down versions of deepfakes. The app Morphin lets you overlay a computerized rendering of your face on actors in GIFs. Snapchat offered a FaceSwap option for years that would switch the visages of two people in frame, or replace one on camera with one from your camera roll, and there are standalone apps that do that too, like Face Swap Live. Then last month, TechCrunch spotted Snapchat’s new Cameos for inserting a real selfie into video clips it provides, though the results aren’t meant to look confusingly realistic.\\nMost problematic has been Chinese deepfakes app Zao, which uses artificial intelligence to blend one person’s face into another’s body as they move and synchronize their expressions. Zao went viral in September despite privacy and security concerns about how users’ facial scans might be abused. Zao was previously blocked by China’s WeChat for presenting “security risks.” [Correction: While “Zao” is mentioned in the discovered code, it refers to the general concept rather than a partnership between ByteDance and Zao.]\\nBut ByteDance could bring convincingly life-like deepfakes to TikTok and Douyin, two of the world’s most popular apps with over 1.5 billion downloads.\\nZao in the Chinese iOS App Store\\nHidden inside TikTok and Douyin\\nTechCrunch received a tip about the news from Israeli in-app market research startup Watchful.ai. The company had discovered code for the deepfakes feature in the latest version of TikTok and Douyin’s Android apps. Watchful.ai was able to activate the code in Douyin to generate screenshots of the feature, though it’s not currently available to the public.\\nFirst, users scan their face into TikTok. This also serves as an identity check to make sure you’re only submitting your own face so you can’t make unconsented deepfakes of anyone else using an existing photo or a single shot of their face. By asking you to blink, nod and open and close your mouth while in focus and proper lighting, Douyin can ensure you’re a live human and create a manipulable scan of your face that it can stretch and move to express different emotions or fill different scenes.\\nYou’ll then be able to pick from videos ByteDance claims to have the rights to use, and it will replace with your own the face of whomever is in the clip. You can then share or download the deepfake video, though it will include an overlayed watermark the company claims will help distinguish the content as not being real. I received confidential access to videos made by Watchful using the feature, and the face swapping is quite seamless. The motion tracking, expressions and color blending all look very convincing.\\nWatchful also discovered unpublished updates to TikTok and Douyin’s terms of service that cover privacy and usage of the deepfakes feature. Inside the U.S. version of TikTok’s Android app, English text in the code explains the feature and some of its terms of use:\\nYour facial pattern will be used for this feature. Read the Drama Face Terms of Use and Privacy Policy for more details. Make sure you’ve read and agree to the Terms of Use and Privacy Policy before continuing. 1. To make this feature secure for everyone, real identity verification is required to make sure users themselves are using this feature with their own faces. For this reason, uploaded photos can’t be used; 2. Your facial pattern will only be used to generate face-change videos that are only visible to you before you post it. To better protect your personal information, identity verification is required if you use this feature later. 3. This feature complies with Internet Personal Information Protection Regulations for Minors. Underage users won’t be able to access this feature. 4. All video elements related to this feature provided by Douyin have acquired copyright authorization.\\nZHEJIANG, CHINA – OCTOBER 18 2019 Two U.S. senators have sent a letter to the U.S. national intelligence agency saying TikTok could pose a threat to U.S. national security and should be investigated. Visitors visit the booth of Douyin (Tiktok) at the 2019 Smart Expo in Hangzhou, east China’s Zhejiang province, Oct. 18, 2019.- PHOTOGRAPH BY Costfoto / Barcroft Media via Getty Images.\\nA longer terms of use and privacy policy was also found in Chinese within Douyin. Translated into English, some highlights from the text include:\\n“The ‘face-changing’ effect presented by this function is a fictional image generated by the superimposition of our photos based on your photos. In order to show that the original work has been modified and the video generated using this function is not a real video, we will mark the video generated using this function. Do not erase the mark in any way.”\\n“The information collected during the aforementioned detection process and using your photos to generate face-changing videos is only used for live detection and matching during face-changing. It will not be used for other purposes . . . And matches are deleted immediately and your facial features are not stored.”\\n“When you use this function, you can only use the materials provided by us, you cannot upload the materials yourself. The materials we provide have been authorized by the copyright owner”.\\n“According to the ‘Children’s Internet Personal Information Protection Regulations’ and the relevant provisions of laws and regulations, in order to protect the personal information of children / youths, this function restricts the use of minors”.\\nWe reached out to TikTok and Douyin for comment regarding the deepfakes feature, when it might launch, how the privacy of biometric scans are protected and the age limit. However, TikTok declined to answer those questions. Instead, a spokesperson insisted that “after checking with the teams I can confirm this is definitely not a function in TikTok, nor do we have any intention of introducing it. I think what you may be looking at is something slated for Douyin – your email includes screenshots that would be from Douyin, and a privacy policy that mentions Douyin. That said, we don’t work on Douyin here at TikTok.” They later told TechCrunch that “The inactive code fragments are being removed to eliminate any confusion,” which implicitly confirms that Face Swap code was found in TikTok.\\nA Douyin spokesperson tells TechCrunch “Douyin follows the laws and regulations of the jurisdictions in which it operates, which is China.” They denied that the Face Swap terms of service appear in TikTok despite TechCrunch reviewing code from the app showing those terms of service and the feature’s functionality.\\nThis is suspicious, and doesn’t explain why code for the deepfakes feature and special terms of service in English for the feature appear in TikTok, and not just Douyin, where the app can already be activated and a longer terms of service was spotted. TikTok’s U.S. entity has previously denied complying with censorship requests from the Chinese government in contradiction to sources who told The Washington Post that TikTok did censor some political and sexual content at China’s behest.\\nConsumerizing deepfakes\\nIt’s possible that the deepfakes Face Swap feature never officially launches in China or the U.S. But it’s fully functional, even if unreleased, and demonstrates ByteDance’s willingness to embrace the controversial technology despite its reputation for misinformation and non-consensual pornography. At least it’s restricting the use of the feature by minors, only letting you face-swap yourself, and preventing users from uploading their own source videos. That avoids it being used to create dangerous misinformational videos like the slowed down one making House Speaker Nancy Pelosi seem drunk, or clips of people saying things as if they were President Trump.\\n“It’s very rare to see a major social networking app restrict a new, advanced feature to their users 18 and over only,” Watchful.ai co-founder and CEO Itay Kahana tells TechCrunch. “These deepfake apps might seem like fun on the surface, but they should not be allowed to become trojan horses, compromising IP rights and personal data, especially personal data from minors who are overwhelmingly the heaviest users of TikTok to date.”\\nTikTok has already been banned by the U.S. Navy and ByteDance’s acquisition and merger of Musical.ly into TikTok is under investigation by the Committee on Foreign Investment in The United States. Deepfake fears could further heighten scrutiny.\\nWith the proper safeguards, though, face-changing technology could usher in a new era of user-generated content where the creator is always at the center of the action. It’s all part of a new trend of personalized media that could be big in 2020. Social media has evolved from selfies to Bitmoji to Animoji to Cameos, and now consumerized deepfakes. When there are infinite apps and videos and notifications to distract us, making us the star could be the best way to hold our attention.',\n",
              "  array([-5.1167054 , -4.3497972 , -0.01908616], dtype=float32),\n",
              "  2),\n",
              " ('For the past few weeks, a Samsung subsidiary named STAR Labs has been teasing what it calls “Neon” — an “artificial human” that will be unveiled at CES 2020 next week.\\nBut what exactly is Neon, and what is an artificial human? So far, we have very few official details, but most signs point toward the release of some sort of digital avatar technology: a realistic CGI human that users can interact with. It could be used for entertainment purposes or by businesses to create digital receptionists, customer service, and so on.\\nWhatever it is, though, it’s being hyped to death before it’s even been announced.\\nNeon has a social media presence a mile wide and just a few GIFs deep. There are Twitter, Facebook, and Instagram accounts for Neon, all sharing the same vague and extremely futuristic-looking images. Posts pose questions like “Have you ever met an ‘artificial’?” and tease technology called “Core R3,” which stands for “reality, realtime, responsive.” They also make clear that, whatever Neon is, it has nothing to do with Samsung’s AI assistant Bixby.\\nHonored to have so much coverage even before we unveil. But contrary to some news, NEON is NOT about Bixby, or anything you have seen before. #NEON is coming to #CES2020, so stay tuned! @neondotlife\\n— NEON (@neondotlife) December 26, 2019\\nThe project is led by Pranav Mistry, a human-computer interaction researcher and former senior vice president at Samsung Electronics. According to his LinkedIn profile, Mistry is now CEO of STAR Labs (which stands for Samsung Technology & Advanced Research) and new company Neon. On his Twitter page, he’s been stoking hype for the project, retweeting appreciative comments from people apparently given early previews. One describes Neon as “Artifical Intelligence that will make you wonder which one of you is real.”\\nUnofficial clues also point to digital avatar tech. US trademarks for “NEON Artificial Human,” “NEON.Life,” and “Core R3” have been registered by Samsung Research America (and spotted by LetsGoDigital). They describe Samsung NEON as offering:\\nEntertainment services, namely, production of special effects including model-making services, computer-generated imagery and computer-generated graphics for the production of motion pictures, videos and movie trailers; augmented reality video production; creating computer generated characters; design and development of computer-modeled versions of human beings using computer animation for use in movies, television, internet and other applications; design and development of software for virtual characters; creating for others custom computer-generated imagery, animations, simulations and models used for entertainment.\\nA job listing for STAR Labs looking for a senior media streaming engineer focuses on similar themes. It says the company is undertaking “independent initiatives to create new end-to-end businesses and expand growth areas for Samsung” and that employees are “building new immersive and intelligent services that are making science fiction a reality.”\\nFor past few years, I have been working on something exciting. FOLLOW @neondotlife to learn more. Please SHARE and ask your friends to join, too. https://t.co/m3DEbTqqYe pic.twitter.com/jaWgALTzj4\\n— Pranav Mistry (@pranavmistry) December 16, 2019\\nIn a recent interview, Mistry describes “digital humans” as a key technology for the 2020s. “Movies are full of examples where AI is brought into our world,” Mistry told LiveMint. “In Blade Runner 2049, Officer K develops a relationship with his AI hologram companion, Joi. While films may disrupt our sense of reality, ‘virtual humans’ or ‘digital humans’ will be reality. A digital human could extend its role to become a part of our everyday lives: a virtual news anchor, virtual receptionist, or even an AI-generated film star.”\\nNone of this is that groundbreaking, though.\\nThanks to advances in AI and CGI, digital human avatars have certainly become increasingly lifelike, but they’re functionally very limited. In 2018, China’s state-run press agency launched what it called an “AI news anchor“ that can read the headlines. Films like Star Wars have resurrected dead actors using CGI, and AI-generated Instagram influencers are also a thing. But all of these examples are of preprogrammed experiences.\\nWhile some efforts have been made to integrate avatars with chatbot technology, the end results are not too impressive. Conversation is slow, limited, and stilted, and none of these bots could be mistaken for humans. Last month, for example, New Age author Deepak Chopra was turned into a digital avatar. But “digital Deepak” looked more like a faction leader from the Civilization video game series than an artificial human.\\nIt’s possible that Samsung has made some leaps forward in this regard and that Neon’s technology will be truly game-changing. But let’s wait and see what the company has to offer, and ignore the ample hype. The company will be announcing more early next week.',\n",
              "  array([-4.865651  , -3.6033888 , -0.03556298], dtype=float32),\n",
              "  2),\n",
              " ('Here’s what AI experts think will happen in 2020\\nby TRISTAN GREENE — in ARTIFICIAL INTELLIGENCE\\n201\\nSHARES\\nIt’s been another great year for robots. We didn’t quite figure out how to imbue them with human-level intelligence, but we gave it the old college try and came up with GPT-2 (the text generator so scary it gives Freddy Krueger nightmares) and the AI magic responsible for these adorable robo-cheetahs:\\n*chirp chirp*\\nWho’s there? It’s early bird tickets to TNW2020\\nCOME IN\\nBut it’s time to let the past go and point our bows toward the future. It’s no  longer possible to estimate how much the machine learning and AI markets are worth, because the line between what’s an AI-based technology and what isn’t has become so blurred that Apple, Microsoft, and Google are all “AI companies” that also do other stuff.\\nYour local electricity provider uses AI and so does the person who takes those goofy real-estate agent pictures you see on park benches. Everything is AI — an axiom that’ll become even truer in 2020.\\nWe solicited predictions for the AI industry over the next year from a panel of experts, here’s what they had to say:\\nMarianna Tessel, CTO at Intuit\\nAI and human will collaborate. AI will not “replace humans,” it will collaborate with humans and enhance how we do things. People will be able to provide higher level work and service, powered by AI. At Intuit, our platform allows experts to connect with customers to provide tax advice and help small businesses with their books in a more accurate and efficient way, using AI. It helps work get done faster and helps customers make smarter financial decisions. As experts use the product, the product gets smarter, in turn making the experts more productive. This is the decade where, through this collaboration, AI will enhance human abilities and allow us to take our skills and work to a new level.\\nAI will eat the world in ways we can’t imagine today: AI is often talked about as though it is a Sci-Fi concept, but it is and will continue to be all around us. We can already see how software and devices have become smarter in the past few years and AI has already been incorporated into many apps. AI enriched technology will continue to change our lives, every day, in what and how we operate. Personally, I am busy thinking about how AI will transform finances – I think it will be ubiquitous. Just the same way that we can’t imagine the world before the internet or mobile devices, our day-to-day will soon become different and unimaginable without AI all around us, making our lives today seem so “obsolete” and full of “unneeded tasks.”\\nWe will see a surge of AI-first apps: As AI becomes part of every app, how we design and write apps will fundamentally change. Instead of writing apps the way we have during this decade and add AI, apps will be designed from the ground up, around AI and will be written differently. Just think of CUI and how it creates a new navigation paradigm in your app. Soon, a user will be able to ask any question from any place in the app, moving it outside of a regular flow. New tools, languages, practices and methods will also continue to emerge over the next decade.\\nJesse Mouallek, Head of Operations for North America at Deepomatic\\nWe believe 2020 to be the year that industries that aren’t traditionally known to be adopters of sophisticated technologies like AI, reverse course. We expect industries like waste management, oil and gas, insurance, telecommunications and other SMBs to take on projects similar to the ones usually developed by the tech giants like Amazon, Microsoft and IBM. As the enterprise benefits of AI become more well-known, the industries outside of Silicon Valley will look to integrate these technologies.\\nIf companies don’t adapt to the current trends in AI, they could see tough times in the future. Increased productivity, operational efficiency gains, market share and revenue are some of the top line benefits that companies could either capitalize or miss out on in 2020, dependent on their implementation. We expect to see a large uptick in technology adoption and implementation from companies big and small as real-world AI applications, particularly within computer vision, become more widely available.\\nWe don’t see 2020 as another year of shiny new technology developments. We believe it will be more about the general availability of established technologies, and that’s ok. We’d argue that, at times, true progress can be gauged by how widespread the availability of innovative technologies is, rather than the technologies themselves. With this in mind, we see technologies like neural networks, computer vision and 5G becoming more accessible as hardware continues to get smaller and more powerful, allowing edge deployment and unlocking new use cases for companies within these areas.\\nHannah Barnhardt, VP of Product Strategy Marketing at Deluxe Entertainment\\n2020 is the year AI/ML capabilities will be truly operationalized, rather than companies pontificating about its abilities and potential ROI. We’ll see companies in the media and entertainment space deploy AI/ML to more effectively drive investment and priorities within the content supply chain and harness cloud technologies to expedite and streamline traditional services required for going to market with new offerings, whether that be original content or Direct to Consumer streaming experiences.\\nLeveraging AI toolsets to automate garnering insights into deep catalogs of content will increase efficiency for clients and partners, and help uphold the high-quality content that viewers demand. A greater number of studios and content creators will invest and leverage AI/ML to conform and localize premium and niche content, therefore reaching more diverse audiences in their native languages.\\nTristan Greene, reporter for The Next Web\\nI’m not an industry insider or a machine learning developer, but I covered more artificial intelligence stories this year than I can count. And I think 2019 showed us some disturbing trends that will continue in 2020. Amazon and Palantir are poised to sink their claws into the government surveillance business during what could potentially turn out to be President Donald Trump’s final year in office. This will have significant ramifications for the AI industry.\\nThe prospect of an Elizabeth Warren or Bernie Sanders taking office shakes the Facebooks and Microsofts of the world to their core, but companies who are already deeply invested in providing law enforcement agencies with AI systems that circumvent citizen privacy stand to lose even more. These AI companies could be inflated bubbles that pop in 2021, in the meantime they’ll look to entrench with law enforcement over the next 12 months in hopes of surviving a Democrat-lead government.\\nLook for marketing teams to get slicker as AI-washing stops being such a big deal and AI rinsing — disguising AI as something else — becomes more common (ie: Ring is just a doorbell that keeps your packages safe, not an AI-powered portal for police surveillance, wink-wink).\\nHere’s hoping your 2020 is fantastic. And, if we can venture a final prediction: stay tuned to TNW because we’re going to dive deeper into the world of artificial intelligence in 2020 than ever before. It’s going to be a great year for humans and machines.',\n",
              "  array([-3.8562398 , -3.8848128 , -0.04259342], dtype=float32),\n",
              "  2),\n",
              " ('WASHINGTON (Reuters) - The Trump administration took measures on Friday to crimp exports of artificial intelligence software as part of a bid to keep sensitive technologies out of the hands of rival powers like China.\\nUnder a new rule which goes into effect on Monday, companies that export certain types of geospatial imagery software from the United States must apply for a license to send it overseas except when it is being shipped to Canada.\\n“They want to keep American companies from helping the Chinese make better AI products that can help their military,” said James Lewis, a technology expert with the Washington-based Center for Strategic and International Studies think tank.\\nThe rule will likely be welcomed by industry, Lewis said, because it had feared a much broader crackdown on exports of most artificial intelligence hardware and software\\nThe measure covers software that could be used by sensors, drones, and satellites to automate the process of identifying targets for both military and civilian ends, Lewis said, noting it was a boon for industry, which feared a much broader crackdown on exports of AI hardware and software.\\nThe measure is the first to be finalized by the Commerce Department under a mandate from a 2018 law, which tasked the agency with writing rules to boost oversight of exports of sensitive technology to adversaries like China, for economic and security reasons.\\nReuters first reported that the agency was finalizing a set of narrow rules to limit such exports in a boon to U.S. industry that feared a much tougher crackdown on sales abroad.\\nThe rule will go into effect in the United States alone, but U.S. authorities could later submit it to international bodies to try to create a level playing field globally.\\nIt comes amid growing frustration from Republican and Democratic lawmakers over the slow roll-out of rules toughening up export controls, with Senate Minority Leader Chuck Schumer, a Democrat, urging the Commerce Department to speed up the process.\\n“While the government believes that it is in the national security interests of the United States to immediately implement these controls, it also wants to provide the interested public with an opportunity to comment on the control of new items,” the rule release said.',\n",
              "  array([-1.0756778 , -1.5758603 , -0.79384315], dtype=float32),\n",
              "  2),\n",
              " ('WASHINGTON (Reuters) - The Trump administration will send a big contingent of senior officials to an annual technology gathering in Las Vegas next week as the tech industry faces increased scrutiny in Washington.\\nFILE PHOTO: The logos of Amazon, Apple, Facebook and Google are seen in a combination photo from Reuters files. REUTERS/File Photo\\nTransportation Secretary Elaine Chao, Commerce Secretary Wilbur Ross, Energy Secretary Dan Brouillette, White House adviser Ivanka Trump, U.S. Chief Technology Officer Michael Kratsios will be among more than 150 other government officials from the United States and around the world will attend CES, the annual tech industry event. In prior years, the event has drawn fewer senior U.S. officials — typically just one cabinet official — and did not draw any in 2019 because of a government shutdown.\\nThe event, which runs from Jan. 7-10, draws more 175,000 visitors and 4,500 companies exhibiting new technologies, including airlines, automakers, battery manufacturers and hundreds of tech start-ups.\\nThe White House has touted technology improvements as a way to boost U.S. employment and is working to ease regulatory barriers to advanced technologies like drones and self-driving cars. At the same time, the Justice Department and Federal Trade Commission are probing whether big tech firms like Facebook Inc, Alphabet Inc, Amazon.com and Apple Inc are violating antitrust laws.\\nThe largest tech firms generally skip CES in favor of holding their own events.\\nFederal Communications Commission Chairman Ajit Pai and Federal Trade Commission Chairman Joseph Simons will speak at a fireside chat, and three of the other four FCC commissioners will attend as will two other FTC commissioners.\\nPai has proposed shifting part of a block spectrum set aside for auto safety to accommodate the rapidly growing number of wireless devices. Numerous companies are displaying new wireless devices at CES. Chao is expected to talk about self-driving car policies.\\nElena Hernandez, spokeswoman for the White House Office of Science and Technology Policy, said Friday that Kratsios will be at CES to discuss the administration’s national strategy for artificial intelligence launched in 2018.\\n“Recognizing the significance of technology on the American workforce, our national security, and the U.S. economy, the Trump administration continues to make leadership in the cutting-edge industries of the future a top priority,” Hernandez said.\\nPresident Donald Trump, who has had strained relations with technology companies and criticized several major firms for alleged bias against conservatives, has met repeatedly with major tech company CEOs since taking office, including meetings in 2019 with the chief executives of Facebook, Apple and Google.',\n",
              "  array([-2.8268595, -0.8893058, -0.6351414], dtype=float32),\n",
              "  2),\n",
              " ('Adobe CTO Abhay Parasnis sees a shift happening.\\nA shift in how people share content and who wants to use creative tools. A shift in how users expect these tools to work — especially how much time they take to learn and how quickly they get things done.\\nI spoke with Parasnis in December to learn more about where Adobe’s products are going and how they’ll get there — even if it means rethinking how it all works today.\\n“What could we build that makes today’s Photoshop, or today’s Premiere, or today’s Illustrator look irrelevant five years from now?” he asked.\\nIn many cases, that means a lot more artificial intelligence; AI to flatten the learning curve, allowing the user to command apps like Photoshop not only by digging through menus, but by literally telling Photoshop what they want done (as in, with their voice). AI to better understand what the user is doing, helping to eliminate mundane or repetitive tasks. AI to, as Parasnis puts it, “democratize” Adobe’s products.\\nWe’ve seen some hints of this already. Back in November, Adobe announced Photoshop Camera, a free iOS/Android app that repurposes the Photoshop engine into a lightweight but AI-heavy interface that allows for fancy filters and complex effects with minimal effort or learning required of the user. I see it as Adobe’s way of acknowledging (flexing on?) the Snapchats and Instas of the world, saying “oh, don’t worry, we can do that too.”\\nBut the efforts to let AI do more and more of the heavy lifting won’t stop with free apps.\\n“We think AI has the potential to dramatically reduce the learning curve and make people productive — not at the edges, but 10x, 100x improvement in productivity,” said Parasnis.\\n“The last decade or two decades of creativity were limited to professionals, people who really were high-end animators, high-end designers… why isn’t it for every student or every consumer that has a story to tell? They shouldn’t be locked out of these powerful tools only because they’re either costly, or they are more complex to learn. We can democratize that by simplifying the workflow.”',\n",
              "  array([-4.2956896 , -3.114675  , -0.05977134], dtype=float32),\n",
              "  2),\n",
              " ('How much tech can you take? Next week aims to stretch your horizons, as well as your credulity, as thousands of new products are launched and demoed at the giant CES expo.\\nArtificial intelligence, 5G, foldables, surveillance tech, 8K and robotics are set to be among this year\\'s buzzwords.\\nBut also expect Trump to feature. The President\\'s clashes with China have led some of the communist country\\'s biggest tech firms to cancel or reduce their involvement in the Las Vegas event. But the prospect of an imminent trade deal points towards tensions easing and greater access to Chinese consumers.\\nIvanka Trump - the US leader\\'s daughter - is also attending to give a \"keynote\" interview to CES chief Gary Shapiro.\\n\\nThis will not be the first time Ms Trump has represented the White House at a tech-themed event\\nHe once called on Americans to oppose her father because of \"his racism and inanity\".\\nNow Mr Shapiro faces criticism himself for inviting Ivanka to discuss \"the future of work\". Critics claim she is benefiting from nepotism while better-qualified female tech champions are overlooked.\\nBut some of Silicon Valley\\'s most powerful women are taking part.\\nApple\\'s privacy chief Jane Horvath is making a rare public appearance. It\\'s the first time her company has formally been involved in CES since it hawked its Newton handheld back in 1992.\\nIn addition, ex-Hewlett Packard chief Meg Whitman will co-host another keynote with former Dreamworks co-founder Jeffrey Katzenberg, to showcase Quibi. The platform aims to outmanoeuvre Netflix and Amazon on mobile with a range of 10-minute-long shows.\\n\\nThe executives are expected to charge a $5 monthly fee for Quibi\\nBut the reason CES generates so much interest is its gadgets. Here are our hot spots from this years show:\\nSMART HOME\\nAmazon and Google will once again be hiring out lots of floor space to spotlight products that tie into their virtual assistants.\\nThe success of their smart speakers helped global demand for net-connected home products grow by a healthy 24% in unit terms in 2019, according to research firm IDC.\\nThe challenge at this point is to pioneer new types of devices, rather than tweak what is already on the market.\\n\\nThe Moxie shower head is detachable so can be removed by those who don\\'t want their bathroom utterances overheard\\nBathroom specialist Kohler is already attracting attention for Moxie. The shower head integrates an Alexa-enabled speaker and microphone - but thankfully no camera.\\nMeanwhile many of CES\\'s smaller start-ups have looked to the kitchen for inspiration.\\n\\nSmartyPans allows owners to \"record\" a recipe so it can be shared with others\\nSmartypans has a frying pan that checks the weight and temperature of ingredients before guiding you through the cooking process via an app.\\nInirv wants you to swap your cooker\\'s knobs for its smart dials.\\n\\nInirv\\'s dials are designed to be retrofitted to existing cookers\\nThey let owners turn up the heat via voice command, and automatically turn off the stove if it is left unattended for too long.\\nAnd PantryOn aims to automate food shopping lists via smart shelves that monitor when a family\\'s favourite groceries run low.\\n\\nPantryOn\\'s shelves require their owners to keep foodstuffs in the same place\\nThe caveat is that its current prototypes look to be quite bulky, leaving less storage space as a result.\\nNot all home tech requires an always-on net connection.\\nBrightLock unlocks front doors by detecting a pattern of light pulses fired from a smartphone\\'s flash.\\n\\nHavr claims that its light-detecting BrightLock can be installed in less than five minutes\\nThe idea is that you can easily share a light-based code with friends, tradesmen or others needing temporary access.\\nTownew promises to liberate you from the toil of having to tie up rubbish bags.\\n\\nTownew can also pop its lid up automatically when it detects a nearby object\\nThe bin self-seals sacks at the touch of a button, but requires you to be locked into buying the manufacturer\\'s bin liner refills.\\nAnd Lua wants to \"turn your plant into a pet\" with a sensor-packed pot that shows animated faces to let you know when your foliage is thirsty, or in need of sunlight.\\n\\nThe makers of Lua raised more than £175,000 via Indiegogo\\'s crowdfunding site\\nOne theme to watch out for is the further rise of the pod, with a number of companies seeking to emulate Nespresso\\'s coffee capsules.\\nThey include Tigout, whose machine makes bite-sized bakes and souffles, and AI-Plus Plantbox, a smart-farming appliance that turns pods full of seeds into small batches of vegetables and herbs.\\n\\nWill consumers think these capsule-based snacks are worth the wait?\\nOne further home-tech trend is smaller appliances for compact homes.\\nMorus Zero is a countertop tumble dryer that uses a vacuum-based system to dry clothes.\\nIts makers claim the technology makes it more energy-efficient than traditional heat-based models. But they may have to address concerns raised by some crowdfunder backers about whether it\\'s possible to deliver what has been promised.\\n\\nMorus says its dryer is less likely to shrink clothes than heat-based alternatives\\nDaanTech\\'s Bob is another example, with what it claims is the world\\'s smallest dishwasher.\\nIt only has space for two people\\'s tableware. Surely the sink wouldn\\'t take too much longer?\\n\\nBob is designed for studio apartments\\nThere will be lots of new home security products too.\\nRing should expand its portfolio of thief-deterrent tech, but will this be the moment it upgrades its surveillance capabilities? Its parent Amazon has the AI know-how, but may be biding its time to avoid controversy.\\nOthers aren\\'t hanging about.\\nAmaryllo will promote Athena, a security camera that recognises people\\'s voices and faces, to distinguish friends and family from strangers.\\n\\nAthena is said to be able to spot flames from up to 20ft (6.1m) away\\nFurthermore, it can recognise a fire from afar and raise the alarm.\\nTELEVISIONS\\nTVs have been at the heart of CES since its start.\\nIt looks like the big news this time will be a no-bezel edge-to-edge screen from Samsung and a flexible OLED model from LG that rolls down from the ceiling. The question for both is whether the impressive engineering involved comes at the cost of fragility.\\n\\nLG Display says it has a 65in ceiling-mounted screen that can be rolled up when not in use\\nThere\\'s also likely to be a big push to take 8K mass-market. The tech features four times as many pixels as 4K sets and 16 times as many as 1080p screens.\\nTokyo\\'s Summer Olympics are being filmed in the \"super hi-vision\" format, but it\\'s still unclear which broadcasters will support it beyond Japan\\'s NHK and Italy\\'s Rai.\\nWith little other 8K content, TV-makers are under pressure to prove that their upscaling technologies noticeably enhance lower-resolution Blu-Ray disks and video-streams. It can take considerable computing smarts to do this well, but several companies say they have trained \"deep learning\" systems that are capable of the task.\\n\\nSamsung says its AI-based upscaling technology has learned rules such as \"lines should be thin\", and \"rough areas should be more vivid\"\\nAs if the differences between LED, OLED, QLED and microLED were not baffling enough, there will be a new technology in town: Mini-LED.\\nThis involves using smaller light-emitting diodes than normal, to illuminate a screen\\'s colour pixels. This allows there to be more distinct lighting zones, which in turn should reduce the blooming effect you sometimes get when light spills from bright objects in a scene into surrounding darker areas.\\nIt won\\'t produce the deep blacks of OLED, where each pixel is self-illuminating. Nor will it match microLED tech, where the diodes are so small they can be assigned to the pixels on a 1:1 basis.\\nBut it should deliver an impressive HDR (high dynamic range) picture at a relatively affordable price.\\nTCL has confirmed it will launch Mini-LED TVs at CES, and other brands may do so too, even if they call them by another name.\\nAlso look out to see which brands adopt the new Filmmaker Mode.\\n\\nTVs are about to get another logo on their box\\nA number of Hollywood directors, including Martin Scorsese, James Cameron and Christopher Nolan, have spearheaded an initiative to let TVs display movies as their creators intended.\\nAt a single button push, motion-blurring is switched off, and the colours, frame rate and aspect ratio are all adjusted.\\nLG, Panasonic and Vizio have already indicated they will adopt this in at least some new TVs.\\nROBOTS & AI\\nSamsung\\'s skunkworks unit Star Labs has teased Neon in the run-up to CES 2020.\\nIt is described as being an \"artificial human\" but little else has been confirmed beyond the fact it isn\\'t intended to replace the firm\\'s Bixby virtual assistant. All will be revealed on Monday.\\n\\nPatents suggest Neon may involve computer-generated humans for use in augmented reality content\\nOther companies have been more forthcoming about their robo-plans.\\nPicnic will show off a machine that can prepare up to 300 pizzas an hour, each with a customised set of toppings controlled by an app. The start-up has ambitions to extend into sandwiches, salads and tortillas soon, and is pitching the product at restaurant chains, rather than consumers.\\n\\nPicnic says its pizza-making robot can be relied on to deliver consistent results\\nOther robots on show will be more focused on keeping us entertained, rather than concentrating on the workplace.\\nChina\\'s Elephant Robotics will demo MarsCat. It\\'s a kind of feline twist on Sony\\'s robo-dog Aibo - it can play with toys, recognise its owner\\'s voice and even interact with real cats.\\n\\nThe MarsCat robot can be programmed to learn new skills\\nBut what extends its appeal is that it runs off a Raspberry Pi, which means it is programmable and can be used to teach students to code AI applications.\\nAt the other end of the scale, Tombot will promote its robotic labrador puppy. The touch-sensitive machine is designed to provide comfort to residents in old people\\'s homes, and others who would benefit from a pet, but cannot deal with a real animal.\\n\\nTombot makes puppy-like sounds and wags its tail to simulate emotional responses\\nIt has been designed by Jim Henson\\'s Creature Shop - the team behind the Muppets - and is certainly cute, if limited in function.\\nPibo looks set to be a trickier sell. The humanoid robot with a camera in its mouth is being pitched at teenage girls. Marketing videos show it taking photos of them and their food, recording their diary entries and telling them it loves them. It seems odd and a little creepy.\\n\\nPibo has a vibration sensor on its head and a 5 megapixels camera in its mouth\\nYukai\\'s Bocco robots may have a better chance of hitting the mark in the cuteness stakes.\\nThey offer a way for children to send and receive voice messages to their parents, and babble back in their own language if addressed themselves.\\nThey can also be paired with add-on sensors to disclose when a family member has arrived home, what the weather is doing and whether or not the front door was closed properly.\\n\\nThe Bocco robots become excited when a family member\\'s birthday nears\\nSwitching tack to software-focused AI, there\\'s a tension between what can be done and how to prevent it.\\nFor example, facial recognition specialist Cyberlink will demo its latest capabilities. They include using its FaceMe system to determine the age, gender and emotional state of passers-by, to show them appropriate ads.\\nBut D-ID is seeking to frustrate facial recognition checks with a program that makes minor changes to photos, to prevent people being recognised by computers even though they remain identifiable to the human eye.\\nIt should give people a way to share images online with less risk of being tracked as a consequence.\\n\\nD-ID says the anonymising changes cannot be reverse-engineered\\nElsewhere, Hour One will demo its synthetic character software.\\nIt uses AI to create the video and voices of computer-generated characters.\\nSkip Youtube post by Niki Pure\\nWarning: Third party content may contain adverts\\nReport\\nEnd of Youtube post by Niki Pure\\nIn time, the company hopes to build up a bank of celebrities who will let their likenesses be leased to promote products, without the stars having to get directly involved.\\nMirriad will be holding private meetings to show off a system that lets TVs and movies add brand placements after they have been shot.\\nMedia caption\\nWATCH: Mirriad\\' product placement tech adds ads to movies\\nCheck out the results in our video above.\\nAnd keeping it quirky, Getcoo will exhibit its crowdfunded Lego-scanner.\\nThe Piqabrick is a small cabinet that uses object recognition software to identify any part of the toy\\'s vast library of pieces.\\n\\nThe makers of the Piqabrick scanner can help users find out what sets they can build with their bricks\\nTRANSPORT\\nYou can keep your self-driving cars (and there will be a lot of them).\\nManta5\\'s Hydrofoil e-bike offers a new way to travel that\\'s ready to roll - or at least glide.\\n\\nThe Hydrofoiler XE-1 can be used in the sea, as long as the water is not too rough\\nThe water cycle\\'s pedals push a propeller, and in the place of wheels there are wing-like parts that create more lift the faster the user cycles. The rider\\'s efforts are aided by an electric motor, which can help the e-bike achieve speeds of up to 13mph (21 km/h).\\nIt\\'s the first commercial product of its kind. But a decade\\'s worth of R&D doesn\\'t come cheap - each e-bike costs £5,800.\\nStaying off-shore, there\\'s a number of companies pitching underwater drones for recreational use, or as a way to aid fishing expeditions. But one aquatic vehicle has the potential to save lives.\\n\\nOceanAlpha says its drone can get to an endangered victim faster than any swimmer\\nOceanAlpha\\'s Dolphin1 is a remote-controlled lifebuoy designed to save people at risk of drowning. Rescuers can avoid putting themselves at risk by staying out of the sea, and may even be in a better position to keep the victim in sight as a result.\\nBack on land, BMW, Mercedes-Benz, Hyundai and Nissan are among the automakers promising to show off new concept designs.\\n\\nBMW says its concept car\\'s interior is inspired by boutique hotels\\nThe Detroit Auto Show was previously held soon after CES. But this year it has shifted to June. That may give the car giants more scope to make real-world announcements, as well as show off dream designs.\\nFor now though, only the electric car start-ups Byton and Fisker are certain to show off new models destined for production.\\nThe former is hosting a press conference to demo the user interface of its forthcoming M-Byte four-wheel drive, which includes a \"dark mode\".\\n\\nByton\\'s M-Byte is due to go on sale in China this year, and in Europe and the US in 2021\\nThe latter will show off its Ocean car to the public for the first time.\\nIt features a full-length solar panel roof to drip-feed the battery, although the tech is at least decades away from being capable of being the main power source.\\n\\nFisker says the solar-panel roof should provide 1,000 miles-worth of free motoring over the course of a year\\nThere will also be lots of chatter about new in-car infotainment experiences too.\\nHonda has said it will unveil its own virtual assistant, which can be summoned with the wake words \"OK Honda\".\\nLG will showcase webOS Auto, an operating system for internet-connected cars. It is based on the firm\\'s much-commended smart TV system, but will face competition from the incumbents Apple Carplay and Android Auto.\\n\\nTo date, LG has only shown concept designs for webOS Auto\\nMeanwhile, Bosch has been teasing a new kind of 3D display for car dashboards that doesn\\'t require the driver to wear special glasses.\\nThe firm claims that drivers\\' brains react more quickly to alerts as a consequence, in addition to it being able to show turnings on sat-nav maps more clearly.\\n\\nBosch says its 3D screen makes alerts seem \"more obvious and urgent\"\\nElsewhere, scooter giant Segway Ninebot will demo a self-balancing, self-driving two-wheeler that can be summoned via an app.\\nIt will also seek feedback to the Apex - its first motorcycle, which it has yet to commit to putting on sale.\\n\\nSegway is considering whether to start making motorcycles\\nAnd French start-up Wello hopes to attract interest for an unusual compact three-wheeler.\\nIt is designed for short journeys in which a single passenger is transported at up to 25mph (40km/h).\\n\\nWello is pitching its vehicle as being faster than a bicycle but more eco-friendly than a car\\nHEALTH & WEARABLES\\nBeyond headphones, the wearable tech market has never taken off to the degree the industry had hoped for, with the possible exceptions of Apple\\'s Watch and, in Asia, Xiaomi\\'s wristbands.\\nFitbit, once the dominant player, is set to be sold to Google, so may be quieter at this year\\'s CES than in the past.\\nBut others are hoping to make headway by aiming above the arm.\\n\\nThe Norm Glasses hope to succeed where Google Glass struggled\\nThey include Human Capable, which will be showing off the Norm glasses.\\nIt says they will be able to make calls, show directions and recipes, and both shoot and play videos - so a less geeky-looking Google Glass for the 2020s.\\nThe start-up has already acknowledged problems with the noise-cancelling tech in the prototype it will exhibit, yet claims it will be able to launch a fixed product within months.\\nWaverly Labs is back at the expo with its second take on language-translating earbuds.\\n\\nAmbassador uses two far-field microphones to help maximise the quality of the audio it captures\\nAmbassador lets up to four people chat by pairing their headsets to a single smartphone.\\nWhen the BBC tested it last month, there was a 2-3 second delay, but the firm says 5G networks should help it deliver near-instantaneous interpretations soon.\\nPlus, Ao Air has a face mask that looks like a cyberpunk movie prop.\\n\\nThe Atmos mask provides protection from pollution without needing to be fitted tightly over the user\\'s face\\nThe Atmos uses a fan-based system to filter the air, and doesn\\'t need a tight seal against the wearer\\'s face. That means make-up shouldn\\'t get smudged and glasses won\\'t steam up, at least in theory.\\nFeet also get a look-in at the show.\\nWahu is a pair of shoes that change the shape of their soles to suit the local environment.\\n\\nThe Wahu shoes use an electronic plunger to fill chambers at their base with fluid\\nThe goal is to produce extra grip or cushioning, as required. The trick will be keeping both shoes in synch.\\nStaying with footwear, Shoeblast has a gadget that promises to prevent older shoes from stinking.\\nThe device uses a humidity sensor to judge how much heat and ultraviolet light to apply to sterilise trainers.\\n\\nShoeblast claims to be able to kill the bacteria that cause shoes to stink\\nOther body parts will be addressed by a new sex toys zone, which was created after last year\\'s confusion about whether such products should be allowed on the show floor at all.\\nAnd there are also all kinds of new ways to address our wider physical health.\\n\\nBisu and Vivoo use apps to scan and analyse the results of proprietary urine tests\\nThey include Bisu and Vivoo, rival pee-on-a-stick smart urine analysers.\\nBoth analyse the results to make diet and lifestyle change suggestions. Just make sure to wash your hands before using their apps.\\nMeanwhile, EnvisionBody wants to help the public get fitter by showing them what they would look like if they did more exercise.\\nIt plans to work with gym equipment-makers to show idealised versions of users\\' physiques as they work out.\\nWhatever the physical benefits, the firm will face questions as to what effect this would have on users\\' mental health.\\nOTHERS\\nAnd there\\'s more.\\nImpossible Foods\\' chief executive is hosting a press conference as well as headlining a high-profile dinner. He\\'s already acknowledged work on a follow-up to the firm\\'s plant-based beef substitute, so is it ready for mass consumption?\\nBig smartphone news is typically held back for Mobile World Congress in February. But OnePlus will have a concept handset at CES that hides its rear cameras when they are not in use by electronically tinting an otherwise transparent glass panel above them.\\nSamsung could also have more to disclose about a folding design it trailed in October.\\n\\nSamsung teased a flip-phone concept last year but has yet to publicly commit to the idea\\nOthers are working on ways to retrospectively turn existing handsets into foldables.\\nThe makers of the Castaway will show off progress on a flip-open case that doubles as a detachable second screen.\\nAnd Pocket Display aims to go one better with an add-on that trebles the owners\\' view.\\n\\nDual and triple-monitors are popular in the workplace but have yet to catch on with handsets\\nPlenty of gadgets will be of even more questionable value.\\nDoes the world really want a circular handset?\\nIs there truly a gap in the market for a table-in-a-suitcase?\\nAnd are dog owners crying out for a harness to show them how their pet is feeling?\\n\\nThe Cyrcle Phone, MoDesk and Inupathy harness are all hoping to convince store buyers to stock them\\nThese may seem like money-losing nightmares. But bad ideas are sometimes the stepping stones to good ones.\\nSo with that in mind, who can be sure that Procter & Gamble\\'s Rollbot - a smartphone-controlled robot that fetches you more loo paper when you run out - might not go on to wonderful things after its CES 2020 debut?\\n\\nThere could come a day when you are very grateful for RollBot',\n",
              "  array([-3.4608555, -1.3134273, -0.3571033], dtype=float32),\n",
              "  2),\n",
              " ('IT systems in the NHS are so outdated that staff have to log in to up to 15 different systems to do their jobs.\\nDoctors can find themselves using different logins for everything from ordering x-rays and getting lab results to accessing A&E records and rotas.\\nThe government in England said it was looking to streamline the systems as part of an IT upgrade.\\nAround £40 million is being set aside to help hospitals and clinics introduce single-system logins in the next year.\\nAlder Hey in Liverpool is one of a number of hospitals which have already done this, and found it reduced time spent logging in from one minute 45 seconds to just 10 seconds.\\nWith almost 5,000 logins per day, it saved over 130 hours of staff time a day, to focus on patient care.\\n\\'Time to get basics right\\'\\nHealth Secretary Matt Hancock said it was time to \"get the basics right\".\\n\"It is frankly ridiculous how much time our doctors and nurses waste logging on to multiple systems.\\n\"Too often outdated technology slows down and frustrates staff.\"\\nThe latest announcement comes after the government demanded the NHS phase out the use of fax machines this year.\\nMr Hancock said the project was one of the steps which the government - which has set up a new agency called NHSX to drive forward progress on technology - was hoping to make in the coming years.\\nThese included closer integration of social care and NHS records, and greater use of artificial intelligence.\\nBritish Medical Association leader Dr Chaand Nagpaul said logging on to multiple systems did waste time.\\nBut he said on its own this move would not solve all the problems, pointing out that many of the IT systems themselves were \"antiquated\" and needed upgrading.\\n\"This will require real investment in IT infrastructure above that in this announcement.\"\\nAdam Brimelow, of NHS Providers, which represents managers, agreed wider IT systems needed investment, particularly in areas such as mental health, community services and ambulance trusts that often involved remote working for staff.\\nBut he said multiple logins was still a \"very real and pressing issue\".',\n",
              "  array([-4.072326  , -3.8825605 , -0.03836223], dtype=float32),\n",
              "  2),\n",
              " (\"SHANGHAI — A bespectacled eight-year-old has become the poster child for China’s campaign to dominate the world of high tech.\\nFrom his home in Shanghai, Vita Zhou hosts training videos for other children on how to code for artificial intelligence. He already has almost 80,000 followers on the Chinese streaming website Bilibili, and some of his videos have gained more than 1.3 million views. Vita has even attracted the attention of Apple CEO Tim Cook, who sent him birthday wishes Monday on Weibo, China’s equivalent of Twitter.\\nChina using artificial intelligence to build economy, monitor population\\nJULY 13, 201902:44\\n“What do you think? Isn’t it easier to write code once you understand how it works?” Vita says in one video. With the help of his dad, Zhou Ziheng, he demonstrates how to write codes with Apple-developed Swift Playgrounds, an app teaching kids basic coding through interactive games.\\nVita’s celebrity comes as China steps up efforts to become a world leader in artificial intelligence by 2030. The trend of teaching young people to code has been on the rise in recent years, particularly as the Asian giant fights to close the gap in its workforce in the technology sector, most notably AI talent. In November, China’s education ministry updated its curriculum to include books about AI, big data, coding and quantum computing.\\nA quarter of the 422-page recommended reading list is now about science, math, chemistry, aerospace, medicine and most notably AI.\\n“Coding’s not that easy but also not that difficult — at least not as difficult as you have imagined,” Vita, who is familiar with Swift, Scratch and C++ languages, told the AFP news agency.\\nChina has a lot of ground to make up on AI, with the number of top researchers in the field standing at one-fifth of that in the United States in 2017, according to research by the Washington-based Center for Data Innovation.\\nAt the same time, it faces a shortage of 5 million AI professionals, according to a 2017 article from the state-owned newspaper People’s Daily.\\nThese disadvantages have not stopped it from setting ambitious targets: The country aims to catch up with the U.S. next year, based on “A Next Generation Artificial Intelligence Development Plan,” a government blueprint.\\nIn order to close in on the talent gap, the country is now speeding up AI education for children, in addition to efforts to increase the talent base from universities. By 2018, there were 96 Chinese universities with AI-related programs, up from just 19 in 2017.\\nDespite some shortcomings, a trove of Chinese AI companies such as iFlytek, SenseTime, Cloudwalk and DJI, have caught the world’s attention for standing out in sound recognition, facial recognition and drone technologies. China’s big tech companies, such as Baidu, Tencent, Alibaba and Huawei, also have invested heavily in AI research and development.\\nA trainer leading a class at a children's computer coding training center in Beijing on Nov. 8, 2019Wang Zhao / AFP - Getty Images file\\nSome of those companies have taken a hit in China’s trade war with the U.S., with Washington blocking a few Chinese tech firms from acquiring its most advanced technologies. But experts say the roadblocks are only fueling China’s desire to get ahead.\\n“The increasingly fierce trade and technology competition between China and the U.S. puts pressure on China to improve its innovative capacity,” said Zhang Xusheng, a science, technology, engineering and math professor at Zhejiang University. “And it naturally means we need to bring the students to study high-tech and be more innovative.”\\nRecommended\\nWORLD\\nChina to ban plastic bags in major cities by end of 2020\\nSECURITY\\nTwo iPhones or the privacy of billions: Why Apple vs. the FBI matters\\nIn 2018, the education ministry added AI to the high school curriculum, encouraging around 25 million teenagers to study the technology. The same year, China’s first AI textbook for high school students — which introduces the basics of image recognition, sound recognition, text recognition and deep learning — was put into use in more than 40 pilot schools.\\n“I would like to read the books to explore the scientific reasoning behind things like AI, aerospace, programming and big data,” Cui Jingjing, 14, a high school student in Fujian, said. “I am also keen to join science competitions.”\\n“I think China will win the AI race with the U.S.,” Cui said, “We are catching up very fast.”\\nChina is not alone in ramping up AI education. While the private sector has led the response to AI, governments like France, South Korea and the United States also have strategies in place to expand their workforce in the sector with increased investments, although predominantly at the postsecondary level, according to a 2019 UNESCO report.\\nMany European Union member states are also reviewing their curricula to integrate more lessons about computational thinking in the classroom. Some countries like Austria, Poland and Lithuania have long provided strong computer science education in high schools.\\nA pupil reading a book outside a classroom as she waits to attend a class at a children's computer coding training centre in Beijing on Nov. 8, 2019.Wang Zhao / AFP - Getty Images file\\nThe enthusiasm for AI education goes beyond policy. The market value of the coding industry for children reached around $57 million in 2018 and is expected to surge to around $4.3 billion by 2023, increasing 650 percent in the span of five years, according to a report by iResearch, a Shanghai-based consulting company.\\nThat investment is transforming classrooms. In Shenzhen, China’s tech hub, an AI program for students in grades 3 to 8 was being piloted in 2019.\\nZheng Weicheng, a primary school math teacher in Fujian province, thinks that teaching AI also has broader benefits by helping children establish scientific concepts and improve their problem-solving ability, which will directly benefit their future development.\\n“Well-equipped youths lead to a powerful country,” Zheng said.\",\n",
              "  array([-4.2216325, -3.514863 , -0.0454436], dtype=float32),\n",
              "  2),\n",
              " ('For some people—roughly 170,000 of us—the new year doesn’t really begin until we descend upon Las Vegas for CES. WIRED’S editors and writers will be at the annual consumer tech trade show all week seeking out the latest trends, products, services, oddities, and absurdities that will help set the tone for technology in 2020.\\n\\nAs usual, our coverage comes with a caveat: Most of the new tech we see in Las Vegas won’t ship right away, or possibly ever. But strip away all the glitz and hype, and CES remains a good opportunity to get a sense of how tech-makers are thinking about the coming months or years. Here’s what the tech industry is buzzing about on the way to CES 2020.\\n\\n5G, Thanks\\nEven after all the talk about how it’s going to change the way our devices connect to the internet and to each other, 5G was a bust at last year’s CES. Consumer electronics companies and wireless carriers alike were understandably excited about the potential of this next-gen wireless tech, and we did see a few 5G-ready, premium handsets hit the market in 2019. But even now, a year later, it’s hard to make sense of where real 5G exists in the US and who can access it.\\n\\nCES 2020, then, will be an opportunity for tech makers to demonstrate that 5G will be an actual thing this year, and they’re not going to hold back. Some of these announcements will be around specific devices, like Dell’s 5G-ready Latitude laptop. Others will be around chipsets: MediaTek, for example, plans to roll out a premium 5G chipset for phones at CES, and Qualcomm will likely expound upon the announcements it made at its annual Hawaii summit back in December. Most importantly, expect infrastructure updates, as the US carriers continue to expand their 5G networks and show off how the fifth-generation of wireless will transform healthcare, “smart” cities, and autonomous vehicles.\\n\\nAI All Day, All Night\\nWe need to talk about AI toothbrushes. Not really, but if there’s anything that might be emblematic of the trend of “Artificial Intelligence” being infused into every product possible, it might just be the internet-connected toothbrush. Why must the miniature scrubbie for your nubs of calcium phosphate include machine learning? I do not know, but there are plenty of products at CES that try to answer that question, and CES 2020 is going to be quite the AI and IoT bonanza.\\n\\nThing is, there are very real, important applications for AI—see aforementioned autonomous vehicles, energy-efficient cities, and advances in healthcare. Big tech companies like Samsung and Google have committed billions of dollars to AI, with the goal of transforming everything from real-time language translation across the globe to your average washing machine at home. Some technologists and analysts note that tech has effectively “disappeared” into our lives, which is to say it is everywhere; AI, which is largely intangible as a physical experience, is a big part of that.\\n\\nBut that also means that companies with less AI expertise will use it to market their wares, whether those are pet collars, lightbulbs, smart toilets, pill packs, gardening tools, or hairbrushes. The idea is that as these products “learn” your habits and get smarter over time, they’ll provide added conveniences. If that’s the actual outcome, then we’re all for it.\\n\\nMicroLED Shines\\nCES wouldn’t be CES without TVs. This year we’re expecting to see a bigger assortment of 8K models, as well as TVs and other displays that flex into a variety of cool form factors—LG has already teased a rollable OLED model that retracts into the ceiling between viewing sessions.\\n\\nHands down though, this year’s most exciting TV trend is MicroLED. It’s similar to the more mature OLED technology, where each pixel on a TV acts as its own own light source, instead of relying on a dedicated backlight. The difference is that MicroLED can potentially get brighter and last longer than OLED technology, which means the picture has even better highlights and deeper contrast. We’ve seen massive MicroLED models like Samsung’s Wall before, but they were primarily proof-of-concept prototypes. This year at CES, we expect to see actual production models with the technology. At least in the short term, it’ll make a bigger difference in picture quality than simply adding more pixels.\\n\\nThe Pleasure Is Mine\\nBy now you might have heard the story: At last year’s CES, a robotic vibrator called the Ose was initially awarded an innovation prize, then was disqualified once the Consumer Technology Association (which puts on the big show) deemed the product “immoral” and “obscene.” Backlash followed, and last summer the CTA released a statement summarizing some policy changes, which included—surprise!—a trial run for sex toys at CES 2020.\\n\\nAll of that buzz (pun absolutely intended) seems to have generated momentum for the woman-led team behind the Ose vibrator. The company says it made more than $1 million within an hour of listing the product for pre-sale in November of 2019. But it also opened the doors for more sex toy and sexual health companies to exhibit at CES this year, whether those are bendable bullet vibrators, kegel exercisers, or “rear gear.” The caveat is that the products have to be “innovative and include new or emerging tech” to qualify. So, OK, we’re looking forward to seeing sex toys served up with AI smarts and a side of blockchain—but really, we mostly look forward to the erosion of taboos around sexual health and sexual pleasure, particularly for women.\\n\\nNew Kinds of Buds\\nApple doesn’t typically exhibit on the CES show floor, but Apple’s presence will be felt this year as every major brand chases the company’s massive AirPods sales numbers. Expect a wider variety of wirefree earbuds than ever before at CES 2020. And it should be a year of great earbuds for travelers, with a number of new noise canceling models designed to compete with Sony and Apple’s current lineup; as well as an increasing push towards affordability. We have a sneaking suspicion that we might see the first legitimately good pair of wireless earbuds below $50.\\n\\nOn the home front, speakers are getting smarter, and not just by adding popular voice assistants. Over the past few years at CES we’ve seen Amazon’s Alexa and Google’s Assistant find their way into a wide variety of products (and some pretty absurd ones), but this year we expect to see a lot of attention paid to other AI-heavy audio tech. This includes smart speakers with the ability to optimize sound to individual rooms for even more immersive listening experiences. And this processing power will likely extend to soundbars, with new models from the likes of Dolby Atmos that offer better object-based audio, and other manufacturers bringing such high-end features to smaller, more affordable bars.\\n\\nScoot On Over This Way\\nOver the years, CES has morphed into something akin to an international auto show, with a dozen different legacy car makers showing off their sleekest vehicles and concept cars. Hundreds of other automotive technology companies also show up to exhibit their in-dash computers, driver assistance tech, or automation systems.\\n\\nBut more recently, factors like climate change, worsening traffic congestion, and significant improvements in battery technology have spurred the popularity of “micromobility” devices: personal transporters and experimental scooters that are changing the way city-dwellers get from here to there. For example, Segway will show a whole host of new electric transporters, the most flashy of which is the S-Pod, a two-wheeled chair that can’t be tipped over and can turn on a dime. Expect more lightweight, ultraportable scooters, compact commuter e-bikes, and high-performance off-road vehicles like those made by the Swedish bike company Cake.\\n\\nLet Me Bend Your Ear\\nElectronics companies will try any number of tricks to make an old idea seem new again–3D TVs! Bluetooth turntables! Celebrity partnerships!–even if that means taking a product and quite literally twisting it on its head. Enter: foldables.\\n\\n\\nTo fold a device is not new; a lot of us spent our formative years snapping flip phones shut, which, technically speaking, folded at the hinge. But companies like Samsung, LG, Lenovo’s Motorola, and others have been hard at work making products with flexible displays—screens that bend without visible hinges or seams, allowing your compact phone unfurl into something that resembles a tablet. In 2019 we saw the first batch of these foldables hit the market, and CES 2020 is likely to be a showcase for even more bendable goods—including laptop-sized computers that fold in half for transport. The technology is still expensive, the resulting gadgets are still fragile, and the concept is still a long way from mainstream adoption. But manufacturers will try their hardest to make the everyday rectangles we carry around with us seem fresh again.\\n\\nI Spy … a Less Private Future\\nWith all of our connected devices and their corresponding apps and platforms that slurp up our data, we humans have developed a real concern about where our personal information ends up. And that’s assuming we’re even fully aware of how it’s being collected in the first place.\\n\\nThe CTA will do its part to make sure that privacy is addressed in some capacity at CES 2020. On Tuesday, January 7, the chief privacy officers from Facebook, Apple, Procter & Gamble and the FTC will convene for a public panel discussion. But keep in mind that comprehensive solutions for privacy (and security) in an IoT world aren’t going to be hammered out in a single roundtable. If the privacy woes of the last decade carry over into a 2020 and beyond, despite new regulations that aim to give consumers control over the information companies collect on them, it’s going to take more than privacy platitudes at a tech-friendly trade show to lock it all down. Because everything is connected now, and there’s no going back.',\n",
              "  array([-3.557872 , -1.5872395, -0.2652538], dtype=float32),\n",
              "  2),\n",
              " ('The man in charge of next week\\'s giant CES tech expo has said that Ivanka Trump has done \"great work\" and will help it focus attendees\\' minds on job-related issues for the future.\\nA decision to offer the US president\\'s daughter one of the show\\'s \"keynote\" sessions has caused controversy.\\nCritics have said other women with more expertise have failed to be granted lower-profile slots at the trade show.\\nBut Gary Shapiro told the BBC that he did not regret the decision.\\n\"There\\'s a lot of focus on jobs of the future, and certainly the keynote that I\\'ll be doing with Ivanka Trump will be focusing on... how industry is working with government on this very important issue,\" he said.\\nHe will interview Ms Trump - who serves as a jobs creation adviser to the president - when she appears on stage on Tuesday 7 January.\\nHe declined, however, to say if her appearance had been his organisation\\'s idea or if the White House had requested the address.\\n\\'Upset\\' and \\'insulted\\'\\nThe US Consumer Technology Association (CTA) confirmed Ms Trump would headline a discussion about \"the path to the future of work\" in a press release shortly before Christmas.\\nOver the following days hundreds of people tweeted objections to the announcement, often using the hashtag #BoycottCES.\\n\"I was far from pleased at the news and the reason is certainly not found in my political beliefs,\" blogged CES regular Carolina Milanesi from the tech consultancy Creative Strategies.\\n\"The reason for my upset is rooted in the fact that there are many more women who are in tech and are entrepreneurs who could run circles around Trump on how technology will impact the future of work.\"\\nRachel Sklar, an entrepreneur who has campaigned for women to have more opportunities in the tech industry, added: \"What an insult to the years and years of protesting how few women were invited to keynote... there are so many great, qualified women. Shame.\"\\nThe BBC sought comment from several of CES\\'s other featured speakers.\\nOnly Mark Cuban, the entrepreneur and star of US TV series Shark Tank replied.\\n\"I don\\'t care one way or another,\" he said.\\n\"No one has to go to her keynote if they don\\'t want to.\"\\nFor his part, Mr Shapiro suggested that Ms Trump had the credentials to warrant the honour.\\nIn total, CES is scheduled to host eight keynote-branded events this year featuring external speakers. Nine of the speakers and moderators are men and eight are women, excluding CTA staff.\\nOne session features both the US Secretary of Transportation Elaine Chao and President Trump\\'s top technology advisor Michael Kratsios.\\nCES 2020: Preview of tomorrow\\'s tech on show in Las Vegas\\nMirriad\\'s AI slips ads into empty spaces in online videos\\nHow AI fitness apps now rival personal trainers\\nWhat he said\\n\\nMore than 175,000 people are expected to attend this year\\'s CES tech expo, which is open to industry insiders but not the public\\nThe transcript below of the interview with Mr Shapiro has been edited for clarity and length:\\nHow did the invitation come about?\\nThere is a concern in the US and elsewhere about jobs. And working with the White House, this jobs-focus is something we\\'re starting to talk about in a very deep way.\\nThere\\'s an obligation by our industry to say we need people that are trained as data scientists and in artificial intelligence. There\\'s a huge shortage in the United States of these type of workers. And there\\'s concern about displacement and jobs.\\nThe White House - through great work by Ivanka Trump and [Secretary of Commerce] Wilbur Ross and others - has basically gotten industry together and said: \\'Look, we have to approach this with a national strategy\\'.\\nAnd part of that strategy is that companies that are involved in technology have to stand up and reskill workers.\\nWe are, in a sense, in an economic battle with other countries, specifically China, over artificial intelligence, self driving, 5G, and other areas where we need skilled workers.\\nChina is producing a million scientists and engineers a year, and we need our own strategy. And that is where these type of discussions will be occurring at CES.\\nPeople are curious though, was it the case that your organisation invited Ivanka Trump? Or did the White House ask if she could appear?\\nWe\\'ve gone out to every administration - we\\'ve had Democrats and Republicans - and we invite them. Sometimes it\\'s a discussion and a dialogue.\\nIn terms of this one, I\\'m not going to go into the details, but we are so excited to have them because it\\'s very important that these issues be discussed in a meaningful way.\\nThere\\'s has been quite a backlash, with questions posed about whether Ivanka Trump deserves the honour and what she could possibly say that her audience doesn\\'t already know. Why do you think she warrants the appearance rather than somebody else who\\'s had more to do with technology in the administration?\\nOne of the big things we did not focus on enough a couple of years ago is jobs. Now, last year, we really went into it big time with apprenticeships and other things.\\nIvanka Trump actually co-chairs the American Workforce Policy Advisory Board, whose members include companies like Apple, Walmart and IBM.\\nShe also co-chairs the National Council for the American Worker, which has received pledges for more than 300 companies, to create over 12 million new training opportunities for students and workers over the next five years.\\nShe also championed the continuation of the Obama-created Global Entrepreneurship Summit.\\nShe\\'s been involved in criminal justice reform. We just got a bill signed into law two weeks ago, which gives governmental employees paid parental leave.\\nThere\\'s been a whole range of issues - substantively, definitely a focus there on workforce issues.\\nAnd that is part of the CES discussion.\\nYou\\'ve previously criticised Ivanka Trump\\'s father, Donald Trump for his behaviour before he became President, and then his trade tariffs after taking power. So might we expect you to ask Ms Trump some tough questions when you interview her?\\nOur focus at this point is not to make this a political event, but to focus on the fact that this is about jobs in the future.\\nCES has about 1,300 speakers. There will be a range of policy discussion.\\nOur focus is generally on issues that are important to innovation.\\nFollow the BBC team covering CES via this Twitter list',\n",
              "  array([-0.71508485, -0.7183128 , -3.7603679 ], dtype=float32),\n",
              "  0),\n",
              " ('What we can do to make sure automation doesn’t negatively affect the work force\\nby AIMEE PEARCY — 16 days ago in SYNDICATION\\n51\\nSHARES\\nArtificial intelligence and automation are continuing to drill deeper into our society.\\nIn the business industry, companies are now utilizing the voice recognition provided by intelligent personal assistants such as Alexa, Cortana, and Siri to speed up their tasks.\\nIn the transport industry, artificial intelligence is powering the upcoming self-driving cars and helps to manage the flow of traffic.\\nIn the education sector, artificial intelligence is used to support personalized learning systems.\\nIn healthcare, new diagnostic tools and decision-support technologies are being powered by artificial intelligence.\\nIn the retail sector, artificial intelligence is improving the design of warehouse facilities to make the process more efficient.\\nIn the film industry, artificial intelligence is being used to compose orchestral music and generate short pieces of film\\nIn the humanitarian sector, artificial intelligence is being used to support the delivery of the UN’s Sustainable Development Goals.\\nYet, there seems to be a disconnect between the people designing and implementing these systems, and those who will be most affected by the outcome.\\nVolume 0%\\nThe reported median annual salary for an AI programmer in the UK in 2019 is currently around £60,000. Meanwhile, the reported median annual salary for all workers in the UK is reportedly around £36,611.\\nThe result of widespread automation\\nBirds are cool\\nEarly birds are even cooler. Get cheap tickets to TNW2020 right now\\nYEAH\\nAutomating routine operations presents a lot of benefits. It presents the opportunity for people to move on from repetitive tasks to more rewarding, challenging work that allows them to engage their emotional intelligence.\\nBut currently, this is far from the case. Instead, low skilled workers are finding themselves being continually downgraded into increasingly insecure, low-paid roles. For some people, their jobs have been completely replaced.\\nIn 2013, researchers at Oxford University studied 702 occupational groupings. They discovered that 47% of US workers have a high probability of seeing their jobs automated over the next 20 years. More recently in 2017, a McKinsey report predicted that 30% of ‘work activities’ would be automated by 2030 — a change that is set to affect up to 375 million workers worldwide. That’s a significant number of people.\\nThroughout history, new waves of technological innovation have always led to a spike in public debates regarding automation. The movement is comparable to the shift away from agricultural societies throughout the Industrial Revolution. Evidence from this can give us some insights to inform policy debates today.\\nPeople have been worrying that automation would leave humans without work as far back as the 20th century. In 1950, John F. Kennedy described automation as a ‘problem’ that would result in ‘hardship’ for humans.\\n15 years later in 1965, an IBM economist said automation would result in a 20-hour workweek. Considering the average American still works an average of 34.4 hours per week, this prediction was clearly quite a way off.\\nBut it took decades to tackle the injustices of the Industrial Revolution. This time, we can’t afford to wait that long.\\nIf employment levels fall significantly enough, there is a fear that Western democracies could resort to authoritarianism, which spread in some countries back in the 1930s following the Great Depression, and as is the case in many countries today that have experienced high levels of income inequality.\\nThe real challenge is managing the transition\\nWestern politics is already becoming increasingly turbulent. Income inequality is slowly beginning to grow even further, contributing to the already shaky political instability. A large proportion of the population will need to retrain for new careers, and they won’t be young — they’ll be middle-aged professionals. Developed economies are likely to be hit hardest by the transition, as increased wage averages increase the incentivization for automation even further.\\nAutomation will vary widely, depending on the industry sector. Jobs in industries such as health care are set to increase to cope with an aging population, while jobs involving manual labor and data processing are set to decline.\\nIt’s impossible to know exactly how many jobs will be affected by AI, as studies give wildly different estimates, depending on the treatment of the input data.\\nA report by PwC suggests there will be three major waves of automation.\\nSource\\nWave 1 will occur in the early 2020s and is expected to displace a very low proportion of jobs — around 3%.\\nWave 2 is expected to arrive in the late 2020s and is expected to displace many jobs in the clerical and administrative sector.\\nWave 3 is expected to arrive in the mid-2030s and could result in the automation of up to 30% of today’s jobs — particularly those that involve automotive equipment and machines.\\nWorkers with lower education levels are likely to be much more vulnerable to being replaced by machines:\\nSource\\nHow can we tackle it?\\nThe McKinsey report used America’s transition away from agriculture during the Industrial Revolution as an example. With the decrease in farming jobs came a significant increase in spending on secondary education and new laws that made attendance compulsory.\\nIn 1910, 18 percent of children between 14 and 17 years of age went to high school. By 1940, 73 percent of children between 14 and 17 years of age went to high school. This increase in education helped to create a booming manufacturing industry.\\nIf we want the future of automation to be successful, it is clear that a similar push is needed today. It’s becoming increasingly clear that AI will not result in the ‘end of work.’ It could create as many jobs as it gets rid of. Instead, the jobs of the future will merely require a different skillset.\\nGovernment advice networks need to support more businesses to use machine learning. We need to build skills at all levels —from schools to industry professionals, to undergraduate and postgraduate students.\\nUnfortunately, this doesn’t seem to be the case. In fact, in the last few decades, spending on training and supporting the labor force has been in decline. In addition, many schools are still failing to teach the key concepts of technology to their students.\\nAutomation doesn’t have to be a disaster — but it will be if politicians don’t understand our need for change\\nIf we want technology to benefit everyone instead of further widening inequality, we need to start training our workforce for the future immediately. Inaction will result in even greater division and polarization between communities.\\nPoliticians, trade unions and business leaders need to act now if they want to make sure the result of technological change is good.',\n",
              "  array([-4.091452  , -4.090601  , -0.03401617], dtype=float32),\n",
              "  2),\n",
              " ('The US will impose new restrictions on the export of certain AI programs overseas, including to rival China.\\nThe ban, which comes into force on Monday, is the first to be applied under a 2018 law known as the Export Control Reform Act or ECRA. This requires the government to examine how it can restrict the export of “emerging” technologies “essential to the national security of the United States” — including AI. News of the ban was first reported by Reuters.\\nTHE BAN IS EXTREMELY NARROW — A RELIEF FOR THE AI INDUSTRY\\nWhen ECRA was announced in 2018, some in the tech industry feared it would harm the field of artificial intelligence, which benefits greatly from the exchange of research and commercial programs across borders. Although the US is generally considered to be the world leader in AI, China is a strong second place and gaining fast.\\nBut the new export ban is extremely narrow. It applies only to software that uses neural networks (a key component in machine learning) to discover “points of interest” in geospatial imagery; things like houses or vehicles. The ruling, posted by the Bureau of Industry and Security, notes that the restriction only applies to software with a graphical user interface — a feature that makes programs easier for non-technical users to operate.\\nReuters reports that companies will have to apply for licenses to export such software apart from when it is being sold to Canada.\\nThe US has previously imposed other trade restrictions affecting the AI world, including a ban on American firms from doing business with Chinese companies that produce software and hardware that powers AI surveillance.\\nAI-powered software, like this program from Descartes Labs, can automatically tag objects and areas of interest. Credit: Descartes Labs\\nUsing machine learning to process geospatial imagery is an extremely common practice. Satellites that photograph the Earth from space produce huge amounts of data, which machine learning can quickly sort to flag interesting images for human overseers.\\nSuch programs are useful to many customers. Environmentalists can use the technology to monitor the spread of wildfires, for example, while financial analysts can use it to track the movements of cargo ships out of a port, creating a proxy metric for trading volume.\\nBut such software is of growing importance to military intelligence, too. The US, for example, is developing an AI analysis tool named Sentinel, which is supposed to highlight “anomalies” in satellite imagery. It might flag troop and missile movements, for example, or suggest areas that human analysts should examine in detail.\\nRegardless of the importance of this software it’s unlikely an export ban will have much of an effect on China or other rivals’ development of these tools. Although certain programs may be restricted, it’s often the case that the underlying research is freely available online, allowing engineers to recreate any software for themselves.\\nReuters notes that although the restriction will only affect US exports, American authorities could try and encourage other countries to follow suit, as they have with restrictions on Huawei’s 5G technology. Future export bans could also affect more types of AI software.',\n",
              "  array([-0.31775945, -1.4095452 , -3.5767002 ], dtype=float32),\n",
              "  0),\n",
              " ('So, here we are on the eve of CES 2020 — the supersized buffet of an annual consumer electronics show in Las Vegas, where we not only get a sneak peek of what to expect from tech companies this year, but also to take the pulse of how people are responding to what’s out there.\\nCES is all about “the future,” and we’ll be here this week covering all the big stories and themes. But what about the past? In the spirit of 2020 hindsight, here are some of the most notable headlines and trends of last 10 years of CES.\\nTake a look and let us know what you think have been the biggest themes coming out of the event in the comments below.\\nCES 2010\\nPalm’s ‘turnaround’. The mobile upstart launched its first smartphones, the Pre and Pixel, in 2009, and found a hardcore group of fans that loved the look of the devices, and all the features that set it apart from Android and iOS. 2010 was about gaining momentum. Palm CEO Jon Rubenstein (one of the early pioneers at Apple making the iPod) made some waves onstage by claiming that he had never used an iPhone. Bullish talk that helped cement the company’s independent image. Palm also saw its stock rise by 10% after it announced at a press conference that it would sell its devices through a lucrative deal with Verizon Wireless (which now owns TechCrunch). (All proved to be short-lived, and many lamented that Palm was probably too ahead of its time.)\\nMeanwhile, the battle between iOS and Android raged on. In 2010 the story was about how AT&T was finally adding its first Android-based smartphones to its lineup (remember that it was the exclusive carrier of the iPhone as its first foray into the new generation of touchscreen smartphones). Motorola “Backflip” smartphones, and tablets and smartbooks from Dell, were among the other mobile devices announced that year.\\n‘Natural User Interface.’ Here’s a prescient article by then-CEO of Microsoft Steve Ballmer written on the heels of CES about the new ways we will be interacting with devices in the way ahead, covering touch, gesture and voice, and very dependent on the cloud. He gave a specific nod to Project Natal, which Microsoft showed off at CES and eventually became Microsoft’s Kinect gesture technology. Ballmer was absolutely on the money, although I’d be interested to know if he suspected just how big of a role his neighbor Amazon would play in that new era.\\nOther themes in the year included the usual boost in TV technology, this time around 3D; and some early signals of a smart car future with news from the likes of Nvidia and Ford.\\nCES 2011\\nRealNetworks and cloud-based music. This was absolutely the direction music would be going. RealNetworks would not ultimately be the top dog in this game, but back in 2011 it was the one leading the charge with a service called Unifi, lauded at the time for being first to market before Apple and Google — and Spotify — in presenting a way to merge what you buy online with what you might already own in terms of digital files. Alas, being an early mover does not always pay off, a theme of its own. Real Unifi quietly died a death and its URL is particularly iffy now.\\n4G. 2011 was also the year of LTE and 4G announcements from many carriers, from T-Mobile announcing sales of 900,000 4G handsets, to HTC unveiling its first 4G device.\\nThe year of many phones (and specifically phone brands) that did not stand the test of time. Sister site Engadget (a monster when it comes to comprehensive CES coverage) highlighted its list of the best tablets and smartphones, a selection including Motorola, Notion Ink, BlackBerry, and Vizio — a veritable graveyard of brands. It was also the year of Android tablets set up for the future using Honeycomb… another dud, as it turned out.\\nVroom vroom. We might take for granted that cars are a major component of CES today — after all, they are essentially very large, mobile pieces of hardware — but that wasn’t always the case (not least because the mammoth Detroit Auto Show is just around the corner in the convention calendar). In 2011, Ford unveiled its first foray into electric cars at the show, setting up a decade of major car advances getting launched at CES.\\nCES 2012\\nRemember when Nokia was the world’s biggest mobile handset maker? This was the year that it made some critical shifts in its downturn. CES 2012 was the event where the company unveiled its first Windows Phone-powered smartphone for the US market, to ship exclusively via AT&T.  This proved to be a step along the road to Microsoft buying Nokia’s handset business outright, an ill-fated move that ultimately left both brands in a ditch as far as that market was concerned. But in 2012, there was still a lot of hope and enthusiasm for both.\\nMore smart TV advances: Today we take integrated services and very light hardware like Fire TV sticks and Chrome Casts for granted. That was not the story in 2012, though, where a Samsung TV that integrated DirectTV without a box made headlines.\\nSteve Ballmer gave his last Microsoft keynote at CES (because Microsoft pulled out of the show keynote roster after that year) and announced a date for the Kinect (some two years after highlighting gesture as something that was going to be coming at us fast).\\nCES 2013\\nHoppin’ mad! This was the year that big media took a bite out of streaming media. CNET, which had been running ‘best of’ awards on behalf of CES for years, was asked to remove the ‘best of show’ award that had been given to Dish’s Hopper with Sling (which let you watch programs recorded on your Dish DVR on your iPad) after the legal team of its parent, CBS, intervened. The interference was nefarious: CBS was embroiled in a lawsuit with Dish, tainting the whole business of then bestowing the award upon Razer. The whole thing was uncovered, criticized even by CNET itself. Then, Dish was awarded Best of Show some weeks later. As with so much else in the question about the best way to move content from one device to another, the answer was coming from somewhere else altogether: the cloud — that is to say, streaming services have trumped both the Dish Hopper and whatever else was competing against it.\\nNvidia Shield. A big leap for Nvidia, the company that had already made its name with graphics and other processors used in high-performance computing devices and connected cars for gaming, artificial intelligence applications and more. 2013 was the year it revealed its own hardware in the form of a gaming device called Project Shield, powered by its newest processors. Shield has continued to grow over the years.\\nOculus Rift. It was still going to be another year and a bit before Oculus got snapped up by Facebook for $2 billion, and the Rift virtual reality headset didn’t actually launch at CES, but this was a kind of coming-out party for it nonetheless, since it was the first time that the company had set up a stand to provide demos to any and all comers. The result was a lot of buzzy exposure (reported on by multiple outlets) to build up interest in the device. This in itself was a stage two media strategy: The company had launched on Kickstarter with a lot of fanfare prior to this — another notable trend for CES this year, where another crowdfunded blockbuster, the Pebble Smartwatch, exhibited for the first time, too.\\nCES 2014\\nWearing thin… Wearable tech was everywhere at CES this year and definitely became a bigger theme as the year went on. Indeed Google did the wide launch of Google Glass as a consumer product just a few months later, although it took Apple until April 2015 to launch its first Watch. (That Watch, incidentally, has been one of the most successful of all wearable efforts…. There’s that late mover advantage again). Many voices were already crying foul: Just because you can build something to put on your wrist, or head, or finger, or on your jacket, or wherever… does that mean you necessarily should? And if you do, will anyone want to buy it, or will it collapse into history books as a novelty?\\nHonk honk. After a couple of fallow years where cars were more certainly present but not previewing blockbuster changes, 2014 was the year they changed gears. Google announced the Open Automotive Alliance, including partnerships with GM, Audi and Honda, Hyundai and Nvidia for Android-powered in-car systems.\\nNetflix and streaming. The streaming wars really didn’t kick off until the year after this, but for a little taster of how OTT services like Netflix’s would soon dominate the conversation about video at CES and elsewhere: this was the year that Netflix CEO Reed Hastings took to the stage to announce an exclusive 4K (high resolution) version of hit series House of Cards that would stream on 4K-capable LG TVs.\\nVR expansion. Oculus (still yet to be acquired by Facebook but easily emerging as a power player in the VR space) released a new version of its headset. Sony followed suit and others like Meta, working in the area of VR meeting augmented reality, also showed off their new kit. The momentum wouldn’t last: Meta and several more efforts are now defunct, others are struggling, and many are wondering what the mainstream market for VR and AR headsets will be longer term. (We’re still waiting to see whether Apple will launch a device here, too.)\\nCES 2015\\n https://www.mercedes-benz.com/en/mercedes-benz/innovation/research-vehicle-f-015-luxury-in-motion/\\nSmart Home gets smarter. Google still hadn’t closed its deal to acquire Nest when CES 2015 rolled around, but the latter company’s momentum spoke a lot to why it got snapped up and integrated and continues to be a central part of the company’s home strategy. It announced a number of new partners at the show through its “works with Nest” program.\\nMeanwhile, Apple — long without an official presence at the show — continued to make itself a part of the conversation, specifically in the area of the connected home. The ecosystem for Apple’s HomeKit platform for managing connected smart devices by way of your iPhone grew by some way during CES with the announcement of a number of new partners.\\nRobots and spycams and drones, oh my! The rise of improvements in AI-based technology such as computer vision, voice recognition and more led to a wave of devices aimed at helping people see and do our bidding, and ultimately than they’d be able to on their own. This bigger trend expanded to creepy humanoids, as well as camera-equipped drones and security devices. The huge swing we’ve seen toward an awareness of our privacy in recent times was not so apparent in 2015, so I wonder how this theme would be handled today.\\nMercedes Benz F 015 – concept autonomous car. This is just one vehicle — literally speaking. It’s a prototype that may never see the light of day on a production line, at least not for a long time. But I’m highlighting it here as a sign of the times. 2015 saw yet more vehicles, yet more automonous demos, and yet more moves from a range of players to plant their flags in the self-driving market. We’re still far from seeing any wide-scale commercial products or services for a number of reasons, but the amount of investment in this area continues to grow, with the hope and expectation that the tech, the consumer appetite, and regulations will all align in its favor. Other big news in automotive included Nvidia’s new car platforms.\\nPlus, too many selfie sticks.\\nCES 2016\\nComing to a screen near you. Netflix has both spearheaded and in many ways led the focus on streamed video in the world of entertainment, and this year it used the show as a platform to announce a huge shift: It was expanding its service to 130 countries, a massive international push. While there are a lot of localized  players, and I’d argue that Netflix doesn’t have quite the right mix of local and general content abroad as it does in its home market (where the catalog is genuinely far bigger), this was a huge move that continues to set Netflix up for more growth in the future, perhaps via consolidation.\\nAs automotive tech continued to be a major theme at CES, there was a new twist this year. One of the huge automotive giants, GM, used the event not just to unveil a new electric car model, but also a $500 million investment in one of the on-demand transportation giants, Lyft. The rationale: to tap into what might be a new market to supply with its future autonomous vehicles, and to get a stake in one of the bigger startups transforming how people get from A to B (which will ultimately have an impact on how, and if, they buy cars).\\nCES 2017\\n\\nAlexa, Everywhere. While the Echo was launched back in 2015, and we saw the beginnings of Alexa-integrated devices and Echo-based hardware the year before by way of Amazon’s platform plays, it was really in 2017 that this trend became nearly all-pervasive, appearing not just in speakers made by third parties, but refrigerators and more. Given the strong recurring theme of home gadgets at the show, this has made Amazon a big presence at the huge event. It still raises the question, though, of how successful any of these integrations have been. We may have a lot of “connected” devices in our homes, including smart speakers, but how much are we really using them?\\nCES 2018\\nGoogle assistant, Everywhere. Not to be outdone by Amazon or Apple in years before, 2018 was the year that Google permeated CES with its own voice — or more specifically its voice bot, the Google Assistant. In addition to blanketing trains, billboards and more with Google slogans, slides and gumball machines, the company’s name and Assistant were brought up in connection with most of the event’s biggest hardware and software splashes, by way of integrations. It’s not a guarantee that this will actually get people to use it, but having the ubiquity makes it ever more convenient for those who do want to go that route. Will be interesting to see how and if this continues as a theme in 2020 and beyond. My guess is that simple messages of “it’s there” will not be enough to hold interest longer term.\\nCES 2019\\nLAS VEGAS, NEVADA – JANUARY 08: An attendee walks by the Huawei booth at CES 2019 at the Las Vegas Convention Center on January 8, 2019 in Las Vegas, Nevada. CES, the world’s largest annual consumer technology trade show, runs through January 11 and features about 4,500 exhibitors showing off their latest products and services to more than 180,000 attendees. (Photo by David Becker/Getty Images)\\nBull in a China shop. The theme of how Chinese companies grow and operate in the US and other Western markets — two major vectors that are being buffeted by large political questions around tariffs and national security — has been a big one in the tech world for a while now. It was interesting to see it play out at CES last year. The event benefits from a massive amount of visitors from China, and also exhibitors, and so many eyes were on both. It played out in some big ways: Huawei downplayed its presence, and ZTE didn’t come at all. And Gary Shapiro, head of the Consumer Technology Association, which organizes CES, came out on the side of doing business with China, criticizing President Trump’s strategy. Given that Trump’s daughter Ivanka will be a keynote speaker this year, all will be watching how and if this theme will continue to develop or simply get swept under the rug.\\nOther themes last year included more advanced media streaming that moved further away from being tied to large and expensive middleware (the only expensive hardware that matters being the big TV screens), and a kind of truce in the voice assistant matrix: support for multiple assistants on single devices.',\n",
              "  array([-3.8583899 , -2.357808  , -0.12299208], dtype=float32),\n",
              "  2),\n",
              " ('CES means there’s no shortage of “smart” connected gadgets, and toothbrushes are no exception. Oral-B and Colgate are launching new smart toothbrushes this year: the Oral-B iO and the Colgate Plaqless Pro Toothbrush.\\nOral-B has released several smart toothbrushes before, like the $220 Oral-B Genius X, which used sensor data and an “AI algorithm” to give suggestions on where to focus on brushing. The Oral-B iO also claims it has artificial intelligence and algorithms to “facilitate achieving perfect brushing results in all 16 zones for superior oral health,” which is just marketing nonsense using AI as a buzzword. This time, the iO has added a new “magnetic drive,” which is responsible for the microvibrations of the bristles. The iO has seven different brushing modes, and it connects to the mobile Oral-B app via Bluetooth to give users feedback on their brushing performance. It’s got 12.5 days of battery life, takes three hours to charge fully, and operates at 145Hz movement speed.\\nThe Oral-B iO and app\\nColgate released its first E1 smart toothbrush in 2018, which was (surprisingly) an Apple Store exclusive. Its new smart toothbrush will detect buildup — presumably plaque and other bacterial films that stick to teeth — and it claims it will help improve user brushing by “providing precise information in real time that is specific to each mouth.” It’s not clear how, exactly, the brush senses that buildup or how that information will be used to make you brush better.\\nThe Colgate Plaqless Pro\\nThere’s no pricing or release information for either toothbrush yet, so you can hold on to those free toothbrushes you get from the dentist for now.',\n",
              "  array([-4.838077  , -4.5607433 , -0.01854755], dtype=float32),\n",
              "  2),\n",
              " ('WASHINGTON (AP) — U.S. technology companies that build artificial intelligence software for analyzing satellite imagery will face new restrictions on exporting their products to China and elsewhere.\\nThe Commerce Department said new export rules take effect Monday that target emerging technology that could give the U.S. a significant military or intelligence advantage. A special license would be required to sell software outside the U.S. that can automatically scan aerial images to identify objects of interest, such as vehicles or houses.\\nThe rules could affect a growing sector of the tech industry using algorithms to analyze satellite images of crops, trade patterns and other changes affecting the economy or environment.\\nThe new export rules are the result of Congress passing a law in 2018 that updated national security-related export controls to protect “emerging and foundational” technology that could end up in the hands of foreign governments.\\nADVERTISING\\nAds by Teads\\nIt’s an interim rule until the public has a chance to weigh in before March, but the Commerce Department says it is in the national security interests of the U.S. to immediately implement the controls Monday.\\nCONTACT US AT EDITORS@TIME.COM.',\n",
              "  array([-2.094142  , -2.1404712 , -0.27545747], dtype=float32),\n",
              "  2),\n",
              " (\"An app on show at the CES tech expo uses artificial intelligence techniques to let its users appear within video games and interactive stories.\\nWonder Painter is the creation of a former University of Cambridge academic who now runs a start-up in Beijing.\\nLG has already adopted its tech in some of its products, but Xiang Cao hopes to bring the experience to a wider audience, as he explained to the BBC's Cody Godwin in Las Vegas.\\n\",\n",
              "  array([-4.876912  , -4.2957644 , -0.02147563], dtype=float32),\n",
              "  2),\n",
              " ('HONG KONG (Reuters) - Chinese artificial intelligence (AI) company Megvii’s plans for a $500 million Hong Kong initial public offering (IPO) are back on track after its application was cleared by the city’s stock exchange, two sources with direct knowledge of the matter said.\\nMegvii - which was blacklisted by the U.S. administration in October - was asked to provide more information in November when the company faced the Hong Kong Stock Exchange listing committee to seek the go-ahead for the transaction.\\nMegvii, which is known for its facial recognition platform Face++, is aiming to be the first Chinese AI firm to go public.\\nThe plan comes after the Trump administration in October placed Megvii on a banned trade list with seven other Chinese firms for their alleged involvement in human rights violations related to Beijing’s treatment of Muslim minority populations in the Xinjiang Uighur Autonomous Region.\\nThe company said at the time it “strongly objected” to the blacklisting.\\nMegvii is due to file updated listing information shortly with the Hong Kong exchange, according to sources who declined to be named because the information has not been made public.\\nCitigroup , Goldman Sachs and JPMorgan are working on the updated listing and the three banks declined to comment. Megvii and a Hong Kong Stock Exchange spokesman also declined to comment.\\nMegvii filed its listing application on Aug. 25 and under the Hong Kong Stock Exchange rules has six months to list from that date.\\nHowever, it can apply for a three month extension once that deadline has passed, according to stock exchange rules.\\nA time table for the deal has yet to be put in place and sources said the company would wait until after the Chinese New Year holidays, which start on Jan. 25, when financial markets start to rev up for the year.\\nHong Kong is entering a seventh month of pro-democracy protests and the listing of a Chinese facial recognition company is expected to be closely watched by local activists.\\nMegvii was founded in 2011 by Chief Executive Yin Qi and two friends from Tsinghua University. The company provides AI technology to governments and companies including Alibaba, Ant Financial and Huawei.\\nThe company raised $750 million in May, which valued it at slightly over $4 billion, and attracted investors including Australian investment bank Macquarie Group and the Abu Dhabi Investment Authority.',\n",
              "  array([-3.2266994, -0.8158934, -0.6576495], dtype=float32),\n",
              "  2),\n",
              " ('In December 2018, Canada and France announced plans for a new international body to study and steer the effects of artificial intelligence on the world’s people and economies.\\n\\nCanadian prime minister Justin Trudeau said the International Panel on Artificial Intelligence would be established by the Group of Seven leading western economies and play a role in “addressing some of the ethical concerns we will face in this area.” It was to be modeled on the UN’s Intergovernmental Panel on Climate Change, which helped establish consensus on the world’s climate crisis and recommends possible responses.\\n\\nJust over a year later, the IPAI has been renamed the Global Partnership on AI, but it still hasn’t quite gotten off the ground. Six of the G7 are on board—with the United States the lone holdout.\\n\\nProponents of the idea say it will help governments get up to speed on AI developments and could build international consensus on limiting certain uses of the technology, such as AI projects designed to control citizens or infringe human rights. The White House says the body is unnecessary bureaucracy that threatens to dampen AI development by being overly cautious.\\n\\nWhen the project now known as the Global Partnership was revealed in 2018, Canada held the G7’s rotating presidency. France took over in 2019 and kept pushing the project. It said in May that the G7—excepting the US—as well as the EU, India, and New Zealand were interested and would discuss the design of the new organization. In 2020, the G7 presidency is held by the US.\\n\\nCédric O, France’s digital affairs minister, raised the question of the Global Partnership’s future in Washington last month with US chief technology officer Michael Kratsios. In a later interview with WIRED, O said “there is a common consensus but for one country.”\\n\\nKeep Reading\\nillustration of a head\\nThe latest on artificial intelligence, from machine learning to computer vision and more\\nO claims to be sympathetic to US concerns the project might slow America’s tech companies, but he warns that without international coordination, unsavory uses of AI could flourish. He notes how China has used facial recognition and other technologies to strengthen its authoritarian security apparatus—prompting US protests and sanctions on Chinese AI companies. “If you don’t want a Chinese model in western countries, for instance to use AI to control your population, then you need to set up some rules that must be common,” O says.\\n\\nBecause advice or principles espoused by the Global Partnership wouldn’t be legally binding, it’s unclear how much the body really could constrain nations’ AI programs. It would also lack the means to regulate private companies. Lynne Parker, US deputy chief technology officer, says the US still worries the group would be too restrictive.\\n\\n“Our concerns are that the group could be too heavy-handed,” she says. “We believe it’s unethical to hamper and squash down the development of AI technology to the point where you don’t want to use it.”\\n\\nParker also says the Global Partnership looks set to duplicate work already underway at the Organization for Economic Cooperation and Development, a separate group of 36 advanced economies. The OECD has established a network of AI experts to advise members on policy, and it has produced a set of AI principles endorsed by more than 40 countries, including the US, that aren’t too restrictive, she says.\\n\\nFrance and Canada say the latest plans for the Global Partnership would make it partially supported by the OECD and designed to complement that organization’s other AI programs. Hans Parmar, a spokesperson for Canada’s Department of Innovation, Science, and Economic Development, said the six members of the G7 besides the US, and several other interested countries, are now holding biweekly meetings and aim to launch the Global Partnership in “early 2020.”',\n",
              "  array([-0.8792486, -1.2011131, -1.2586184], dtype=float32),\n",
              "  0),\n",
              " ('How AI is preventing email phishing attacks\\nby EYAL BENISHTI — in SYNDICATION\\n12\\nSHARES\\nSince its invention in 1970, email has undergone very little changes. Its ease of use has made it the most common method of business communication, used by 3.7 billion users worldwide. Simultaneously, it has become the most targeted intrusion point for cybercriminals, with devastating outcomes.\\nWhen initially envisioned, email was built for connectivity. Network communication was in its early days, and merely creating a digital alternative for mailboxes was revolutionary and difficult enough. Today, however, it is naively easy to spoof an email and impersonate others. Last year, 70 percent of organizations reported they had become victims of advanced phishing attacks. There are 56 million phishing emails sent every day, and it takes 82 seconds on average for a phishing campaign to hook their first victim.\\nEarly birds get all the good stuff\\nLike a great price on TNW2020 tickets\\nGIMME\\nThe problem is not lack of awareness, but rather misinformation. Many propose the DMARC email authentication protocol (short for “Domain-based Message Authentication, Reporting, and Conformance) as the solution to prevent identity spoofing. However, DMARC only works if both sides of the communication enforce it, making it only effective for inter-organization communications. This means a celebrity or known source cannot use it to prevent a hacker from impersonating them and contacting their clients. Hackers are aware of DMARC’s limitations, and while law enforcement bodies are still trying to catch up, hackers have already moved beyond SEG and DMARC, making the protocols only function as a false sense of security.\\nHuman operators are not enough either. Hackers use Distributed Spam Distraction and polymorphic attacks to stress out operators and cloak their malicious actions. By defeating rule-based email verification systems, the agents are forced to spend tremendous amounts of time on each individual email. Clearly, some things fall through the cracks, as we witnessed with the case of John Podesta the chairman of Hillary Clinton’s presidential campaign in 2016.\\nWhat makes things worse is a recently popularized attack, called Visual Similarity attacks. Criminals create fake login pages that look identical to legitimate websites, for instance, a Gmail login page, and trick their victims into entering their credentials in these intermediate locations. This attack has tricked both human operators as well as email protection tools; humans because of the similarities, and mechanized tools because these fake pages usually live in domains with short lives and no prior history of criminal activity.\\nSo far, only artificial intelligence has been able to cope with all the above attack scenarios.\\nHow AI rewrites the rules of email protection\\nPhishing attacks are much like application-based zero-day hacks. In application-based zero-day attacks, hackers discover and exploit an unknown vulnerability in a specific application to infiltrate a system. Emails are used with various apps, but in this case, the targets are the users who are manipulated into revealing their passwords or downloading malware in a way that was never seen before. Since email users have varying levels of cyber-security knowledge, many inbox protection tools try to prevent malicious emails from reaching those users in the first place.\\nHowever, hackers are extremely creative, and as much as 25 percent of phishing emails bypass traditional Secure Email Gateways. For that reason, we need a tool that fights phishing where it is most effective: inside the mailbox.\\nAI has the capability to go beyond signature detection and dynamically self-learn mailbox, and communication habits. Thus, the system can automatically detect any anomalies based on both email data and metadata, leading to improved trust and authentication of email communications.\\nAnything predictable will be automated by AI, leaving the human worker to handle exceptional situations.\\nAI also can move beyond detecting blacklisted URLs. Using computer vision, the system can scan inbound links in real-time, and detect visual indications to determine whether or not a login page is fake, automatically blocking access to verified malicious URLs.\\nAnother advantage of AI is its ability to scan disparate systems and detect patterns. Currently, cybersecurity tools such as SEG, spam filters, anti-malware, and incident response tools operate in silos, which creates a gap that hackers exploit.\\nIt’s important to stress that AI should never be conceived as a silver bullet. Technology alone cannot stop all threats, but it can reduce the noise so human operators can make informed decisions faster. A system is only complete if it can efficiently involve humans in the loop. These operators can make the system smarter by detecting edge cases, from which the AI learns. Simultaneously, AI’s learning capabilities spares the operators from repeatedly dealing with similar incidents.\\nA complete AI protection system should also make it easy to empower employees with email protection toolsand make reporting suspicious cases easy. A company’s employees can sometimes be its last line of defense, as a security system is only as strong as its weakest link. By creating a democratized system for incident reporting and resolving, we can share incidents across organizations. The AI can be trained against this crowd-sourced professional community, enabling it to predict and prevent incidents in all organizations as soon as one organization has detected an attack.\\nSuch system can defeat phishing attacks at scale. Many hackers go with a “spray and pray” attack, mass-mailing victims and hoping for someone to fall into the trap. A decentralized incident repository could gather information from many different sources and make it available to other organizations instantly, making sure the entire system becomes immune to the attack as soon as the first case is detected. And with the AI being trained on the same repository, deviations and polymorphic attacks can be automatically detected. As AI detects patterns instead of hard-wired signatures, hackers find it extremely difficult to disguise their operations.\\nSaving private email\\nWe send 269 billion messages on average every day, and the era of social media and instant messaging apps have not replaced the mailbox. Email’s strength lies in its simplicity, and the ability to connect to perfect strangers. This strength is also an email’s greatest weakness when it comes to cybersecurity. As hackers have advanced their tools to orchestrate attacks, we also need systems that keep the convenience of the email while protecting the average users with little security training. AI is the perfect tool to offer this convenience, while constantly evolving and adapting to new threats and attacks.\\nThis story is republished from TechTalks, the blog that explores how technology is solving problems… and creating new ones. Like them on Facebook here and follow them down here:\\nRead next: Bitmain’s former co-CEO speaks out against proposed layoffs\\nTECHEMAILATTACK (COMPUTING)HACKER (TERM)\\nSHARE ON FACEBOOK (0)\\nSHARE ON TWITTER (0',\n",
              "  array([-4.1796527 , -4.320818  , -0.02900957], dtype=float32),\n",
              "  2),\n",
              " ('From the Keurig of cocktails and soft, squishy comfort robots to artificial intelligence-fueled projector screens and personalized skincare regiments, more than 200 new products appeared at a media event on Sunday ahead of the annual Consumer Electronics Show (CES) in Las Vegas, Nevada.',\n",
              "  array([-4.5862494 , -3.9580653 , -0.02972855], dtype=float32),\n",
              "  2),\n",
              " ('The co-founder and former co-CEO of cryptocurrency mining hardware manufacturer Bitmain has spoken out against the company’s supposed staff layoffs.\\n\\nIn a social media post published earlier today, Zhan said he is “firmly opposed” to the potential redundancies, The Block reports. He further described the potential action as “unreasonable.”\\n\\nThe open letter was addressed to Bitmain’s employees and went on to discuss Zhan’s take on the company‘s current situation.\\n\\nAccording to Zhan, it would be “ridiculous” to undertake staff cuts and reduce head count by 50 percent. The former co-CEO certainly pulled no punches as he described the reportedly planned redundancies as “near suicide.”\\n\\n“Bitmain’s cash flow is healthy, and there are huge amounts of virtual currency assets,” he wrote.\\n\\nZhan also believes that Bitmain could hold on to its 1,300 employees. Compared to the company‘s profit, staff overheads are not that large, he said.\\n\\nBitmain‘s potential layoffs came to light last month. It’s since been reported that the decision will be made at its annual company meeting on January 17.\\n\\nThe original report stated the upcoming Bitcoin halving as the main driver behind the potential layoffs. However, Zhan says the company‘s Artificial Intelligence division will be the focus of redundancies, as it’s not generating profit.\\n\\nZhan was ousted from Bitmain towards the end of last year and isn’t taking it lightly.\\n\\nFollowing his removal from the firm, Zhan has begun legal proceedings in an attempt to regain control of the company he helped setup.',\n",
              "  array([-0.77100605, -0.65876466, -3.9139082 ], dtype=float32),\n",
              "  1),\n",
              " (\"5 things you need to know Tuesday\\n6:01 AM EST Jan 21, 2020\\nSenate's legacy also on line at Trump impeachment trial\\n5:40 AM EST Jan 21, 2020\\nWhat to expect in the Trump impeachment trial\\n5:58 AM EST Jan 21, 2020\\nSearch for missing Montana girl ends with 'great sadness'\\n9:29 PM EST Jan 20, 2020\\nMLB pitcher on Astros scandal: 'It was all a facade'\\n3:09 PM EST Jan 20, 2020\\nThousands rally for gun rights in Virginia\\n2:37 PM EST Jan 20, 2020\\nWhat your commute could look like in 2030\\n4:01 AM EST Jan 21, 2020\\nProposed rules for Trump's impeachment trial will set up heated debate\\n7:42 PM EST Jan 20, 2020\\nPence praises senator who sold impeachment vote\\n3:15 AM EST Jan 21, 2020\\n11:52 AM EST Jan 19, 2020\\nTim Tebow, ex-Miss Universe announce their marriage\\n10:01 PM EST Jan 20, 2020\\nLev Parnas accuses AG Barr of conflict of interest\\n6:34 PM EST Jan 20, 2020\\nSAG Awards 2020: Stars sparkle on the red carpet\\n3:16 PM EST Jan 20, 2020\\nTorn flag stops uniformed man in his tracks\\n11:15 AM EST Jan 20, 2020\\nPatient dog helps owner work out\\n11:58 AM EST Jan 20, 2020\\nLeBron: Fan who threw something at son 'disrespectful'\\n11:34 PM EST Jan 20, 2020\\nWhat you need to know about the flu\\n3:33 PM EST Jan 17, 2020\\nKnow what's up before you finish your cup. Sign up to get the Daily Briefing.\\n12:56 PM EST Jan 4, 2019\",\n",
              "  array([-1.7255218 , -1.6666815 , -0.45721048], dtype=float32),\n",
              "  2),\n",
              " (\"LAS VEGAS (Reuters) - Toyota Motor Corp (7203.T) said on Monday it planned to build a prototype “city of the future” at the base of Japan’s Mt. Fuji, powered by hydrogen fuel cells and functioning as a laboratory for autonomous cars, “smart homes,” artificial intelligence and other technologies.\\nAkio Toyoda, president of Toyota Motor Corporation, speaks at a news conference, where he announced Toyota's plans to build a prototype city of the future on a 175-acre site at the base of Mt. Fuji in Japan, during the 2020 CES in Las Vegas, Nevada, U.S. January 6, 2020. REUTERS/Steve Marcus\\nToyota unveiled the audacious plan for what it will call “Woven City”, in a reference to its origins as a loom manufacturer, at the big annual technology industry show, CES.\\n“It’s hard to learn something about a smart city if you are only building a smart block,” James Kuffner, chief executive officer for the Toyota Research Institute-Advanced Development, told Reuters.\\nThe “Woven City” idea, under discussion for a year, is aimed at creating safer, cleaner, more fun cities and learning lessons that could be applied around the world, he added.\\nIt will have police, fire and ambulance services, schools and could be home to a mix of Toyota employees, retirees and others, Kuffner said.\\nThe development, to be built on the site of a car factory that is planned to be closed by the end of 2020, will begin with 2,000 residents in coming years, and also serve as a home to researchers.\\nToyota did not disclose costs for the project, whose construction is scheduled to start next year, and which seeks to re-imagine a city, but executives said it had been extensively vetted and had a budget.\\nThe plan for a futuristic community on 175 acres (71 hectares) is a big step beyond proposals from Toyota’s rivals.\\nExecutives at many major automakers have talked about how cities of the future could be designed to cut climate-changing emissions, reduce congestion and apply internet technology to everyday life.\\nThe company’s proposal showcases not only the ambition of Chief Executive Akio Toyoda, but also the financial and political resources Toyota can bring to bear, especially in its home country.\\n“You know if you build it, they will come,” said Toyoda, who called the project “my personal ‘field of dreams.’”\\nToyota Housing, a company unit, has sold more than 100,000 homes in Japan in 37 years.\\nToyota said it had commissioned Danish architect Bjarke Ingels to design the community. Ingels’ firm designed the 2 World Trade Center building in New York and technology giant Google’s offices in Silicon Valley and London.\\nToyota said it is open to partnerships with other companies seeking to use the project as a testing ground for technology.\\nStill Toyoda acknowledged not all may see the wisdom of what could be an expensive and lengthy project.\\n“You may be thinking, ‘Has this guy lost his mind?’” Toyoda asked an audience in Las Vegas, to laughter. “‘Is he like a Japanese version of Willy Wonka?’ Perhaps.”\",\n",
              "  array([-4.5775647 , -4.0538697 , -0.02802396], dtype=float32),\n",
              "  2),\n",
              " ('Alexa Jett has suffered some heavy blows in recent years.\\nNow 28, she was diagnosed with thyroid cancer in 2016. Although she was given the all-clear, in August 2019 another crisis hit when her best friend and former boyfriend died of cancer at the age of 33.\\n\"I completely shut down. I started wondering whether I was next,\" Ms Jett says.\\nShe was unable to get out of bed and household chores piled up leaving the house in a mess.\\nIn desperation, she sought help online and from a mental health chatbot, called Vivibot.\\n\"Hey, why don\\'t we make a goal?\" the chatbot texted her on 10 September.\\nShe only had to paint her toenails for a start, but this simple task combined with the chatbot\\'s \"funny and friendly personality\" and 24/7 presence, encouraged Ms Jett progressively to get more tasks accomplished.\\n\"She pulled me out of that really dark place and I started functioning again,\" says Ms Jett.\\nMore Technology of Business\\nTech trends 2020: New spacecraft and bendy screens\\nAre charity apps making us more generous?\\nThe secrets of \\'food porn\\' viral videos\\nBug busters: The tech behind new vaccines\\nHow to make longer-lasting phone batteries\\nVivibot is offered through GRYT, an app-based social community for people affected by cancer.\\nDozens of similar services are available, which chat with their users on matters of mental health. They offer mood reports and tips on how to improve their mental and emotional state.\\n\"These chatbots are a great first step for people who may be experiencing sad or depressed mood or anxiety to reclaim their mental health,\" says Danielle Ramo, director of research at Hopelab, which designed Vivibot.\\nShe is quick to add that chatbots cannot treat clinical depression or clinical anxiety, and are not designed to replace a human interaction of any sort.\\nHowever, clinical psychologist Noel Hunter says that some chatbots are not marketed that way and instead present themselves as a solution for mental health problems.\\n\"They\\'re very careful to not explicitly say that because then they get sued. But people do get that message,\" says Dr Hunter.\\nFor Dr Hunter, chatbots reinforce the idea that we are at fault for our own suffering.\\n\\nPsychologist Noel Hunter says mental health apps can\\'t replace human doctors\\n\"They make you believe that, if you just look on your phone and do a couple of self-help kind of things, that\\'s going to take the place of the healing nature of a healthy relationship,\" she says.\\nBots of course can\\'t pick up on non-verbal communication that can indicate a lot about the way we feel.\\n\"A big chunk of this non-verbal communication, imperative to our overall well-being and to our being fulfilled, is missed out in non human-to-human settings,\" Dr Hunter says.\\nHowever, there\\'s a growing interest in using tech to improve the world\\'s mental health. The World Health Organisation says one in four of us suffers some kind of mental health problem and other research suggests that people are more honest with robots than with fellow humans.\\nEven social media giants like Facebook are entering the arena of digital mental health.\\nIn October 2019, Facebook and Messenger launched the Let\\'s Talk Stories filter and a series of Let\\'s Talk Messenger stickers - tools prompting people to begin conversations leading to support.\\n\"We found that private messaging can make it easier to talk about emotional or serious subjects. In fact, 80% of people who use messaging apps feel they can be completely honest when messaging versus in person,\" says Antigone Davis, head of global safety at Facebook.\\n\\nFacebook says that private messaging can make it easier to talk about serious issues\\nLooking ahead, there could come a time when artificial intelligence (AI) might be advanced enough to have a deep understanding of human mental health.\\n\"We might have human-level AI in 2029,\" says Peter Diamandis, physician, engineer, and author of The Future is Faster Than You Think.\\nWe are only in the nascent days of AI particularly in the medical arena, he says.\\n\"The amount of data that\\'s now collected by medical exams, whether it\\'s looking at a brain MRI or your genome and the results from various tests, all these things are way beyond the ability of a single human,\" Mr Diamandis says.\\n\"It will actually be malpractice not to use AI in diagnosis in the next 20 years, possibly 10 years,\" he continues.\\nNot everyone agrees that AI will advance that fast. And the question remains, will humans relate to AI in the same way as they do to a human therapist?\\nMs Jett certainly thinks so. She points out that her generation has grown up with digital technology - to her it is an extension of her existence.\\n\\nPeter Diamandis says in the future it will be \\'malpractice\\' not to use AI in medicine\\nBut Dr Hunter only sees a techno-bubble that, once it bursts, will get people to turn to more traditional ways of healing, either in the form of human-to-human therapy or spirituality.\\n\"Belonging to certain kinds of groups of peers, something involving relationships,\" she says.\\nMr Diamandis sees a balance, tilted towards a heavy involvement of AI in our lives.\\n\"I would imagine that a human therapist using AI is going to be much more powerful than a human therapist on their own,\" he says, adding that in almost every single area where an AI and a human coexist to evaluate and treat patients, success rates are better.\\nMr Diamandis urges us to think of the Iron Man movie to understand how AI will transform our mental health.\\nIn the movie, genius superhero Tony Stark (Iron Man) has a digital helper Jarvis, who arranges his meetings, answers the door, and even organises his playlists.\\n\"I think we\\'re all going to have a version of Jarvis in the next decade,\" Mr Diamandis says.\\n\"A Jarvis who will do our administrative tasks, such as read our emails or answer our phone calls; a Jarvis that will sense a depressed mood in our house and reverse it by playing our favourite movie, or a song it knows uplifts our spirits; a Jarvis that will study us 24/7 and learn us in many ways we don\\'t even know ourselves.\"',\n",
              "  array([-1.9995714 , -2.2360356 , -0.27743435], dtype=float32),\n",
              "  2),\n",
              " ('Facebook has banned users from posting computer-generated, highly manipulated videos, known as deepfakes, seeking to stop the spread of a novel form of misinformation months before the 2020 presidential election.\\nBut the policy — first reported by The Washington Post, and confirmed by Facebook late Monday — does not prohibit all doctored videos. The tech giant’s new guidelines do not appear to address a deceptively edited clip of House Speaker Nancy Pelosi that went viral on the social network last year, prompting criticism from Democratic leaders and digital experts.',\n",
              "  array([-0.09851871, -2.6975489 , -3.6324644 ], dtype=float32),\n",
              "  0),\n",
              " ('Samsung has unveiled a tennis ball-like robot called Ballie that beeps and rolls around, following its owner.\\nThe device has a built-in camera so that it can capture and store \"special moments\", the South Korean tech giant told an audience at the CES tech show in Las Vegas.\\n\"I love this guy,\" said president and chief executive H S Kim as Ballie chased him about on stage.\\nOne analyst said the bot was a fun idea - though it might struggle with stairs.\\nAs well as shadowing its owner, Ballie acts as a fitness assistant and can also help with household chores. For example, it can activate smart home devices such as robotic vacuums, when it determines that cleaning is required.\\nOn social media, the device has already been compared to robot companions from several movies - including BB-8 from the latest Star Wars trilogy films.\\nReport\\n\"It\\'s fun - it reminded me of a cross between a Sphero toy and R2-D2 with the sounds it was making,\" said Simon Bryant at market research firm Futuresource.\\n\"How practical it is when it can\\'t go up stairs, I\\'m not sure.\"\\nMr Bryant said he thought it was strange that Samsung had introduced Ballie, which has voice recognition capabilities, without any mention of the firm\\'s voice-activated virtual assistant Bixby, which is built in to smartphones and TVs.\\nPork made from plants launched by Impossible Foods\\nSony announces electric car concept at CES\\n\"I can see a lot of people who will be hesitant with security and privacy concerns,\" commented Paul Gagnon, an analyst at IHS Markit.\\nSamsung said that Ballie would adhere to privacy and data protection standards.\\nThe firm has not yet revealed when the device will be available to buy, or what it will cost.\\nWhen asked why now was the right time to launch a personal robot like this, Samsung spokesman Benjamin Braun told the BBC:\"There are two technologies that are helping this come to life. One is artificial intelligence and the other one is 5G. Those are very much focus areas for Samsung in 2020.\"\\n\\nA Samsung executive was chased on stage - playfully - by Ballie\\nDuring its keynote presentation, Samsung also discussed other technologies including a mini-exoskeleton worn around the waist and thighs that it said could be used during exercise or by people with mobility issues.\\nThe device is called Gems, which stands for Gait Enhancing and Motivating System.\\nIt was suggested that a user could don Gems along with a pair of augmented reality glasses in order to experience a workout with a virtual personal trainer, or explore underwater landscapes.\\nSamsung also said it would seek to develop new technologies for use in smart buildings and cities.\\nHowever, Mr Bryant said he was left unimpressed by many of these ideas.\\n\"I thought it just smacked as a desperate attempt to move on from mobile,\" he told the BBC.',\n",
              "  array([-4.898928  , -3.878186  , -0.02854649], dtype=float32),\n",
              "  2),\n",
              " ('With a presidential election campaign underway in the United States, Facebook announced Monday that it has banned manipulated videos and photos, often called deepfakes, from its platforms.\\nThe policy change was announced through a blog post late Monday night, confirming an earlier report from The Washington Post. In the post, Facebook said that it would begin removing content that has been edited “in ways that aren’t apparent to an average person and would likely mislead someone” and are created by artificial intelligence or machine learning algorithms.\\nTHE POLICY DOESN’T COVER “PARODY OR SATIRE”\\nBut the policy does include content that is parody or satire, or video that has been edited to remove words or change the order in which they appear, the company said.\\nThis policy change comes ahead of a House Energy and Commerce hearing on manipulated media that is scheduled for Wednesday. The author of Monday’s post — Monika Bickert, Facebook’s vice president of global policy management — is set to represent Facebook in front of lawmakers at this week’s hearing.\\nThe deepfakes ban comes after an altered video of House Speaker Nancy Pelosi (D-CA) went viral on social media platforms last summer. This video was widely viewed on Facebook, and when reached for comment by The Verge at the time, the company said that it did not violate any of the company’s policies. And Monday’s ban against deepfakes doesn’t appear to cover videos like the viral Pelosi clip, either. That video wasn’t created by AI, but was likely edited using readily available software to slur her speech.\\nOther platforms were also caught in the crossfire following the Pelosi video, including Twitter. In November, Twitter began crafting its own deepfakes policy and requested feedback from users concerning the platform’s future rules. The company has yet to issue any new guidance on manipulated media.',\n",
              "  array([-0.08739194, -3.61904   , -2.866922  ], dtype=float32),\n",
              "  0),\n",
              " ('LAS VEGAS (Reuters) - The White House on Tuesday proposed regulatory principles to govern the development and use of artificial intelligence (AI) aimed at limiting authorities’ “overreach”, and said it wants European officials to likewise avoid aggressive approaches.\\nFILE PHOTO: A sign advertising AI is seen at CES (Consumer Electronics Show) Asia 2019 in Shanghai, China June 11, 2019. REUTERS/Aly Song\\nIn a fact sheet, the White House said federal agencies should “conduct risk assessment and cost-benefit analyses prior to any regulatory action on AI, with a focus on establishing flexible frameworks rather than one-size-fits-all regulation.”\\nThe comments come at a time when companies are racing to integrate AI and deep machine learning into their businesses to remain competitive. However, the technology raises ethical concerns about control, privacy, cyber security and the future of work, companies and experts have said.\\nThe Trump administration said agencies should “promote trustworthy AI” and “must consider fairness, non-discrimination, openness, transparency, safety, and security.”\\nAs an example, the White House cited the Food and Drug Administration which is currently considering how to regulate the use of AI and machine learning technologies by medical device manufacturers.\\nThe White House said, “Europe and our allies should avoid heavy handed innovation-killing models.” It added, “the best way to counter authoritarian uses of AI is to make sure America and our international partners remain the global hubs of innovation.”\\nLast year, the European Commission’s High-Level Expert Group on Artificial Intelligence issued a set of ethical guidelines and European Union leaders are considering regulatory action.\\nTHREAT\\nSome U.S. states have raised concerns about AI applications. California’s legislature in September passed a three-year ban on state and local law enforcement using body cameras with facial-recognition software, the latest curb on technology that some say poses a threat to civil liberties. Some U.S. cities have also voted to bar facial-recognition technology by law enforcement.\\nThe White House’s Michael Kratsios, chief technology officer of the United States, who will talk about the administration’s AI strategy at the CES trade show in Las Vegas later this week, in a statement said Tuesday’s “principles set the nation on a path of continued AI innovation and discovery.”\\nIn February, U.S. President Donald Trump signed an executive order for federal government agencies to dedicate more resources and investment to AI-related research, promotion and training.\\nA 2018 study from consultancy PwC said 30% of jobs around the world are at risk of automation by the mid-2030s, including 44% of workers with low education. The study also found automation could boost global gross domestic product by $15 trillion by 2030.\\nThe White House held a meeting on AI in 2018 with over 30 major companies from a variety of industries, including Ford Motor Co, Boeing Co, Amazon.com Inc and Microsoft Corp, vowing not to stand in the way of the technology’s development.',\n",
              "  array([-0.7517759, -0.7404242, -2.9650078], dtype=float32),\n",
              "  1),\n",
              " ('FILE PHOTO: Stickers bearing the Facebook logo are pictured at Facebook Inc\\'s F8 developers conference in San Jose, California, U.S., April 30, 2019. REUTERS/Stephen Lam\\n(Reuters) - Facebook Inc said it will remove deepfakes and other manipulated videos from its platform if they have been edited, but not content that is parody or satire, in a move aimed at curbing misinformation ahead of the U.S. presidential election.\\nRELATED COVERAGE\\nPelosi spokesman dismisses Facebook decision to remove some manipulated videos\\nIt would also remove misleading media if it was a result of technologies like AI that \"merges, replaces or superimposes content on to a video, making it appear to be authentic\", the California-based company said in a blogpost here dated Jan.6.\\n“This policy does not extend to content that is parody or satire, or video that has been edited solely to omit or change the order of words,” Facebook said.\\nThe social media giant told Reuters that as part of its new policy it will not remove a heavily edited video that attempted to make U.S. House Speaker Nancy Pelosi seem incoherent by slurring her speech and making it appear like she was repeatedly stumbling over her words.\\n“The doctored video of Speaker Pelosi does not meet the standards of this policy and would not be removed. Only videos generated by artificial intelligence to depict people saying fictional things will be taken down,” Facebook said in a statement.\\n“Once the video of Speaker Pelosi was rated by a third-party fact-checker we reduced its distribution, and critically, people who saw it, tried to share it or already had received warnings that it was false”.\\nFacebook has been criticized over its content policies by politicians from across the spectrum. Democrats have blasted the company for refusing to fact-check political advertisements, while Republicans have accused it of discriminating against conservative views, a charge that it has denied. In the run-up to the U.S. presidential election in November 2020, social platforms have been under pressure to tackle the threat of deepfakes, which use artificial intelligence to create hyper-realistic videos where a person appears to say or do something they did not.',\n",
              "  array([-0.20247851, -2.9236064 , -2.043638  ], dtype=float32),\n",
              "  0),\n",
              " ('Samsung subsidiary STAR Labs has officially unveiled its mysterious “artificial human” project, Neon. As far as we can tell, though, there’s no mystery here at all. Neon is just digital avatars — computer-animated human likenesses about as deserving of the “artificial human” moniker as Siri or the Tupac hologram.\\nIn fairness to STAR Labs, the company does seem to be trying something new with its avatars. But exactly what it’s doing we can’t tell, as its first official press release today fails to explain the company’s underlying tech and instead relies solely on jargon and hype.\\n“Neon is like a new kind of life,” says STAR Labs CEO Pranav Mistry in the release. “There are millions of species on our planet and we hope to add one more.” (Because nothing says “grounded and reasonable” like a tech executive comparing his work to the creation of life.)\\nNEON’S PROMO IMAGES ARE “FICTIONALIZED AND SIMULATED FOR ILLUSTRATIVE PURPOSES ONLY”\\nEven more annoyingly, it seems that the teaser images and leaked videos of the Neon avatars we’ve seen so far are fake. As the company explains (emphasis ours): “Scenarios shown at our CES Booth and in our promotional content are fictionalized and simulated for illustrative purposes only.” So really we have no idea what Neon’s avatars actually look like.\\nSorting through the chaff in STAR Labs’ press release today, here’s what we know for sure.\\nEach Neon avatar is “computationally generated” and will hold conversations with users while displaying “emotions and intelligence,” says the company. Their likenesses are modeled after real humans, but have newly generated “expressions, dialogs, and emotion.” Each avatar (known individually as “NEONs”) can be customized for different tasks, and is able to respond to queries “with latency of less than a few milliseconds.” They’re not intended to be just visual skins for AI assistants, but put to more varying uses instead:\\n“In the near future, one will be able to license or subscribe to a NEON as a service representative, a financial advisor, a healthcare provider, or a concierge. Over time, NEONs will work as TV anchors, spokespeople, or movie actors; or they can simply be companions and friends.”\\nSo far, so good. It’s no secret that CGI humans have become more lifelike in recent years, and are already being used in some of the scenarios outlined above. If STAR Labs can make these avatars more realistic, then they might be adopted more widely. Fine.\\nBut if you’ve ever interacted with, say, a virtual greeter at an airport or museum, you’ll know how paper-thin the “humanity” of these avatars are. At best, they’re Siri or Alexa with a CGI face, and it’s not clear if STAR Labs has created anything more convincing.\\nSTAR Labs’ “Core R3” technology. Strong buzzwords, weak explanations. Image: STAR Labs\\nIn its PR, the company veers into strange territory is in its description of the avatars’ underlying technology. It says it’s using proprietary software called “Core R3” to create the avatars, and that its approach is “fundamentally different from deepfake or other facial reanimation techniques.” But it doesn’t say how the software does work, and instead relies on wishy-washy assurances that Core R3 “creates new realities.” We’d much rather know if the company is using, say, high-resolution video captures pinned onto 3D models or AI to generate facial movements — whatever the case may be.\\nWe’ve reached out to STAR Labs with our questions, but it seems we’ll have to wait to see the technology in person to get a better understanding. The firm is offering private demos of its avatars at CES this week, and The Verge is scheduled to check out the technology.\\nWe look forward to giving you our hands-on impressions later this week, but until then, don’t worry about any “AI android uprising” — these aren’t the artificial humans you’re looking for.',\n",
              "  array([-4.284756  , -3.7284107 , -0.03854118], dtype=float32),\n",
              "  2),\n",
              " (\"Facebook vows to crack down on ‘misleading’ deepfakes\\nby IVAN MEHTA — 14 days ago in FACEBOOK\\n41\\nSHARES\\nFacebook today announced it will ban misleading manipulated media — including Photoshopped images and deepfakes. This announcement comes right on time, ahead of the upcoming US presidential elections this year. \\nThe declaration of the new policy comes just before Monika Bickert, Vice President, Global Policy Management, is set to testify against the House Energy and Commerce consumer protection subcommittee tomorrow to discuss how the platform will tackle manipulative media.\\nBirds are cool\\nEarly birds are even cooler. Get cheap tickets to TNW2020 right now\\nYEAH\\nThe social network will remove media from its platform if it satisfies any of these two criteria:\\nIt has been edited or synthesized – beyond adjustments for clarity or quality – in ways that aren’t apparent to an average person and would likely mislead someone into thinking that a subject of the video said words that they did not actually say. \\nIt is the product of artificial intelligence or machine learning that merges, replaces or superimposes content onto a video, making it appear to be authentic.\\nThe company said it won’t be removing modified content that’s supposed to be parody or satire. That means videos made for fun using less sophisticated techniques, called ‘Shallowfakes.’\\nThe policy also doesn’t consider videos that “have been edited solely to omit or change the order of words.” So, there are chances that media such as a video edited to make Joe Biden sound racist released last week might be not be removed. \\nThis is such a narrowly-drawn definition of what would be booted off the platform that it's baffling. You can cut up a speech to make someone say something diametrically opposed to what they actually said and it's ok. Edit a la the Nancy Pelosi video, it's ok. Weird.\\n— Chris Stokel-Walker (@stokel) January 7, 2020\\nMany critics are arguing that it’s a narrowly written policy that might let a lot of videos off the hook.\\nNaturally, the challenge for the tech giant will be detecting manipulated media and deciding its misleading. Last October, the company joined Amazon and Microsoft to help researchers to develop tools for better detection. In September, it also announced a Deepfake Detection Challenge program by partnering with universities. These partners pledged $10 million and released 5,000 videos to help developers.\\nHowever, it’s one thing to invest in a research project, and it’s another thing to implement policies in the same area when millions of posts are going live at the same time. Facebook says it’s consulting with more than 50 experts across the world with technical, policy, media, legal, civic and academic backgrounds to tackle this issue. \\nSocial media has always had controversial takedown cases. Facebook is one of the first major social networks to lay down the norms for deepfakes. We’ll have to wait and see if it can do this job efficiently.\\nRead next: Mozilla, GitHub, and Cloudflare appeal to Indian government over its online content policing plans\\nFACEBOOKFACEBOOKBAN (LAW)DEEPFAKE\\nSHARE ON FACEBOOK (0)\\nSHARE ON TWITTER (39\",\n",
              "  array([-1.3977435, -0.7190832, -1.3255857], dtype=float32),\n",
              "  1),\n",
              " ('Soleimaini burial, Harvey Weinstein, \\'Jeopardy!\\' champs battle: 5 things to know Tuesday\\nEditors USA TODAY\\nPublished 8:09 AM EST Jan 7, 2020\\nStampede at Iranian Gen. Qasem Soleimani\\'s funeral kills dozens, state media reports\\nIranian state television reported Tuesday that at least 40 people were killed and 213 others injured in a stampede that erupted at the funeral procession for slain Gen. Qasem Soleimani in his hometown of Kerman, in southeastern Iran. Soleimani, the leader of Iran\\'s elite Quds Force, was killed in an U.S.-ordered drone strike in Baghdad last week. The deaths at the funeral came as Iran\\'s Parliament on Tuesday approved a bill designating the entire U.S. military and Pentagon terrorist organizations after Soleimani\\'s killing. Hossein Salami, the leader of Iran\\'s Revolutionary Guard, earlier threatened to \"set ablaze\" places supported by the United States. Iran\\'s supreme leader wept over Soleimani\\'s casket as a mammoth crowd filled Tehran on Monday. The outpouring of grief was an unprecedented honor for a man viewed by Iranians as a national hero. The U.S. blames him for the killing of American troops in Iraq and accused him of plotting new attacks just before his death.\\nQuestions answered: What you need to know about the death of Gen. Soleimani and the escalating situation with Iran\\nReactions: Iranian Americans aren\\'t mourning Soleimani. They\\'re glad he\\'s dead. But, now what?\\nNew poll: 43 percent of Americans approve of Trump\\'s airstrike of Soleimani\\n\\'Bigger than bin Laden\\': Petraeus says it\\'s \\'impossible to overstate\\' impact of Soleimani death\\nPrefer to listen? Check out the 5 things podcast below and subscribe for free on Apple Podcasts: \\nHarvey Weinstein\\'s New York trial: Jury selection begins\\nPre-jury selection starts Tuesday in Harvey Weinstein\\'s New York sex crimes trial, the first celebrity #MeToo case to open in a criminal court. The court sent questionnaires to 2,000 potential jurors, and the selection process is expected to last for at least the next week. Weinstein was indicted in May 2018 in Manhattan and had been charged with five sex crimes, including rape and predatory assault, involving two women in encounters dating to 2006 and 2013. The disgraced movie mogul, who has pleaded not guilty, has been free on $1 million bail (recently increased to $5 million). Shortly after Monday’s New York trial kickoff, Weinstein was hit with new charges of raping one woman and sexually assaulting another in separate incidents over two days in Los Angeles in 2013.\\nNew complaint: Former model files suit against Weinstein alleging assault when she was 16\\nSpeaking out: Weinstein says, \\'I feel like the forgotten man;\\' accusers call him \\'an unrepentant abuser\\'\\nHarvey Weinstein scandal images: Accusers step forward\\nSpace mice set for Pacific splashdown\\nForty mighty mice that went up in the International Space Station last month as part of science experiments are due to return to Earth on Tuesday. The mice were genetically engineered or were given a test drug to learn more about the effects of muscle and bone degradation, which can be a problem for astronauts and also for persons with diseases such as osteoporosis or muscular dystrophy. The SpaceX Dragon resupply spacecraft is scheduled to leave the space station at 5:03 a.m. ET, with splashdown in the Pacific Ocean set for 10:47 a.m., according to NASA.\\nA major first: Pair make history in NASA\\'s first all-female spacewalk\\nDeep pockets needed: NASA wants to open ISS to private travelers soon\\n\\'Jeopardy!\\' champs battle for GOAT title\\nThe three biggest winners in Jeopardy! history are set to face off this week in a quest to win $1 million – and finally settle the question of who\\'s the best ever. In “Jeopardy! The Greatest of All Time,” which kicks off Tuesday on ABC (8 p.m. ET), James Holzhauer, Ken Jennings and Brad Rutter and will compete in a series of one-hour matches, each consisting of two complete games. The winner of each match will be decided by their total point score in the two games. The first to win three matches receives the $1 million prize – and big-time bragging rights.\\nSpoiler alert? Winner may have already leaked, sportsbook says\\nAlex Trebek: How he\\'ll eventually say goodbye to \\'Jeopardy!\\'\\nStill got it: Holzhauer wins 2019 \\'Jeopardy!\\' Tournament of Champions, adds to streak\\nHere\\'s what to expect at CES 2020\\nFlying cars, sex toys, 8K TV, even Ivanka Trump. That\\'s some of what to expect from the CES, the mammoth tech trade show that officially opens on Tuesday in Las Vegas. For the consumer watching from afar (since the show isn\\'t open to the public), you can expect a wide range of products to be represented: robots, headphones, cameras, computers, and though not an over-the-top wireless show, smartphones, too, maybe even more of the emerging foldable-type handsets. What you’ll also hear plenty about are 5G wireless networks, artificial intelligence and tech that will enable entire smart cities. As for the latest in TV tech: CES attendees will also get to peer at 16K — even if it’ll be years before you’d even think about buying one.\\nCES 2020: 4 cool gadgets that already caught our attention\\nMeet the woman responsible for the return of sex toys at CES 2020\\nContributing: Associated Press\\nPublished 8:09 AM EST Jan 7, 2020',\n",
              "  array([-0.8681196 , -0.77378637, -2.12866   ], dtype=float32),\n",
              "  1),\n",
              " ('Samsung is working on lifelike AI-powered avatars to fill in for humans\\nby ABHIMANYU GHOSHAL — 14 days ago in ARTIFICIAL INTELLIGENCE\\nCredit: STAR Labs\\n74\\nSHARES\\nSamsung has been teasing its NEON project for a few days now, and we finally have some idea of what it’s about: AI-powered digital avatars that look like humans.\\nIt’s actually an effort by STAR Labs, an experimental subsidiary of the Korean giant led by scientist and inventor Pranav Mistry. From what I can make of the vague press release and FAQ, the idea is to create software-based ‘artificial humans’ (called NEONs) that you can communicate and interact with, just as you would with a real person.\\nVolume 90%\\nBirds are cool\\nEarly birds are even cooler. Get cheap tickets to TNW2020 right now\\nYEAH\\nThey’ll be accessible via screens and perhaps holographic displays. STAR Labs says they’ll be far more capable than today’s voice assistants. They can serve as “an individualized teacher, a personal financial advisor, a healthcare provider, or a concierge. A NEON can also be an actor, a spokesperson, or a TV anchor. A NEON can be our friend, collaborator or companion.”\\nCredit: STAR Labs\\nSTAR Labs’ NEONs are essentially digital avatars powered by AI and built to help people more than voice assistants can\\nThese avatars are powered by two technologies: CORE R3, a proprietary platform (R3 stands for ‘Reality, Realtime, and Responsive’), and SPECTRA, which “complements CORE R3 with the spectrum of Intelligence, Learning, Emotions and Memory.” No, I don’t know what any of that means.\\nSTAR Labs explains that while NEONs can be based on real people and resemble them, they won’t be exact replicas. That said, it’s worth thinking about the potential danger of creating realistic avatars that could be used to imitate real people and spread misinformation. This may not be a huge issue if the company tightly controls who has access to the tech to create avatars, but that’s not yet clear.\\nThe company also claims that NEONs can also learn from experiences — but it doesn’t explain how that will work. It’s also not immediately clear if these avatars will operate as part of a cloud service run by Samsung or STAR LABS, or whether businesses will have the option to run them locally. Similarly, we don’t know how NEONs will be made available to people to interact with. So, yeah, still more questions than answers at this point.\\nHopefully we’ll get a bit more clarity when NEONs are demonstrated live at CES over the next couple of days. STAR Labs will also talk more about this project at an event called NEONWORLD 2020 later this year. Hurray for hype.\\nRead next: Tumblr’s literacy initiative wants to educate people on misinformation and cyberbullying\\nSAMSUNGTECHSAMSUNGAVATARPRANAV MISTRYARTIFICIAL INTELLIGENCE\\nSHARE ON FACEBOOK (0)\\nSHARE ON TWITTER (64',\n",
              "  array([-4.754691  , -3.8670337 , -0.02997638], dtype=float32),\n",
              "  2),\n",
              " (\"European Commission President Ursula von der Leyen has made responding to technology-driven change a key priority for her five year term which began last month, setting it in the same breath as challenges posed by climate change and demographic shifts, tacitly linking all three to a rise in regional unease.\\nHer solution, set out in a document entitled My Agenda for Europe, is a set of “headline ambitions” for the next five years — which include a Green Deal for Europe which she commits to proposing within her first 100 days in office; greater inclusion via “an economy that works for people,” and “a Europe fit for the digital age,” where innovation occurs “within safe and ethical boundaries.”\\nThe program implies pan-EU economic changes; an update to industrial policy which includes substantial investment in strategic R&D such as clean technologies, and in infrastructure; plus regulation of digital technologies such as AI, as well as along an axis of fairness — which puts platforms and data monopolies in the frame.\\n“If we do our job well, the Europe of 2050 will be the first continent in the world to be carbon neutral,” Von der Leyen told the EU parliament, while laying out the ambition for the bloc to be “a leading digital power” with an economy that balances market forces and “social concern.”\\nGreen Deal\\n“We will invest record amounts in cutting-edge research and innovation, using the full flexibility of the next EU budget to focus on the areas with the greatest potential,” wrote von der Leyen, ahead of taking up the mandate, setting out a plan for a European Green Deal that pledged to make the region “a world leader in circular economy and clean technologies.”\\nSpeaking to the EU parliament ahead of the confirmatory vote, she described the Green Deal as Europe’s “new growth strategy,” saying it would require “massive investment” in “innovation, research, infrastructure, housing, and the training of people,” which would need to be funded by both the public and private sector at European and national levels.\\n“At the core of it will be an industrial strategy that enables our businesses — big and small — to innovate and to develop new technologies while creating new markets,” she told MEPs. “We will be global standard setters. This is our competitive advantage. And the best way to ensure a level-playing field.”\\nThe strategy includes ramping up not just public but private investment in climate financing — with a goal of at least 50% of the European Investment Bank’s funds being dedicated to “green and sustainable” financing by 2025.\\nOn the circular economy — which her Agenda identifies as Europe’s “future economic model” — she trails an action plan that will focus on sustainable resource use,  “especially in resource-intensive and high impact sectors such as textiles and construction”. She also urged measures to reduce plastic waste.\\nThe overarching policy offer is a European Climate Law that will enshrine a 2050 climate neutrality target into law, with proposals to extend the emissions trading system to cover the maritime sector and to reduce free allowances allocated to airlines “over time.”\\nA Carbon Border Tax is also trailed, starting with “a number of selected sectors,” then “gradually extended,” along with a review of the Energy Taxation Directive which sets minimum excise duty rates EU Member States must apply to energy products for fuel and transport, and electricity.\\nA Biodiversity Strategy is another offer, along with a focus on “sustainable food” to support European farmers to transition to less environmentally costly production methods.\\nVon der Leyen’s plan also includes what’s couched as a “zero-pollution ambition,” although the document lacks specific targets — beyond a promise of “cross-cutting strategy to protect citizens’ health from environmental degradation and pollution.”\\nIt’s notable that the green bloc in the EU parliament elected to abstain rather than back Von der Leyen’s Commission at the final vote — urging her to go further and faster to address the climate emergency, such as agricultural reforms, while saying they would be prepared to be a “constructive partner” to genuine environmental and social reforms.\\nSupport for startups to scale up\\nEconomic rewiring in the EC president’s five-year plan includes some encouraging words for entrepreneurs dreaming of scaling big.\\n“We need more young and nimble innovators with breakthrough technologies, like this generation’s tech giants were only a decade ago,” von der Leyen writes in a section of her Agenda on supporting small business which touts a “dedicated SME strategy” to cut red tape and improve market access for startups.\\nOn developing the pan-EU growth finance market she commits to completing the Capital Markets Union — an initiative that’s intended to reduce fragmentation of capital access across the bloc and so shrink the costs of raising capital while widening the funding pipe.\\nShe also wants to create a private-public fund specializing in Initial Public Offerings of SMEs — with the hope that private investors will come in to match an initial investment from the EU.\\nSo late-stage startup financing is on the Commission’s radar.\\nDiscussing the policy announcement, James Wise, partner at Balderton Capital, told us more funding in Europe would be welcome. “There is still a funding gap with the U.S. even if it is shrinking,” he suggested. However he also warned of too much focus being put on “overly financializing many assets classes in the last few decades, resulting in a cycle of boom and bust, and a focus on quarterly reports, not necessarily long term goals.”\\n“As a result we believe companies should go public at the right time for them, and not be pushed into exits by ‘quick-flip’ capital,” he said. “A bigger challenge for European start-ups isn’t necessarily access to capital, but capital markets which understand them. Many European banks have skeleton tech teams, while U.S. and Asian banks have whole departments focused on them. The lack of good equity analysts makes going public in Europe harder, hence the relative strength of U.S. indexes, and another later stage fund is unlikely to change this.”\\nTechnological sovereignty\\nIn remarks to the EU parliament, Von der Leyen also called for Europe to have “mastery and ownership of key technologies” — name-checking quantum computing, artificial intelligence, blockchain, “critical” chip technologies, and urging a pooling of resources, money, research capacity and knowledge in order for Europe to stake a claim to the leading edge of tech.\\nOn the surface it sounds switched on to the potential of emerging technologies. Although quite how this buzzwordy big ambition will trickle down at the policy level — such as via investment distributed to strategic R&D — and so whether it will really make a difference to European startups working on emerging technologies remains to be seen.\\nAt least Von der Leyen’s Commission can’t be accused of technical illiteracy.\\nIn November 2019, Bloomberg reported that the Commission is planning a $3.9BN EU fund to launch in 2021 — and be run by the European Innovation Council — to support early stage ‘deep tech’ startups working in areas such as biotech, health tech and AI. (Although the size of the fund remains tbc, as it’s dependent on the outcome of budget talks.)\\n“It may be too late to replicate hyperscalers, but it is not too late to achieve technological sovereignty in some critical technology areas,” is Von der Leyen’s suggestion, with supercomputing citied as an exemplar.\\nShe appears to be spreading her bets on the next tech paradigm shift, saying “we will invest in blockchain, high-performance computing, quantum computing, algorithms and tools to allow data sharing and data usage”.\\nAnother pledge is to push for jointly defined standards, such as for 5G — and for what she calls “this new generation of technologies that will become the global norm.”\\nShe also wants to update the Digital Education Action Plan to increase access to digital literacy for young people and reskilling adults, such as by increasing the use of MOOCs (massively open online courses) in the area of digital skills — to “get Europe up to speed.”\\nThe new Commission also has its eye on developments in the cryptocurrency space. This month the European Council and Commission issued a joint statement on so-called stablecoins — such as Facebook’s planned global digital currency, Libra — essentially saying such efforts will be barred from launching in the region ahead of a common EU regulatory framework being put in place to mitigate any risks.\\n“We reaffirm our willingness to appropriately tackle the challenges raised by these initiatives on the basis of an EU common understanding and coordinated approach,” they wrote. “These initiatives should not undermine existing financial and monetary order as well as monetary sovereignty in the European Union.\\n“While the Council and the Commission are committed to put in place a framework that will harness the potential opportunities that crypto-assets may offer, we acknowledge the risks that some present. The Council and the Commission are prepared to take all necessary measures to ensure appropriate standards of consumer protection and orderly monetary and financial conditions. All options should be on the table, including any measures to prevent the creation of unmanageable risks by certain global ‘stablecoins.'”\\nGig economy rights\",\n",
              "  array([-1.4321417, -0.3302969, -3.158422 ], dtype=float32),\n",
              "  1),\n",
              " ('How robots, AI, and drones are changing toy manufacturing\\nby THE CONVERSATION — in SYNDICATION\\n5\\nSHARES\\nI’m a geek. And as a geek, I love my tech toys. But over time I’ve noticed toys are becoming harder to understand.\\nSome modern toys resemble advanced devices. There are flying toys, walking toys, and roving toys. A number of these require “configuring” or “connecting.”\\nEarly birds get all the good stuff\\nLike a great price on TNW2020 tickets\\nGIMME\\nThe line between toy, gadget and professional device is blurrier than ever, as manufacturers churn out products including drones for kids and plush toys with hidden nanny cams.\\nWith such a variety of sophisticated, and sometimes over-engineered products, it’s clear manufacturers have upped their game.\\nBut why is this happening?\\nThe price of tech\\nToys these days seem to be designed with two major components in mind. It’s all about the smarts and rapid manufacture.\\nIn modern toys, we see a considerable level of programmed intelligence. This can be used to control the toy’s actions or have it respond to input to provide real-time feedback and interaction – making it appear “smarter.”\\nThis is all made possible by the falling price of technology.\\nOnce upon a time, placing a microcontroller (a single-chip microprocessor) inside a toy was simply uneconomical.\\nThese days, they’ll only set you back a few dollars and allow significant computing power.\\nMicrocontrollers are often WiFi and Bluetooth enabled, too. This allows “connected” toys to access a wide range of internet servicesor be controlled by a smartphone.\\nAnother boon for toy manufacturers has been the rise of prototype technologies, including 3D modeling, 3D printing, and low-cost CNC (computer numerical control) milling.\\nThese technologies allow the advanced modeling of toys, which can help design them to be “tougher.”\\nThey also allow manufacturers to move beyond simple (outer) case designs and towards advanced multi-material devices, where the case of the toy forms an active part of the toy’s function.\\nExamples of this include hand grips (found on console controls and toys including Nerf Blasters), advanced surface textures, and internal structures that support shock absorption to protect internal components, such as wheel suspensions in toy cars.\\nBot helpers and robot dogs\\nMany recent advancements in toys are there to appease our admiration of automatons or self-operating machines.\\nThe idea that an inanimate object is transcending its static world, or is “thinking,” is one of the magical elements that prompt us to attach emotions to toys.\\nAnki’s Cozmo (the Vector’s predecessor) is an example of a cloud-connected robotic toy. shutterstock\\nAnd manufacturers know this, with some toys designed specifically to drive emotional attachment. My favorite example of this is roaming robots, such as the artificially intelligent Anki Vector.\\nWith sensors and internet connectivity, the Vector drives around and interacts with its environment, as well as you. It’s even integrated with Amazon Alexa.\\nAnother sophisticated toy is Sony’s Aibo. This robot pet shows how advanced robotics, microelectronics, actuators (which allow movement), sensors, and programming can be used to create a unique toy experience with emotional investment.\\nSony’s Aibo robot dog is cute and robotic – it’s a geek’s dream pet. Shutterstock\\nScreens not included\\nToy manufacturers are also leveraging the rise of smartphones and portable computing.\\nQuadcopters (or drones) and other similar devices often don’t need to include their own display in the remote control, as video can be beamed to an attached device.\\nSome toys even use smartphones as the only control interface (used to control the toy), usually via an app, saving manufacturers from having to provide what is arguably the most expensive part of the toy.\\nThis means a smartphone becomes an inherent requirement, without which the toy can’t be used.\\nIt would be incredibly disappointing to buy a cool, new toy – only to realize you don’t own the very expensive device required to use it.\\nMy toys aren’t spying on me, surely?\\nWhile spying may be the last thing you consider when buying a toy, there have been several reports of talking dolls recording in-home conversations.\\nThere are similar concerns with smart-home assistants such as Amazon Alexa, Google Assistant, and Apple’s Siri, which store your voice recordings in the cloud.\\nThese concerns might also be warranted with toys such as the Vector and Aibo.\\nIn fact, anything that has a microphone, camera or wireless connectivity can be considered a privacy concern.\\nToys of the future\\nWe’ve established toys are becoming more sophisticated, but does that mean they’re getting better?\\nVarious reports indicate in 2020, artificial intelligence (AI) and machine learning will continue to be pervasive in our lives.\\nThis means buying toys could become an even trickier task than it currently is. There are some factors shoppers can consider.\\nOn top of my list of concerns is the type and number of batteries a toy requires, and how to charge them.\\nIf a device has in-built lithium batteries, can they be easily replaced? And if the toy is designed for outdoors, can it cope with the heat? Most lithium-ion batteries degrade quickly in hot environments.\\nAnd does the device require an additional screen or smartphone?\\nIt’s also worth being wary of what personal details are required to sign-up for a service associated with a toy – and if the toy can still function if its manufacturer should cease to exist, or the company should go bust.\\nAnd, as always, if you’re considering an advanced, “connected” toy, make sure to prioritize your security and privacy.',\n",
              "  array([-3.5635798 , -3.8496716 , -0.05089745], dtype=float32),\n",
              "  2),\n",
              " ('Facebook has announced it will remove videos modified by artificial intelligence, known as deepfakes, from its platform.\\nDeepfakes are computer-generated clips that are designed to look real.\\nThe social media company said in a blog that these videos distort reality and present a \"significant challenge\" for the technology industry.\\nWhile deepfakes are still relatively uncommon on the internet, they are becoming more prevalent.\\nAI software creates deepfakes of people - often politicians or celebrities - by merging, replacing, or superimposing content on to a video in a way that makes it look real.\\nFacebook said it would remove videos if it realised they had been edited in ways that weren\\'t obvious to an average person, or if they misled a viewer into thinking that a person in a video said words they did not actually say.\\n\"There are people who engage in media manipulation in order to mislead,\" wrote Monika Bickert, vice president of global policy management at Facebook in the blog.\\nFacebook staff and independent fact-checkers will be used to judge a video\\'s authenticity.\\nThe new policy will not apply to parody or satire videos.\\n\\'Difficult area\\'\\nLast September, Facebook announced it was contributing $10m (£7.6m) to a fund to improve deepfake detection technologies.\\nMark Zuckerberg, Facebook\\'s chief executive, has himself featured in a deepfake video. The clip featured a computer-generated version of Zuckerberg crediting a secretive organisation for the success of the social network.\\nWilliam Tunstall-Pedoe, a computer scientist who sold his AI company to Amazon, told BBC News that Facebook deserved credit for trying to tackle the \"difficult area\".\\n\"The fact the video is fake and intended to be misleading is the key thing for me,\" he said. \"Whether sophisticated AI techniques are used or less sophisticated techniques isn\\'t relevant.\"\\nOther companies like Google and Microsoft are also trying to combat deepfakes.\\nFacebook said it plans to work with academia, government, and businesses to expose the people behind deepfakes.\\nThe Californian tech giant added that it would continue to remove all videos that include nudity, graphic violence, voter suppression and hate speech.\\nAltered video of US politician\\nFacebook has been criticised for refusing to remove an altered video of US House of Representatives Speaker Nancy Pelosi that went viral last summer. The video is edited to make it look like she was slurring her words.\\nHowever, Facebook told Reuters it would not be taking the video down under the new policy.\\n\"The doctored video of Speaker Pelosi does not meet the standards of this policy and would not be removed,\" the company reportedly said.\\n\"Only videos generated by artificial intelligence to depict people saying fictional things will be taken down.\"',\n",
              "  array([-1.5875058 , -3.3391573 , -0.27430743], dtype=float32),\n",
              "  2),\n",
              " ('(LONDON) — Facebook says it is banning “deepfake” videos, the false but realistic clips created with artificial intelligence and sophisticated tools, as it steps up efforts to fight online manipulation.\\nThe social network said late Monday that it’s beefing up its policies to remove videos edited or synthesized in ways that aren’t apparent to the average person, and which could dupe someone into thinking the video’s subject said something he or she didn’t actually say.\\nCreated by artificial intelligence or machine learning, deepfakes combine or replace content to create images that can be almost impossible to tell are not authentic.\\n“While these videos are still rare on the internet, they present a significant challenge for our industry and society as their use increases,” Facebook’s vice president of global policy management, Monika Bickert, said in a blog post.\\nHowever, she said the new rules won’t include parody or satire, or clips edited just to change the order of words. The exceptions underscore the balancing act Facebook and other social media services face in their struggle to stop the spread of online misinformation and “fake news” while also respecting free speech and fending off allegations of censorship.\\nRELATED STORIES\\nTECH\\nSex Tech Might Just Be the Biggest New Thing at CES 2020\\nU.S.\\nMystery Drones Are Hovering Over Colorado and Nebraska\\nThe U.S. tech company has been grappling with how to handle the rise of deepfakes after facing criticism last year for refusing to remove a doctored video of House Speaker Nancy Pelosi slurring her words, which was viewed more than 3 million times. Experts said the crudely edited clip was more of a “cheap fake” than a deepfake.\\nThen, a pair of artists posted fake footage of Facebook CEO Mark Zuckerberg showing him gloating over his one-man domination of the world. Facebook also left that clip online. The company said at the time that neither video violated its policies.\\nThe problem of altered videos is taking on increasing urgency as experts and lawmakers try to figure out how to prevent deepfakes from being used to interfere with U.S. presidential elections in November.\\nFacebook said any videos that don’t meet existing standards for removal can still be reviewed by independent third-party fact-checkers. Those deemed false will be flagged as such to anyone trying to share or view them, which Bickert said was a better approach than just taking them down.\\n“If we simply removed all manipulated videos flagged by fact-checkers as false, the videos would still be available elsewhere on the internet or social media ecosystem,” Bickert said. “By leaving them up and labelling them as false, we’re providing people with important information and context.”\\nCONTACT US AT EDITORS@TIME.COM.',\n",
              "  array([-0.2658139, -3.3473444, -1.6182745], dtype=float32),\n",
              "  0),\n",
              " ('Facebook says it\\'s cracking down on deepfakes, manipulated media\\nBrett Molina USA TODAY\\nPublished 10:47 AM EST Jan 7, 2020\\nFacebook says it will remove manipulated media including deepfakes from its platform, as questions linger over how the social network will control the spread of misinformation ahead of another presidential election.\\nOn Monday, Facebook said in a statement it will remove deepfakes if a video has been edited in a way that would \"likely mislead someone into thinking that a subject of the video said words that they did not actually say.\"\\nFacebook said it will also remove manipulated media created by artificial intelligence or machine learning that could look authentic.\\n\"While these videos are still rare on the internet, they present a significant challenge for our industry and society as their use increases,\" Monika Bickert, vice president of global policy management at Facebook, said in a statement.\\nWait, is that video real?: The race against deepfakes and dangers of manipulated recordings\\nFacebook disinformation: What you can do to stop its spread\\nA Facebook employee entering computer code on his laptop.\\nFacebook\\nThe policy would exclude content considered parody or satire, says Facebook, as well as \"video that has been edited solely to omit or change the order of words.\"\\nFacebook\\'s ability to curb the spread of misinformation was scrutinized following the 2016 presidential election after it was revealed Russian agents successfully posted fake political advertisements. \\nLast month, Facebook said it was planning to push back against efforts to spread false information on the 2020 U.S. Census. \\nDeepfakes are particularly concerning because audio and video recordings can be manipulated in a way so altered sound bites or video clips appear like they\\'re legitimate.\\nLast year, Facebook was criticized for failing to take down a video of House Speaker Nancy Pelosi doctored to show her slurring words. Instead, the video was \"downranked\" to prevent it from spreading.\\nContributing: The Associated Press. Follow Brett Molina on Twitter: @brettmolina23.\\nPublished 10:47 AM EST Jan 7, 2020',\n",
              "  array([-0.1284716, -3.4594858, -2.4178221], dtype=float32),\n",
              "  0),\n",
              " ('While experts worry about AI technologies like intrusive surveillance and autonomous weaponry, the US government is advocating a hands-off approach to AI’s regulation.\\nThe White House today unveiled 10 principles that federal agencies should consider when devising laws and rules for the use of artificial intelligence in the private sector, but stressed that a key concern was limiting regulatory “overreach.”\\nThe public will have 90 days to submit feedback on the plans, reports Wired, after which federal agencies will have 180 days to work out how to implement the principles.\\nAGENCIES SHOULD ENCOURAGE “FAIRNESS, NON-DISCRIMINATION, SAFETY, AND SECURITY”\\nAny regulation devised by agencies should aim to encourage qualities like “fairness, non-discrimination, openness, transparency, safety, and security,” says the Office of Science and Technology Policy (OSTP). But the introduction of any new rules should also be preceded by “risk assessment and cost-benefit analyses,” and must incorporate “scientific evidence and feedback from the American public.”\\nThe OSTP urged other nations to follow its lead, an uncertain prospect at a time when major international bodies like the Organisation for Economic Co-operation and Development (OECD), G7, and EU are considering more regulation for AI.\\n“Europe and our allies should avoid heavy handed innovation-killing models, and instead consider a similar regulatory approach,” said the OSTP. “The best way to counter authoritarian uses of AI is to make sure America and our international partners remain the global hubs of innovation, shaping the evolution of technology in a manner consistent with our common values.”\\nMichael Kratsios, chief technology officer of the United States, will formally announce the principles at CES on Wednesday. In a call with reporters yesterday, an administration official said that the US was already over-regulating AI technologies at “state and local levels.”\\nExperts are worried about the spread of AI surveillance using technology like facial recognition. Photo by Rolf Vennenbernd / picture alliance via Getty Images\\n“[W]hen particular states and localities make decisions like banning facial recognition across the board, you end up in tricky situations where civil servants may be breaking the law when they try to unlock their government-issued phone,” said an administration official, according to a report from VentureBeat.\\nThe announcement of these 10 principles follows an agenda set by the White House in February 2019 when it launched the “American AI Initiative.” But at a time when experts warn about the damaging effects of unaccountable AI systems in government domains like housing and health care, and when the rise of facial recognition systems seem to be eroding civil liberties, many may question the wisdom of the White House’s approach.\\nAnd while the US follows a laissez-faire attitude domestically, it is also introducing new restrictions on the international stage. This Monday a new export ban came into place, forbidding US companies from selling software abroad that uses AI to analyze satellite imagery without a license. The ban is widely seen as America’s attempt to counter the rise of tech rivals like China, which is quickly catching up to the US in its development of AI.',\n",
              "  array([-0.09535214, -2.6058235 , -4.068385  ], dtype=float32),\n",
              "  0),\n",
              " ('Samsung’s CES 2020 press conference on Monday featured robots, exoskeletons, AI, augmented reality, smart cities, and basically every other technology category you could think of.\\nThe star of the show was Ballie, a robotic sphere with a passing resemblance to a volleyball. It’s fused with Samsung’s artificial intelligence tech, meaning it can identify people, stream live video, and operate various smart home gadgets like lighting, smart TVs, and even a Roomba. It’s somewhere between a Star Wars droid or perhaps real-life company Sphero’s very own discontinued BB-8 toy and the ill-fated Cozmo robot from defunct startup Anki. Hopefully, Samsung’s Ballie fares better than those two droids.\\nFrom there, Samsung’s press conference veered off into a meandering series of tech demos and high-level concepts, including an augmented reality demo featuring its GEMS exoskeleton from last year’s CES and new AR glasses that combine with a virtual personal trainer for full-body fitness applications. There were also discussions on “smart kitchen” concepts, talk of smart cities, and new cutting-edge firefighter and health care tech. It’s a lot to take in, but thankfully, we’ve broken out the most interesting moments in the video above.',\n",
              "  array([-4.8963237 , -4.2687693 , -0.02170686], dtype=float32),\n",
              "  2),\n",
              " ('Delta CEO unveils cool new travel tech at CES, but free Wi-Fi didn\\'t make the cut\\nDawn Gilbertson USA TODAY\\nPublished 1:38 PM EST Jan 7, 2020\\nLAS VEGAS – A futuristic message board that delivers tailored airport information to travelers, in their own language, with a scan of a boarding pass.\\nNew app alerts that let you know when your group, not just your flight, is boarding so you\\'re not glued to the gate.\\nIn-flight movies you can start on your way to the airport, and a new binge button on seatback screens.\\nDelta Air Lines CEO Ed Bastian, the first airline CEO to deliver a keynote address at the mammoth tech trade show CES, took advantage of the spotlight Tuesday and unveiled a flurry of tech features the airline says will smooth travel in the future.\\nSome, like the more detailed boarding alerts, will be introduced this month, others, including the parallel reality airport screens, will only be tested this year, using a small sample of travelers in Detroit. Many more features Bastian touted, including a permanent bag tag with GPS and seamless security screening with biometrics at every step of the travel journey, are farther out.\\nAll are designed, the airline said in a video at CES, \"to make sure traveling isn\\'t something that stresses you but something that delights you.\\'\\' \\nOne thing on travelers\\' minds not rolled out at CES: a timetable for free Wi-Fi on Delta. Bastian reiterated that free Wi-Fi, which Delta has been publicly floating since late 2018, remains Delta\\'s goal and said in his keynote address Tuesday that the airline has a team working “nonstop” to add bandwidth and make its Wi-Fi faster ahead of offering it for free. He said he’s confident they can do that “in the next couple years.”\\nThe airline did a limited test of free Wi-Fi in May. In an interview with Bloomberg television last fall, Bastian said offering free Wi-Fi comes with a lot of technological challenges, including travelers\\' crashing the system. JetBlue Airways is the only  U.S. airline offering free Wi-Fi. Budget airlines don\\'t even offer Wi-Fi, though Spirit plans to roll it out.\\nAlso at CES: Amazon is planning to bring movies and video to cars with Fire TV\\nWhat\\'s an airline CEO doing at a consumer electronics show? \\nDelta doesn\\'t make 8K TVs, flying cars, sex toys or other high-tech wares being peddled or promised at CES this year. It sells travel.\\nBut like so many businesses today, technology underpins travel. When Bastian was announced as a keynote speaker in September, after serving on a panel about technology in travel at last year\\'s show, the head of the group that puts on CES said attendees would for the first time hear about the promise technology holds for the travel and tourism industry.\\nCES 2020: But what happened to 2019\\'s hits?\\n“Biometrics, AR/VR, mobile technology and more are simplifying travel today and fundamentally changing travel in the future,” Gary Shapiro,president and CEO of the Consumer Technology Association said.\\nVeteran travel industry analyst Henry Harteveldt of Atmosphere Research Group said Delta is a good choice to showcase the promise of technology for travelers because it\\'s been a  leader among U.S. airlines in things like mobile app features from flight rebooking to bag tracking.\\nGot a question or complaint about your Delta flight? Text the airline\\n\"I think Delta has a unique credibility to deliver this message,\\'\\' he said.\\nBut the key is execution, Harteveldt said.\\nBastian assured the CES audience that the airline up to the task. “We’re dedicated to solving your travel problems and making your journeys and your lives easier and less stressful.\"\\nDelta is admittedly going on the offense after playing defense on the technology front for the past few years, making sure it had the technology in place to ensure a reliable operation and infrastructure, Bastian said at the airline\\'s investor day in December, teasing his CES presentation.\\n\"We could talk about technology for hours,\\'\\' Gil West, Delta\\'s Chief Operating Officer, said at the investor day.\\n4 tech features Delta touted at CES –and when travelers might see them  \\n1. A souped-up mobile app. Delta is billing its Fly Delta app, which 60% of its travelers use when they fly, as a \"digital concierge.\" In addition to the more detailed boarding alerts at the airport coming later this month and increased tie-ins with ride-hailing company Lyft, the airline has long-term visions of personalized service, including real-time information about weather and traffic on the day of travel. A video shown at CES shows a traveler getting an alert that their Lyft ride is going to arrive earlier because of heavy traffic.\\n\"We\\'ll intuitively coordinate your travel day using real-time data connecting ground transportation and other travel arrangements with your flight based on factors like weather and traffic,\\'\\' the airline said.\\nHarteveldt said the app alert letting travelers know when their group is boarding should smooth boarding if people pay attention and only line up when it\\'s their turn. \\n\"Maybe that helps to better manage the flow of travelers and reduce the bottlenecks that occur at the gate during boarding,\\'\\' he said.\\nBastian said the general your-flight-is boarding alert is “helpful, but only to a point.” The alert about boarding “means there’s no need to cluster at the gate. You know who you are,” he said to CES attendees.\\nAnd for travelers who like to hang out at an airport lounge or bar before their flight, it has the potential to let them linger longer.\\nThe bottom line: Delta wants the app to be more than a place to check in for your flight or check your SkyMiles frequent flyer balance.\\n2. Parallel reality at the airport: where the electronic information you see, such as gate directions and lounge information, is relevant to you. \\nDelta, in partnership with Misapplied Sciences, will start small, with a test at Detroit Metro Airport involving nearly 100 travelers this summer. The volunteer testers will swipe their boarding pass at the parallel reality display post security in Concourse A, near the Delta Sky Club lounge. Each will see personalized messages as they walk past the same screen. \\nBastian said it looks like science fiction, “but it’s actually closer than you think.”\\nDelta rolled out new tech at CES in Las Vegas Jan. 7, 2020.\\nChristian Purdie, Delta\\nHarteveldt said this is one of the \"coolest\" things Delta announced at CES and said it has great potential if executed well. He likened it to something out of \"Minority Report,\\'\\' the 2002 sci-fi flick from Steven Spielberg and starring Tom Cruise.\\n\"There\\'s all these ways where Delta can use this messaging in a way to contribute to the (travel) journey by answering questions before the customer asks,\\'\\' he said.\\n3. In-flight movies and shows on the go, plus a binge-watching button.\\nDelta, which has seatback screens on more than 700 planes, more than any U.S. carrier, says it is  \"doubling down\" on in-flight entertainment. The airline plans to enable travelers to start their free in-flight entertainment on the Delta app as soon as they check in for their flight and finish it on the plane, using seatback screens or the app.\\nWhat\\'s coming when is unclear, but the airline said it will take the first step this year by testing a binge button on seatback screens so travelers can stream TV series like they do at home and a feature that will recommend shows and movies based on a passengers\\' previous entertainment picks.\\nHarteveldt says passengers generally don\\'t pick their airline based on in-flight entertainment but that a good lineup is always a plus and can boost passenger satisfaction.\\n\"if you are enthralled by watching the latest movie ... or binge watching you favorite TV program, it may take your mind off the fact that you can\\'t feel your ankles because the legroom is so tight.\\'\\'\\n4. Artificial intelligence to the rescue during bad weather.\\nBeginning this spring, Delta will add artificial intelligence to its lineup of tools designed to help the airline make better decisions during storms and other bad weather.\\nThe goal: inconvenience fewer travelers and airline crews.\\nDelta says machine learning will use historical data to \"nimbly\" simulate operating challenges. It says it will be the largest such simulation by a U.S. airline.\\nDelta unveiled new technology at CES in Las Vegas Jan. 7, 2020.\\nChristian Purdie, Delta\\nDelta COO West said in December that so-called decision science tools can do what humans can\\'t: \"play it forward.\"\\n\"How many flight do we need to proactively cancel so that we can recover quickerand make those up on the back end?\"\\nHarteveldt said the AI tool is one of the most promising, if non-sexy, tech moves Delta is making.\\n\"It has the potential to move the customer really close to the center of the decision making,\\'\\' he said.\\nPublished 1:38 PM EST Jan 7, 2020',\n",
              "  array([-4.3332977 , -3.5676095 , -0.04222653], dtype=float32),\n",
              "  2),\n",
              " (\"This is the web version of Eye on A.I., Fortune’s weekly newsletter covering artificial intelligence and business. To get it delivered weekly to your in-box, sign up here.\\nHappy New Year to all of you Eye on A.I. readers! \\nIf 2019 was any barometer, this year will be a busy one in the artificial intelligence world. The challenge will be to turn the advancements into something that businesses can use.\\nMost companies are still struggling to harness A.I. technologies like neural networks, or software that learns from data, according to surveys that Fortune covered last year. I believe this dilemma will continue in 2020, due in part to the lack of available A.I. talent that can build and maintain these complicated systems.\\nDespite the potential A.I. setbacks, businesses will continue spending big money on A.I. in the hope that their investments will eventually pay off. There's just too much at stake (and too much hype) to ignore it.\\nHere's my predictions for what to expect in A.I. in 2020:\\n1. The regulators are coming\\nFor several years, lawmakers have been talking about regulating A.I.—but it will finally happen in 2020. For instance, on Tuesday, the White House introduced guidelines that federal agencies like the Federal Drug Administration and Department of Transportation must adhere to when crafting A.I. regulations, like whether their rules ensure that A.I. is “fair” to everyone. The principles, while vague, are a precursor to separate and more formal regulations that could impact how businesses use specific A.I. technologies like facial recognition.    \\nEven if federal A.I. rules fail to be enacted in 2020, local and state governments will continue to fill in the regulatory gaps, as cities like San Francisco have done for the use of facial recognition by law enforcement.  \\n2. A.I. chips go mainstream\\nAfter years of internal development and trials, you can expect startups to debut specialized computer chips for A.I. tasks like data training. Companies are increasingly seeking A.I. chips that consume less energy and that can crunch data more quickly. Also, startups may begin to challenge Nvidia, the leading maker of computer chips used to train A.I. systems.\\n3. Cashierless stores will debut worldwide\\nFearing an increasing number of Amazon Go cashierless stores, retailers will debut their own versions. But they'll likely lose money on these in the short term because the technology required to operate them is expensive.\\n4. China steps up in A.I.\\nChina will play a bigger role in A.I. research in 2020. As Stanford University's AI Index for 2019 shows, A.I. research papers from China are increasingly cited in other academic papers, underscoring their quality and importance. Jack Clark, an AI Index author and policy director of A.I. research group OpenAI, said Monday during an online presentation about the Stanford report that lawmakers believe that “the Chinese can’t invent things, they just steal things.”\\n“I sit there as someone who reads these research papers, and no, that’s very wrong,” Clark said. \\nJonathan Vanian \\n@JonathanVanian\\njonathan.vanian@fortune.com\\nA.I. IN THE NEWS\\nAre A.I. regulations coming? Over the next 18 months, the U.S. and Europe may introduce regulations impacting how businesses use A.I., the Wall Street Journal reported. The article included a statement by European Commission President Ursula von der Leyen, who said, “In my first 100 days in office, I will put forward legislation for a coordinated European approach on the human and ethical implications of artificial intelligence.”\\nDon’t send this A.I. software overseas. The U.S. government introduced a new rule intended to prevent U.S. companies from exporting certain kinds of A.I. software that could benefit “adversaries like China,” Reuters reported. The report said that companies must apply for a license to export certain geospatial imagery software from the United States to other countries except Canada.\\nA.I. hype hits healthcare. Kaiser Health News examined the burgeoning use of A.I. in healthcare and found that “many health industry experts fear AI-based products won’t be able to match the hype.” Among some of the problems the articles explores includes the fact that many health-tech startups do not publish their research in peer-reviewed journals, which means outside experts are unable to validate company claims.\\nA sad pizza party. Zume, a startup specializing in robots that make pizza, could lay off 400 employees as it aims to raise more money, Business Insider reported, citing unnamed sources. The startup has faced a number of setbacks and has attempted to move into new areas like “compostable packaging and food-truck services,” the report said.\\nEYE ON A.I. TALENT\\nGroupon hired John Higginson to become CTO of the online deals and coupons company. Higginson was previously the CTO of the financial technology company Enova.\\nThe Iowa Department of Transportation picked Matthew Rensch to be the state’s first chief data officer. Rensch previously worked at companies like Athene USA and Berkley Technology Services, according to the trade publication Government Technology.\\nEYE ON A.I. RESEARCH\\nA.I. that screens for breast cancer. Researchers from various Google units like DeepMind published a paper in Nature detailing an A.I. system that can help doctors more easily identify breast cancer by reading mammograms. In a New York Times article about the Nature paper, an outside expert developing similar A.I. technology said Google’s paper was strong, but cautioned that “the patients studied might not be a true reflection of the general population.”\\nFORTUNE ON A.I.\\nHoliday Superstar Wyze Suffers Data Leak, and Amazon Makes Returns Easy—By Don Reisinger\\nRwanda Is Bringing Tech Buzz to Africa—By Richard Morgan\\nWhy Workplace A.I. Needs a Human Touch—By Gwen Moran\\nBRAIN FOOD\\nWhere’s the U.S.? Wired examines why the United States is not participating in an international consortium called the Global Partnership on AI, whose representatives from counties like Canada and France intend to “study and steer the effects of artificial intelligence on the world’s people and economies.” All of the so-called G7 countries have pledged support to the international group except for the U.S. “The White House says the body is unnecessary bureaucracy that threatens to dampen AI development by being overly cautious,” the article said.\",\n",
              "  array([-4.181342  , -3.5630012 , -0.04461203], dtype=float32),\n",
              "  2),\n",
              " ('For months, Facebook has struggled with deceptively altered videos, but late Monday evening, the company cracked, announcing that it would ban deepfakes as part of a new policy on manipulated media. But while Facebook’s new policy bans many of the most egregious examples of manipulation, the new push has been bogged down by concerns over the new rules’ limitations and some unusual confusion about exactly what the policy covers.\\nMost ominously, the new policy has been met with harsh criticism from members of Congress — the very people Facebook was hoping to impress ahead of a congressional hearing about online deception that’s scheduled for Wednesday morning.\\n“THE REAL PROBLEM IS FACEBOOK’S REFUSAL TO STOP THE SPREAD OF DISINFORMATION.”\\nIn the initial blog post outlining the change, Facebook’s vice president of global policy management, Monika Bickert, said videos that have been edited “in ways that aren’t apparent to an average person and would likely mislead someone” and were created by artificial intelligence or machine learning algorithms would be removed under the new policy. Yet, two of the most hotly criticized altered videos — one edited to make House Speaker Nancy Pelosi sound drunk and another clipping a statement from former Vice President Joe Biden, suggesting he made racist remarks — were likely created using widely available editing software similar to iMovie or Photoshop.\\nBecause those videos weren’t edited to make Biden or Pelosi say new words, they probably wouldn’t be covered by Facebook’s new policy. If Facebook’s fact-checkers reported them as false or misleading, the company could still add links to news articles debunking them, but it would likely leave them up. That’s what happened in the case of the Pelosi video last November, and it looks to be the company’s plan for future cases.\\nNot surprisingly, many members of Pelosi’s staff are unimpressed with the new rules. In a statement to The Verge responding to the updated policy, Pelosi’s deputy chief of staff Drew Hammill acknowledged the change, but said, “Facebook wants you to think the problem is video-editing technology, but the real problem is Facebook’s refusal to stop the spread of disinformation.”\\nEven Biden’s campaign railed Facebook for doing the bare minimum when it comes to fighting disinformation online. Biden’s campaign spokesman, Bill Russo, echoed the Pelosi team’s statement, saying, “Facebook’s announcement today is not a policy meant to fix the very real problem of disinformation that is undermining face in our electoral process, but is instead an illusion of progress.”\\n“Banning deepfakes should be an incredibly low floor in combating disinformation,” Russo continued.\\n“BANNING DEEPFAKES SHOULD BE AN INCREDIBLY LOW FLOOR IN COMBATING DISINFORMATION.”\\nIn the hours after the new policy was announced, even Facebook officials seemed confused about whether the company was banning deepfakes in political ads. After some flip-flopping with reporters over whether politicians could pay to promote deepfakes, Facebook landed on prohibiting them from being used in advertising. “Whether posted by a politician or anyone else, we do not permit manipulated media content in ads,” a Facebook spokesperson told The Verge.\\nThis decision is a slight reversal of what Facebook and its CEO Mark Zuckerberg said last year about moderating political speech. Last October, President Donald Trump’s reelection campaign ran an ad on Facebook making misleading claims about Joe Biden and his son Hunter’s relationship with the Ukrainian government. That ad set off a debate over whether social media platforms like Facebook and Twitter should allow politicians to lie in digital advertisements. Weeks after the false Biden ad was posted, Twitter announced that it would be banning all political advertisements. Facebook didn’t announce any changes, decidedly allowing politicians across the globe to unjustifiably smear their opponents online.\\nI shared inaccurate information earlier. I sincerely regret the error and need to correct the record. With regard to our new policy, whether posted by a politician or anyone else, we do NOT permit in ads manipulated media content, as defined here: https://t.co/CAtmBPczlG\\n— Andy Stone (@andymstone) January 7, 2020\\nThat decision to allow politicians to say whatever they want in ads started to chip away on Tuesday after Facebook said it wouldn’t allow deepfakes anywhere on its platform. But since 2016, Facebook has given itself a newsworthiness exemption for deciding whether to remove posts that violate its community standards.\\nAccording to a September blog post, Facebook’s newsworthiness exemption doesn’t apply to advertising, but feed posts. This could give politicians some wiggle room when it comes to posting manipulated videos. If Facebook deems a future deepfake or shallowfake as “newsworthy” it could be left up for people to like and share across the platform.\\nOne thing’s for sure: lawmakers are paying attention. Tomorrow, the House Energy and Commerce Committee will be holding a hearing on deepfakes and synthetic media. Bickert, who authored Monday’s blog post, will be representing Facebook and taking questions from lawmakers.\\n“As with any new policy, it will be vital to see how it is implemented and particularly whether Facebook can effectively detect deepfakes at the speed and scale required to prevent them from going viral,” House Intelligence Committee Chairman Adam Schiff (D-CA) said in a statement on Tuesday.',\n",
              "  array([-0.09342655, -2.666763  , -3.9262035 ], dtype=float32),\n",
              "  0),\n",
              " ('(Corrects to ‘improving’ in paragraph 3)\\nBy David Shepardson\\nLAS VEGAS, Jan 7 (Reuters) - Delta Air Lines Chief Executive Ed Bastian said on Tuesday the company plans to use technology to shift employees away from rote tasks and improve the flying experience for passengers.\\nDelta took center stage at Tuesday’s opening event at the annual four-day CES technology conference - a spot typically held by an automaker or traditional technology companies.\\nBastian offered a litany of technology improvements aimed at improving the sometimes unpleasant experience of flying.\\n“Our people should not spending their time taking tickets and scanning boarding passes. They’re way too talented for that,” Bastian said at the CES technology forum in Las Vegas. He said the airline’s goal is to get “Delta people from out behind the counter so they can assist you, solve your problems in real time.”\\nSurveys show price is the top factor when most passengers buy tickets and as a result many airlines have increased charges for services that were previously included in the lowest ticket prices, like picking seats or carrying on a bag.\\nThe Atlanta-based airline said it will launch “virtual queuing” later this month with its app “to notify customers when their seat – not just their flight – is boarding,” aimed at preventing lengthy lines.\\nDelta said it is using artificial intelligence and machine learning to try to reduce delays. Its AI-platform “analyzes millions of operational data points - from aircraft positions to flight crew restrictions to airport conditions - to create hypothetical outcomes.”\\nDelta also wants passengers to be able to start a movie on one flight and pickup where they left off on the next flight. Delta will test a feature like a “binge button” that would allow a passenger to watch an entire session of a TV show and pilot a program that will recommend future viewing based on prior viewing behavior.\\nBastian said research shows people are more likely to cry when watching a movie in the air versus on the ground because they are in an “elevated emotional state.” Delta said its research shows entertainment can play a key role in helping customers relax while traveling.\\nDelta also plans to test “do not disturb” or “wake me for meal service” features for coach passengers on some longer flights.',\n",
              "  array([-4.457538 , -3.6661062, -0.037875 ], dtype=float32),\n",
              "  2),\n",
              " ('LAS VEGAS (Reuters) - Delta Air Lines Chief Executive Ed Bastian said on Tuesday the company plans to use technology to shift employees away from rote tasks and improve the flying experience for passengers.\\nFILE PHOTO: A Delta Air Lines flight is pushed put of its gate at the airport in Salt Lake City, Utah, U.S., January 12, 2018. REUTERS/Mike Blake\\nDelta took center stage at Tuesday’s opening event at the annual four-day CES technology conference - a spot typically held by an automaker or traditional technology companies.\\nBastian offered a litany of technology improvements aimed at improving the sometimes unpleasant experience of flying. He said Delta has invested billions in technology upgrades over the last five years, including real-time bag tracking.\\n“Our people should not be spending their time taking tickets and scanning boarding passes. They’re way too talented for that,” Bastian said at the CES technology forum in Las Vegas. He said the airline’s goal is to get “Delta people from out behind the counter so they can assist you, solve your problems in real time.”\\nDelta is working with start-up companies, including one company to keep airplane cabins clean that uses antimicrobial lights and another start-up to help transport and monitor pets.\\nSurveys show price is the top factor when most passengers buy tickets and as a result many airlines have increased charges for services that were previously included in the lowest ticket prices, like picking seats or carrying on a bag.\\nThe Atlanta-based airline said it will launch “virtual queuing” later this month with its app “to notify customers when their seat – not just their flight – is boarding,” aimed at preventing lengthy lines.\\nDelta said it is using artificial intelligence and machine learning to try to reduce delays. Its AI-platform “analyzes millions of operational data points - from aircraft positions to flight crew restrictions to airport conditions - to create hypothetical outcomes.”\\nDelta also wants passengers to be able to start a movie on one flight and pick up where they left off on the next flight. Delta will test a feature like a “binge button” that would allow a passenger to watch an entire session of a TV show and pilot a program that will recommend future viewing based on prior viewing behavior.\\nBastian said research shows people are more likely to cry when watching a movie in the air versus on the ground because they are in an “elevated emotional state.” Delta said its research shows entertainment can play a key role in helping customers relax while traveling.\\nDelta also plans to test “do not disturb” or “wake me for meal service” features for coach passengers on some longer flights.',\n",
              "  array([-4.822992  , -3.6056507 , -0.03584733], dtype=float32),\n",
              "  2),\n",
              " ('\"Artificial humans\" - virtual characters - have been shown off by Samsung-backed start-up Neon at the CES tech show in Las Vegas.\\nNeon says it intends its virtual characters to act like digital \"friends\".\\nHowever, one tech industry analyst told the BBC the demonstration failed to impress him.\\nBen Wood of CCS Insight said: \"It was not the revolution that I was expecting.\"\\nThere had been great interest in Neon after the California-based outfit ran a viral teaser campaign across social media in the lead-up to the expo.\\n\\nVisitors to Neon\\'s CES booth can pose with the digital avatars\\nReddit users subsequently found links to videos of the characters hidden on the firm\\'s website.\\nThose have since been removed, but Neon has been showcasing some of its life-size \"artificial humans\" to CES attendees.\\nIvanka Trump unfazed by critics at tech show\\n\\'Reverse microwave oven\\' cools drinks in seconds\\nThe electric bike that rides on water\\nIts show exhibit features a row of large screens, on which the animated characters are displayed.\\nReport\\n\\'High expectations\\'\\nNeon\\'s chief executive Pranav Mistry claimed the digital avatars represent a new life-form.\\n\"There are millions of species on our planet and we hope to add one more,\" Mr Mistry told the press.\\n\\nOne of the animations plays music to passersby while another does magic\\nHe added that they have the ability to show emotions and intelligence, and can speak a wide range of languages.\\nBut Mr Wood said the virtual beings had the appearance of being little more than short video clips of real people.\\n\"They could get people to shake their head or do a selfie pose or whatever but that\\'s the sort of thing you could pre-program in a video of an actor,\" said Mr Wood.\\n\"Expectations were exceedingly high. On visiting, it was hard to get excited at this stage.\"\\n\\nNeon had run a series of teaser images in advance of CES\\nLarry Dignan at news site ZDNet also had reservations.\\nHe wrote that Neon\\'s creations might be useful if deployed in public to greet shoppers or tourists.\\nBut he added that giving them a \"brain\" would be a bigger challenge than making them \"picture-perfect\".\\nNeon has not yet revealed where the virtual characters will first appear in public.\\nCatch up with all the BBC\\'s CES 2020 coverage',\n",
              "  array([-4.015389  , -3.745835  , -0.04254418], dtype=float32),\n",
              "  2),\n",
              " (\"Wacky concept cars, flying machines and smart bikes are being exhibited at the CES tech show in Las Vegas.\\nIn recent years, car tech has become an increasingly important staple of the expo as firms seek the next mega-product after the smartphone.\\nChip-makers and app-creators are all piling in. But the traditional car-makers are not resting on their laurels.\\nThe efforts go far beyond new sensors for driverless vehicles - this year there are a host of flamboyant designs for entire vehicles on show.\\nHonda, for one, is allowing attendees to sit in its bulbous, buggy-like concept car with minimalistic driving controls.\\nIt has no pedals and a disc-shaped steering wheel that is pushed or pulled like a giant button in order to control acceleration.\\nFiat is exhibiting a version of its Centoventi modular concept car.\\nBuyers can, in theory, mix and match the components.\\nThis autonomous concept car from Audi has a retractable desk at the driver's seat.\\nUseful for a game of bridge with your passenger?\\nNot all the cars on show are mere concepts.\\nFord's Mustang Mach E GT, below, is an electric vehicle set to be released by the end of the year.\\nIt has a range of around 250 miles (402km) on a full charge and will cost $60,500 (£46,000).\\nThere's also been some surprise appearances.\\nSony, the electronics giant, startled CES by unveiling a whole concept car of its own called the Vision S.\\nIts interior is crammed with entertainment tech, including a panoramic screen next to the dashboard.\\nAnother car with a sizeable dashboard display is the Byton M-Byte electric vehicle.\\nDuring a presentation, the Chinese start-up emphasised its idea that the car of the future would run on data power, not just horsepower.\\nThe M-Byte is due to go on sale later this year.\\nReport\\nAmerican-Danish entrepreneur Henrik Fisker unveiled an electric sport utility vehicle, the Fisker Ocean, which he plans to make commercially available by early 2022.\\nIt will cost $37,000 and have solar panels on its roof, he told attendees.\\nToyota is displaying its LQ Level 4 concept car, which is designed for automated driving.\\nIt also has a built-in artificial intelligence assistant of its own called Yui.\\nA number of flying machine concepts are also on show.\\nPerhaps the most talked about is Hyundai's S-A1 electric Urban Air Mobility concept vehicle.\\nHyundai announced that it had entered a partnership with ride-hailing giant Uber and is seeking to manufacture flying taxis for the firm in the future.\\nThe SA-1 is designed to reach speeds of 180mph.\\nLast but not least, Cybic displayed its folding e-bike - with built-in Alexa voice assistant.\\nIt allows riders to speak to Amazon's virtual assistant on the go, such as asking for directions or information about traffic and the weather.\",\n",
              "  array([-4.909725  , -3.6261826 , -0.03458324], dtype=float32),\n",
              "  2),\n",
              " (\"TOKYO (Reuters) - Japanese fashion tycoon Yusaku Maezawa is giving away $9 million to his Twitter followers in what he says is a “social experiment” to see if the payment boosts their happiness.\\nMaezawa will give 1 million yen ($9,000) to 1,000 followers selected at random from those who retweeted a Jan. 1 post, with the impact of the money to be tracked through regular surveys.\\n“It’s a serious social experiment,” said Maezawa on YouTube, adding he hopes to attract interest from academics and economists.\\nMaezawa, who is to be the first private passenger to fly around the moon with Elon Musk’s SpaceX, is known for his high spending on art and sports cars but also has a predilection for musing on ideas like a world without money.\\nHe tied the giveaway to the idea of basic income, or the theory of providing a periodic no-strings-attached payment to all citizens, that has gained traction in some political circles and is backed by Democratic U.S. presidential candidate Andrew Yang.\\n“Basic means a regular minimum amount offering a sense of security, what Maezawa is offering is totally different,” said Toshihiro Nagahama, senior economist at Dai-ichi Life Research Institute.\\nMaezawa said that given that he “has the money and free time” to make the payments, he felt the need to try and inspire greater debate over the merits of the theory in Japan.\\nThe idea of a universal basic income has gained support over fears technology such as artificial intelligence will wipe out large numbers of jobs but that concern is for now less pronounced in Japan with its tight labor market, said Nagahama.\\nIts the second, larger, giveaway by the entrepreneur, who in November secured a $900 million payday through the sale of his online fashion business Zozo Inc (3092.T) to SoftBank Group Corp (9984.T).\\nMaezawa, who recently grabbed headlines after his split from actress girlfriend Ayame Goriki, has gathered almost 7 million followers on Twitter with his mix of displays of conspicuous consumption and folksy pronouncements on the meaning of life.\\nFILE PHOTO: Japanese billionaire Yusaku Maezawa, founder and chief executive of online fashion retailer Zozo, who has been chosen as the first private passenger by SpaceX, poses for photos as he attends a news conference at the Foreign Correspondents' Club of Japan in Tokyo, Japan, October 9, 2018. REUTERS/Toru Hanai/File Photo\\nYouTube is the latest online outlet for the businessman, with videos including a tour of his private jet, a visit to the barber to dye his hair and updating his bank book after November’s windfall.\\nThe debate over basic income comes as income inequality continues to grow in the United States, where in recent years some of its wealthiest entrepreneurs, from Microsoft (MSFT.O) co-founder Bill Gates to investor Warren Buffett, have pledged to give away most of their wealth.\\n(The story corrects to read ‘Buffett’ in last paragraph)\\nEditing by Jacqueline Wong\\nOur Standards:The Thomson Reuters Trust Principles\",\n",
              "  array([-3.9658856 , -2.9341688 , -0.07485946], dtype=float32),\n",
              "  2),\n",
              " ('TEL AVIV (Reuters) - Israeli private high-tech companies raised $8.3 billion in 2019, up 30% from 2018, the Israel Venture Capital Research Center and ZAG law firm said on Wednesday.\\nThis brings to $39.1 billion the amount Israeli tech firms raised since 2010.\\n“2019 marked a record year, capping a decade of successive increases in capital invested in the Israeli high-tech industry,” ZAG managing partner Shmulik Zysman said. “This growth is partly due to the growing foreign capital invested in the Israeli high-tech industry.”\\nIn the fourth quarter, high-tech firms raised $2.3 billion, the highest since 2012, though the number of deals declined to 122, the IVC-Zag report showed.\\nCapital raised in early-stage companies declined in 2019 with seed round amounts shrinking to $148 million from $169 million in 2018.\\nIVC forecast that allocated capital for more mature companies will continue to grow in 2020, barring a dramatic change in macroeconomic conditions. Artificial intelligence and cybersecurity companies will remain the most attractive for investors.',\n",
              "  array([-4.774677  , -3.2692354 , -0.04759095], dtype=float32),\n",
              "  2),\n",
              " ('We’re used to social networks waiting until the damage has already been done before announcing a cleanup effort. When it comes to the synthetic media known as “deepfakes,” they’ve been notably ahead of the curve. In November, Twitter announced a draft policy on deepfakes and began soliciting public input. And on Monday night, Facebook announced that it would ban certain manipulated photos and videos from the platform. Here’s the blog post from Monika Bickert, Facebook’s vice president of global policy management:\\nGoing forward, we will remove misleading manipulated media if it meets the following criteria:\\n- It has been edited or synthesized – beyond adjustments for clarity or quality – in ways that aren’t apparent to an average person and would likely mislead someone into thinking that a subject of the video said words that they did not actually say. And:\\n- It is the product of artificial intelligence or machine learning that merges, replaces or superimposes content onto a video, making it appear to be authentic.\\nThis policy does not extend to content that is parody or satire, or video that has been edited solely to omit or change the order of words.\\nThe move comes ahead of a planned hearing Wednesday about misinformation at which Bickert is scheduled to speak.\\nThe change represents a significant step forward at a time where anxieties over deepfakes, and their potential role in shaping the 2020 election, are running high. The technology is improving at a steady clip: see these companies selling synthetic (but convincing) people to populate dating apps. It’s not hard to imagine an unscrupulous campaign posting synthetic videos on Facebook or Instagram of their opponent saying or doing something they didn’t. As of today, that’s officially against policy.\\nNotably — and contra to what Facebook initially said — the policy will apply to advertisements as well as regular posts. Create a phony video of your opponent clubbing a baby seal and Facebook will make (yet another) exception to its policy against fact-checking political speech in advertisements, and remove anything found to be fake.\\nStill, some doubts lingered. Nina Jankowicz, who has a book coming out this year on Russian disinformation operations, said she is “still more worried about cheap fakes than deep fakes. Crudely edited, deliberately misleading videos and images are still effective, and they’re still allowed on most platforms.”\\nWhat’s a cheap fake? Something like this video of campaign workers doing a corny dance in support of presidential candidate Michael Bloomberg. In reality, they aren’t campaign workers at all — they’re audience members at an improv show filming a bit for a comedian, who shared it on a Twitter profile he had edited to make it appear as if he worked for Bloomberg. The ruse was exposed relatively quickly, but plenty of people still fell for it.\\nThere are all sorts of ways to trick people like this. You can also grab an old video and put a new date on it, or just tweet it as if it’s brand new. You can Photoshop. You don’t need a state-of-the-art media lab to wreak havoc. That’s one reason why, even as the technology improved, information operations haven’t yet seemed very interested in deepfakes, as my colleague Russell Brandom wrote last year. “Uploading an algorithmically doctored video is likely to attract attention from automated filters, while conventional film editing and obvious lies won’t,” Brandom wrote. “Why take the risk?”\\nThere’s one last workaround to Facebook’s new rule: comedy. For good reason, Facebook permits people to post satire and parody. Unfortunately, this rule is often exploited by fake-news purveyors and other sites adept at straddling the line between comedy and misinformation. Last week, in the wake of the military strike in Iran, an article titled “Democrats Call For Flags To Be Flown At Half-Mast To Grieve Death Of Soleimani” was posted to a site called the Babylon Bee. From there, it was shared more than 660,000 times on Facebook.\\nSurely some of the people who shared the article knew that the Babylon Bee is a satirical site. But read the comments in the original Facebook post and you’ll see that just as many seem to believe the article is real. In the flattened design of the News Feed, where every shared article carries equal weight, it can be hard to tell.\\nAll these many asterisks help explain why Democratic politicians seem mostly unimpressed with Facebook’s deepfakes ban. “Facebook wants you to think the problem is video-editing technology, but the real problem is Facebook’s refusal to stop the spread of disinformation,” said a spokeswoman for Rep. Nancy Pelosi, the unwitting star of a famously misleading (though not deepfaked) viral video last year.\\nJoe Biden’s campaign struck a similar note: “Facebook’s policy does not get to the core issue of how their platform is being used to spread disinformation, but rather how professionally that disinformation is created.”\\nStill, Monday’s action doesn’t preclude the company from addressing some of these nuances down the road. And the more we see this sort of thing in 2020, I suspect that it will. In the meantime, one of the big platforms has established at least a partial bulwark against the infocalypse — though its strength will depend entirely on how strongly Facebook defends it. Policy, as ever, is what you enforce.\\nUPDATE\\nBefore the break, I reported here that Pinterest had cut contractors’ vacation benefits, forcing them to work over the holiday if they wanted to be paid during Christmas week. After I published that piece, employees were upset, and the company reversed course. Contractors got their paid week off after all, just like Pinterest’s full-time employees. “We realized our communication of this change may have come too late in the year for people to plan accordingly for this holiday season,” a spokesman told me.”\\nNot much more to say here, other than that journalism is the best job in the world and don’t let nobody ever tell you different.\\nTHE RATIO\\nToday in news that could affect public perception of the big tech platforms.\\nTrending up: Facebook fundraisers have generated $37 million for fire relief in Australia, the company says. Actor Celeste Barber’s fundraiser alone raised $30 million from 1.1 million people, and is now the largest fundraiser in Facebook history.\\nTrending sideways: Facebook is setting up a new engineering team in Singapore to focus on its lucrative China ad business. The news comes as CEO Mark Zuckerberg has ramped up criticism of the country over human-rights issues.\\nTrending sideways: Shares of Google hit an all-time high yesterday, closing out at $1,397.81 per share. Apparently, investors are unfazed by the ongoing antitrust investigation into the company, as well as employee unrest.\\nGOVERNING\\n⭐ The FBI asked Apple to help unlock two iPhones linked to a shooting at Naval Air Station in Pensacola, Florida last month. Apple said it has been cooperating with the government and had already handed over all the data in its possession. Here’s what the company told Pete Williams at NBC:\\n“We have the greatest respect for law enforcement and have always worked cooperatively to help in their investigations,” Apple said in a statement. “When the FBI requested information from us relating to this case a month ago, we gave them all of the data in our possession and we will continue to support them with the data we have available.”\\nA law enforcement official said there’s an additional problem with one of the iPhones thought to belong to Alshamrani, who was killed by a deputy during the attack: He apparently fired a round into the phone, further complicating efforts to unlock it.\\n⭐ A leaked Facebook memo shows longtime executive Andrew “Boz” Bosworth told employees that the company has a moral duty not to tilt the scales against President Trump in the 2020 election. Kevin Roose, Sheera Frenkel and Mike Isaac from The New York Times have the scoop:\\nIn a meandering 2,500-word post, titled “Thoughts for 2020,” Mr. Bosworth weighed in on issues including political polarization, Russian interference and the news media’s treatment of Facebook. He gave a frank assessment of Facebook’s shortcomings in recent years, saying that the company had been “late” to address the issues of data security, misinformation and foreign interference. And he accused the left of overreach, saying that when it came to calling people Nazis, “I think my fellow liberals are a bit too, well, liberal.”\\nBoz then shared the memo in its entirety from his own Facebook page. Read it.\\nThe White House unveiled 10 principles that federal agencies should consider when devising laws and rules for the use of artificial intelligence in the private sector, but stressed that a key concern should be limiting regulatory “overreach.” (James Vincent / The Verge)\\nThe 2020 election is likely the most anticipated event in US history when it comes to digital security. Russia still poses a massive threat, as do Iran and China. Experts are also warning that it’s not just the general election that is at risk — the primaries will be a target, too. (Joseph Marks / The Washington Post)\\nExperts warn that the United States needs to be prepared for cyber retaliation from Iran, which employs different tactics than Russia. Iran has spent years building an online influence apparatus that uses fake websites and articles meant to mimic real news and disappear quickly. (Sara Fischer / Axios)\\nSonos sued Google, seeking financial damages and a ban on the sale of Google’s speakers, smartphones and laptops in the United States. Sonos accused Google of infringing on five of its patents, including technology that lets wireless speakers connect and synchronize with one another. (Jack Nicas and Daisuke Wakabayashi / The New York Times)\\nA researcher dove into the narratives surrounding the death of Qassem Soleimani, a top Iranian commander, on one of Iran’s most popular social media platforms, Telegram.\\nAs Taiwan gears up for a major election this week, officials and researchers worry that China is experimenting with social media manipulation to sway the vote. Voters are already awash in false or highly partisan information, making such tactics easy to hide. (Raymond Zhong / The New York Times)\\nViolence erupted at Jawaharlal Nehru University in India last week, after members of a student group — apparently coordinating through WhatsApp — attacked fellow students and teachers. An investigation into the group revealed who the attackers were and how they coordinated the violence. (Meghnad S, Prateek Goyal and Anukriti Malik / Newslaundry)\\nINDUSTRY\\n⭐ Politicians, parties, and governments, are hiring dark-arts public-relations firms to spread lies and misinformation. One firm promised to “use every tool and take every advantage available in order to change reality according to our client’s wishes.” Craig Silverman, Jane Lytvynenko and William Kung have the story:\\nIf disinformation in 2016 was characterized by Macedonian spammers pushing pro-Trump fake news and Russian trolls running rampant on platforms, 2020 is shaping up to be the year communications pros for hire provide sophisticated online propaganda operations to anyone willing to pay.\\nAlso — the threat isn’t limited to the US:\\nMost recently, in late December, Twitter announced it removed more than 5,000 accounts that it said were part of “a significant state-backed information operation” in Saudi Arabia carried out by marketing firm Smaat. The same day, Facebook announced a takedown of hundreds of accounts, pages, and groups that it found were engaged in “foreign and government interference” on behalf of the government of Georgia. It attributed the operation to Panda, an advertising agency in Georgia, and to the country’s ruling party.\\nAI start-ups are selling pictures of computer-generated faces that appear to be real people. They offer companies a chance to “increase diversity” in their ads without needing human beings. They’ve also signed on dating apps that need more images of women. (Drew Harwell / The Washington Post)\\nThe new trick to going viral on Instagram is making an Instagram filter, as seen by the bewilderingly popular “What Disney Character Are You?” sensation. This story breaks down how it works. (Chris Stokel-Walker / Input)\\nMichelle Obama launched an Instagram video series about students navigating their first year of college. The former First Lady partnered with digital media company ATTN: to launch a video series on IGTV, Instagram’s video platform. (Sara Fischer / Axios)\\nThe CES gadget show in Las Vegas is all-in on surveillance technology, from face scanners that check in some attendees to the cameras-everywhere array of digital products. (Matt O’Brien / Associated Press)\\nAND FINALLY...\\nWoe unto the big tech executive who uses an extended metaphor about Lord of the Rings without checking his facts. Here’s Chaim Gartenberg on the Boz memo:\\nAs part of his argument, Boz makes the comparison by citing none other than J.R.R. Tolkien’s The Lord of the Rings to explain his decision. Facebook, Boz argues, is akin to Sauron’s One Ring, and wielding its power — even with noble intent — would only lead to ruin. [...]\\nIn Tolkien’s books and the film adaptations, Galadriel is concerned about the power of the Ring corrupting her — as it does all, save the Dark Lord himself. But not once does she contemplate using its power for good. “In place of the Dark Lord you will set up a Queen. And I shall not be dark, but beautiful and terrible as the Morning and the Night!.. All shall love me and despair!” Tolkien writes.\\nLater, nerds.\\nTALK TO US\\nSend us tips, comments, questions, and Lord of the Rings analogies casey@theverge.com and zoe@theverge.com.',\n",
              "  array([-1.3810167 , -1.7620894 , -0.54992807], dtype=float32),\n",
              "  2),\n",
              " ('Chinese-owned video-sharing platform TikTok has updated its community guidelines, after a series of questions about how it moderates content.\\nThe firm has faced criticism for removing some posts, particularly those related to politics.\\nTikTok said the new guidelines provide more detail and clarity.\\nThe changes come after it was forced to fix serious security issues which could have allowed hackers to alter videos and other content.\\nIn a blogpost, TikTok said it was \"an inclusive platform built upon the foundation of creative expression\".\\n\"Our community is diverse and global, and we aim to cultivate an environment for authentic interactions,\" wrote authors Lavanya Mahendran and Nasser Alsherif, from the firm\\'s global trust and safety team.\\nChanges include:\\nclarifying that educational, historical, satirical, artistic content that can be clearly identified as such, or that raises awareness, will be allowed in the section on terrorist groups and other dangerous organisations\\nthe section on misinformation has been expanded to include manipulated content or fake news around elections\\nviolations are grouped into 10 distinct categories, each including an explanation of the rationale to clarify the type of misbehaviour that would led to posts being removed\\nAll social media platforms, including Facebook and Twitter, struggle to provide enough moderators to deal with the amount of content uploaded, and how they handle political content, hate speech or fake news. Many are turning increasingly to artificial intelligence to spot issues.\\nTikTok has faced a stream of criticism about what content it allows and what it blocks.\\nMedia caption\\nFeroza Aziz: \"I\\'m not scared of TikTok\"\\nIn November, the app was at the centre of a row about a US teenager who was blocked from the service after she posted a video criticising China\\'s treatment of the Uighur Muslims.\\nAfter a flurry of headlines, the ban was lifted with TikTok insisting human moderation error was to blame for the video being taken down. The 17-year-old\\'s prior conduct on the app led to her being blocked, it said, and it had nothing to do with Chinese politics.\\nTikTok changes virtual gifts policy after BBC probe\\nTikTok sent US user data to China, lawsuit claims\\nTikTok: Should we trust the Chinese social-media video app?\\nIn the US it is facing increasing scepticism from Congress about developer ByteDance\\'s relationship to the Chinese government. The app has been banned from government-issued phones in the US army over security fears.\\nThe company maintains that it is independent from the Chinese government, but a report in the Washington Post, which talked to six former TikTok employees, claimed that moderators in China had the final say on whether flagged videos were approved.\\nIn September, the Guardian newspaper claimed that the firm had previously censored material that is politically sensitive to the Chinese government, after it was given access to the site\\'s internal moderation guidelines.\\nTiananmen Square protests, Tibetan independence and the religious group Falun Gong were among banned or restricted content.\\nAt the time, TikTok claimed the community standards the paper had seen were old ones that the firm no longer used.\\nUnder the newly updated guidelines, a TikTok spokesman told the BBC that there would be \"no censorship\" of such posts.',\n",
              "  array([-1.5065721 , -1.8934357 , -0.46556902], dtype=float32),\n",
              "  2),\n",
              " ('How sharing animal photos online endangers wildlife\\nby THE CONVERSATION — 12 days ago in SYNDICATION\\n3\\nSHARES\\nLimbani the chimpanzee has about 650,000 Instagram followers. In recent months the account has featured viral photos and videos of the captive young ape playing the guitar, bouncing on a trampoline and wearing a giant banana costume.\\nFans are also offered real-life encounters with the chimp at a Miami facility, paying US$700 for a ten-minute session.\\nBirds are cool\\nEarly birds are even cooler. Get cheap tickets to TNW2020 right now\\nYEAH\\nExperts, including renowned primatologist Dr. Jane Goodall, have raised concerns about Limbani’s care. They question why he is not in the company of other chimpanzees, and say his exposure to humans could cause stress and other health issues.\\nSo before you click on or share wildlife content online, it’s worth considering how you might affect a species’ welfare and conservation in the wild.\\nSmiling chimps are actually stressed\\nChimpanzees are frequently depicted in greeting cards, advertisements, film, television, and internet images. They are often clothed, in human-like poses and settings. These performing animals are usually taken from their mothers as infants, physically disciplined in training, and can spend their retirement in poorly regulated roadside attractions or breeding facilities.\\nFor example, the chimpanzee, who appeared with Leonardo DiCaprio in The Wolf of Wall Street has reportedly since been kept in a roadside zoo, dragged around by the neck and forced to perform circus tricks.\\nPrimates are complex social animals, and the trauma they suffer when forced to perform is often clear. Research has shown the “cheeky chimp grins” we associate with happiness are actually a sign of fear or submission.\\nBut it’s not just primates who are suffering. Earlier this year US banking giant JPMorgan Chase suspended an advertising campaign featuring captive elephants. The move followed an outcry from conservationists, who explained that elephants are often trained “using harsh and cruel methods” to perform unnatural behaviors and interact directly with people.\\nTrained captive elephants perform in Sri Lanka. EPA\\nEndangered in the wild\\nImages of wildlife in human-like poses and environments can also skew public perception about their status in the wild.\\nFor example, the International Union for Conservation of Nature classifies chimpanzees as endangered. In the last century, their numbers have decreased from some 1-2 million to as few as 350,000.\\nHowever, research has shown that the prevalence of chimpanzees in media and entertainment can lull viewers into believing wild populations are thriving. This undermines both the need and urgency for in-situ conservation.\\nA 2008 article published in Science reported on the findings of two surveys where participants were asked to identify which of three great apes were endangered. In the first, 66 percent of respondents thought chimpanzees were endangered (compared with 95 percent for gorillas, and 91 percent for orangutans). In the second, 72 percent believed chimpanzees to be endangered (compared with 94 percent for gorillas and 92 percent for orangutans).\\nParticipants in both studies said the prevalence of chimpanzees in television, advertisements, and movies meant they must not be in jeopardy in the wild.\\nSuitability as pets\\nImages of animals in close proximity with humans also affect their perceived desirability as exotic pets. Such images include “wildlife selfies” shared on social media by tourists, pet collectors, and celebrities.\\nThe demand for exotic pets drives the illicit trade in live animals. In Japan, an unprecedented demand for otters as pets is likely fuelled by an increase in the visibility of pet otters in social and mass media. The pet trade has been identified as a pressing threat to the survival of otters.\\nSocial media provides an easy way for traffickers and buyers to connect. Over six weeks in 2017 in France, Germany, Russia , and the UK, the International Fund for Animal Welfare identified more than 11,000 protected wildlife specimens for sale via more than 5,000 advertisements and posts. They included live otters, tortoises, parrots, owls, primates, and big cats.\\nFacebook is also allegedly profiting from advertisements on pages illicitly selling parts and derivatives of threatened animals, including elephant ivory, rhino horn, and tiger teeth.\\nOtter sold via Instagram in Indonesia. Instagram\\nSlow progress\\nSocial media giants have gone some way to recognizing the harmful impact of their wildlife content.\\nFacebook and Instagram are partners of the Coalition to End Wildlife Trafficking Online which aims to reduce wildlife trafficking online by 80 percent by 2020. Both platforms also banned the sale of animals in 2017 – however it is not well policed, and the advertisements persist.\\nIn 2017, Instagram encouraged users not to harm plants or animals in pursuit of a selfie, and consider the potential animal abuse behind photo opportunities with exotic animals.\\nBut there are persistent claims these measures aren’t proactive or effective enough.\\nThere is cause for cautious optimism. Researchers and social media platforms are collaborating to develop artificial intelligence to help in wildlife trafficking investigations and facial recognition technology is being used to track individual animals.\\nSocial media users are also key to promoting respect and safety for wildlife. To find out more, you can access resources on “responsible tagging”, “wildlife selfie codes,” ethically sourcing footage, and how to research wildlife attractions.',\n",
              "  array([-2.9928718 , -3.213907  , -0.09468708], dtype=float32),\n",
              "  2),\n",
              " ('Before we augment people with tech, we’ll need proper rules\\nby IRA VAN KEULEN AND RINIE VAN EST — in SYNDICATION\\n4\\nSHARES\\nNew technologies – from artificial intelligence to synthetic biology – are set to alter the world, the human condition, and our very being in ways that are hard to imagine. The discussion of these developments limits itself as a rule to individual values. But it is also crucial to talk about the collective human values that we wish to guarantee in our intimate technological society. That brings an important political question at the table. How to develop and implement human enhancement technologies in a socially responsible way?\\nDuring the last few decades, the human being has become an increasingly acceptable object of study and technological intervention. We are an engineering project ourselves. An important engine behind this development is the combination of nano-, bio-, information, and cognitive technology. This so-called NBIC convergence is creating a new wave of applications, consisting in large part of intimate technologies capable of monitoring, analyzing, and influencing our bodies and behavior. In essence, the NBIC convergence means a steadily more profound interaction between the natural sciences (nano and info) and the life sciences (bio and cogno). This interaction leads to two megatrends: “Biology becomes technology” and “technology becomes biology.”\\nBirds are cool\\nEarly birds are even cooler. Get cheap tickets to TNW2020 right now\\nYEAH\\nIn the natural sciences, a revolution has occurred in the area of materials. If in the seventies we could research and manufacture materials on a micro-scale, we have now learned to do it on a nanoscale. A DNA strand, for example, is almost two nanometers (or two-millionths of a millimeter) thick. Nanotechnology laid the groundwork for the computer revolution. In turn, those computers make it possible to make better materials and machines. That way nanotechnology and information technology spur each other on. Digitization makes it possible to gather large amounts of data about the material, biological and social world, in order to analyze and apply it. Consider the self-driving car that makes use of digital maps and adds new information to those maps with every meter traveled. In this way, a cybernetic loop arises between the physical and digital worlds.\\nLiving organisms, like the human body, are seen more and more as measurable, analyzable, and manufacturable\\nThe above developments in the natural sciences stimulate the life sciences, such as genetics, medicine, and neuroscience. Modern equipment, from DNA chips to MRI scans, offers countless opportunities to investigate and intervene in body and brain. This leads to the statement that “biology is increasingly becoming technology.” That means that living organisms, like the human body, are seen more and more as measurable, analyzable, and manufacturable. Germline technology is a typical example of this trend. In the summer of 2017, an American research team succeeded for the first time in using CRISPR-Cas9 technology to repair a hereditary disorder in the DNA of a (viable) human embryo.\\nAt their turn insights from the life sciences inspire the design of new types of devices: think of DNA computers and self-repairing materials. Simulation of the workings of the brain in hardware and software is, for instance, an important goal of the largescale European Human Brain Project, into which the European Commission has been investing a billion euros for ten years. This leads to the statement that “technology is increasingly becoming biology.” Engineers increasingly attempt to build qualities typical of living creatures, such as self-healing, reproduction, and intelligence, into technology. Examples of this second trend are artificial intelligence and android social robots.\\nThe trends “biology becomes technology” and “technology becomes biology,” when applied to the human being, ensure that humans and technology are increasingly merging with each other. The Rathenau Insituut, therefore, speaks of an intimate technological revolution.\\nConsider technologies external to our bodies too\\nThe trend “biology becomes technology” drives the debate over “human enhancement.” Traditionally, this debate focuses on invasive medical technologies that work inside the human body. Consider psycho-pharmaceuticals like methylphenidate (Ritalin), which are used to suppress powerful behavioral impulses and improve the storage capacity of our random-access memory, or modafinil, which can help make us more alert and thoughtful. But also neurotechnologies like deep brain stimulation and other brain implants, biotechnologies like synthetic blood substitutes, artificial retinas, gene therapy, and germline modification – all commonly cited examples in discussions about human enhancement.\\nWhat does it mean to be human in the 21st century? That question also pertains to the trend “technology becomes biology,” that is, technologies outside the body that have an impact on people’s physical, mental, and social achievements. One example is the Tactical Assault Light Operator Suit (TALOS), an exoskeleton developed by the US Army to make soldiers stronger and less vulnerable to bullets. Besides that, consider persuasive technology: information technology designed to influence human behavior. Think for example of smartphone apps giving people advice on what (not) to eat, on their driving, and on how they should handle social relations or money. Or a smart bracelet that monitors perspiration and heartrate and vibrates if the wearer displays aggression. The wearer has learned by means of a role-playing game that aggressive behavior doesn’t pay off. Consequently, it is expected that he or she will avoid similar behavior in the real world. Through EEG neurofeedback, people can also get insight into their brain activity and learn to influence it in order to change their behavior.\\nIntimate technologies offer opportunities for human enhancement, but can also lead to essential changes in human skills and the way we communicate with one another.\\nThe above technologies, working outside the body, raise questions about autonomy and informed consent: are people in “smart” environments really able to make informed decisions? When does the concept of technological paternalism become relevant? Can persuasive technology further weaken an already weak will? Is it morally permissible to influence people’s behavior – even for the better – without their knowledge? Just like invasive technologies, non-invasive technologies raise questions about privacy, as well as bodily and mental integrity. In the case of many persuasive technologies, you have to give away a lot of your data in order to improve yourself. Do users really remain in control of their own data? Do we have the right to remain anonymous, to opt-out of being measured, analyzed, and coached? And how could we, in a world full of sensors? The rise of facial and emotion recognition, in particular, makes this a pressing question.\\nPeople can voluntarily insert the above invasive and non-invasive technologies into their bodies and lives, for instance, to become stronger or more attractive. But technology can also have unintended side-effects. Through the increasingly intensive use of technology, our abilities begin to change. We develop new competencies (a phenomenon called “reskilling” or “upskilling”), such as all kinds of digital skills. Other competencies might be reduced (“deskilling”). There is, for example, a body of research appearing to indicate that our social skills, such as empathy, are crumbling through excessive computer use. Intimate technologies, then, offer opportunities for human enhancement, but can also lead to essential changes in human skills and the way we communicate with one another. Such changes in the human condition transcend the level of the individual. They touch upon collective questions and values and demand public debate and, where necessary, political consideration.\\nPaying attention to collective values\\nThe current debate on human enhancement, though, largely limits itself to individual goals. Examples of classic questions are: is human enhancement an individual right? Can people decide for themselves whether they want technological enhancements? In The Techno-Human Condition, Braden Allenby and Daniel Sarewitz argue that such an approach is inadequate. They suggest that the debate over the impact of human enhancement ought to be conducted on the following three levels of complexity:\\nThe direct impact of a single technology;\\nThe way in which technology influences a socio-technological system and the social and cultural patterns affected by the same;\\nThe impact of technology on a global level.\\nTake the car as an example. The car, in principle, gets you from A to B faster than a bike would (level 1 reasoning). But if many people drive cars, the bike can sometimes be a faster option in the city (level 2 reasoning). On a global scale, the rise of the car has led to a variety of important developments, such as the development of the oil economy, Fordism (the model of mass production and consumption), and climate change. Allenby and Sarewitz posit that the current debate over human development frequently remains on the instrumental level. It revolves especially around the question of whether people have the right, on the basis of free choice, to opt into technologies designed to enhance their bodies and minds. In opposition to what transhumanists often suppose, they show that – just as the car isn’t the faster choice than the bike under every circumstance – the use of human enhancement technology on an individual level doesn’t straightforwardly lead to a better individual quality of life, let alone to a better society. The application of human enhancement technology will frequently be driven by economic or military motives (level 2 reasoning). Such a scenario complicates the issue of individual free choice, because in that case, “The posthuman person is not a self-made man, but a person designed by others.”\\nThe posthuman person is not a self-made man, but a person designed by others.\\nThe mass deployment of human enhancement technology will also have effects – although hard to predict – on a global level. In Homo Deus, Harari sketches two (parallel) long-term scenarios: first, the arrival of the physically and mentally enhanced “superman” (Homo Deus) and a division between supermen and normal people (level 3 reasoning). According to Harari, in the long term, this could lead to the abandonment of the principle of equality that forms the basis of the Universal Declaration of Human Rights. In addition to this “biology becomes technology” scenario, Harari presents a “technology becomes biology” scenario. He anticipates the rise of “dataism,” in which humanity embeds itself in an Internet-of-All-Things and allows itself to be guided purely by AI-generated advice dispensed by computers. In this scenario, humanity has given up all its privacy, autonomy, individuality, and consequently democracy, which is based on personal political choices. Although such scenarios are speculative, they show us which important issues are at stake and show that it is important to look (far) beyond the individual, instrumental level.\\nThe Dutch discussion of germline technology shows that this often does not happen. So far collective interests play a negligible role in that debate. And that is in spite of the fact that CRISPR-applied modifications in the DNA of the embryo are irreversible and heritable by future generations. In the current debate, the pragmatic approach we know from the medical-ethical regime still dominates. In this debate, a lot of attention is paid to the international position of the Netherlands. The country doesn’t want to fall behind as a knowledge economy. Second, there is a special focus on the health benefits germline modification can deliver for the individual in question. A traditional risk-benefit analysis is central to this. Third, significant emphasis is placed on strengthening reproductive autonomy. It is about the opportunity germline modification offers to prospective parents with a hereditary condition: to have a genetically healthy child of their own.\\nBut germline modification also raises questions that do not fit neatly within the framework of medical-ethical principles oriented towards safety, informed consent, and reproductive autonomy. In terms of collective values and international human rights, there should also be a place in the debate for the notion that the human genome is our common heritage, and thus our collective property.\\nTechnological citizenship\\nNew NBIC technologies are set to alter the world, the human condition, and our very being beyond our imagination. Above, we argued that in relation to human enhancement we must consider both invasive medical technologies (the trend “biology becomes technology”) and technologies outside the body that nevertheless have an impact on people’s bodily, mental, and social performance (the trend “technology becomes biology”). Futurist thinkers from Harari to Aldous Huxley and Raymond Kurzweil show us what is potentially at stake this century: radical improvement of human capacities and choices, division between “natural” and “enhanced” humans, the abolition of the individual and in its wake, democracy. This brings a crucial political question at the table: how can we develop and implement human enhancement technology in a societally responsible way?\\nTechnological citizenship is the collection of rights and duties that makes it possible for citizens to profit from the blessings of technology and protects them against the attendant risks.\\nTo give direction to that potentially radical transition, a democratic search for shared moral principles is necessary, principles that can set the fusion of human and technology off on the right track. An absolute condition for that collective search is a well-developed “technological citizenship” for all citizens. Technological citizenship is the collection of rights and duties that makes it possible for citizens to profit from the blessings of technology and protects them against the attendant risks. It means understanding how statistical results, (genetic) profiling and self-learning algorithms work, seeing how that affects us, and being prepared to defend against unwanted influences and choose (potentially non-technological) alternatives where necessary. Besides, it is important that citizens have the option of participating in the decision-making process regarding technology at every stage of development, from research to application. Technological citizenship emancipates the regular citizen in relation to the experts and developers of technology.\\nThe role of institutions\\nEducation plays a central role in the promotion of technological citizenship. And that begins with primary and secondary education. Here lies a clear role for the government. Meanwhile, in April 2017 the Dutch House of Representatives approved a curriculum revision prepared by Platform Onderwijs2032 (Education2032). It adds two new fields to the curriculum: digital literacy and citizenship. In 2018 development teams are getting started making those fields a reality. It would be good for the two development teams to work in close cooperation, taking into account the fact that citizenship in a technological culture only has meaning if we can engage in an informed discussion about the effect of technology on our private lives and our society.\\nBut education is not enough. To make their citizenship a reality, people need institutions. Without suitable administrative institutions, technological citizenship is an empty shell. It must be possible for rights and duties to be democratically demanded, fixed, and implemented. Individuals, then, can only be considered true technological citizens if they know themselves to be protected by an optimally equipped system of governance. The following four components are crucial to this: 1) rights and compliance monitoring, 2) public debate, 3) political vision, and 4) socially responsible companies.\\nRobots should not replace human relationships but improve them, whether we are talking about care for the elderly or the upbringing of children\\nFirst, citizens must be able to appeal to fundamental human rights suitable to the time we live in. At the request of the Parliamentary Assembly of the Council of Europe, the guardian of human rights in Europe, the Rathenau Instituut researched how robotization, artificial intelligence, and virtualization could challenge our current conception of human rights. The Rathenau Instituut proposed, among other things, two new human rights. First, the right not to be measured, analyzed or coached. People must have the right not to be surveilled or covertly influenced, and to evade continuous algorithmic analysis. Secondly, the right to meaningful human contact within caregiving. Robots should not replace human relationships but improve them, whether we are talking about care for the elderly or the upbringing of children. Already-existing rights and duties should be put into practice in everyday lifeso that technological citizens can count themselves truly protected. We wonder whether the current Dutch supervisory authorities are really able to carry out their missionand whether their mandate is truly adequate. The Netherlands Institute for Human Rights pays little attention to the question of how digitalization can place human rights under pressure. The Dutch Data Protection Authority is given little scope to look at collective values other than privacy.\\nSecond, a social debate over the impact of new technologies is necessary. While civil society is strongly organized to address environmental problems, the Netherlands still has few established social organizations willing to enter into a critical discussion about the new intimate technology revolution, except in relation to privacy and security. Meanwhile, we ought to be asking questions regarding which collective human values we wish to guarantee in our intimately technological society. If we don’t debate these issues at this early stage, we effectively leave the course of technological advancement to the engineers, to the market, and to individual choice. Pessers warns for the collective effect of individual self-determination, which society stealthily confronts with a fait accompli, without any democratic debate. For example, in the case of prenatal diagnostics, the abortion of a number of children with Down syndrome doesn’t change society. But if that starts to happen on a mass scale, it raises the question of whether we really want a society entirely without people with Down syndrome.\\nIf we don’t debate these issues at this early stage, we effectively leave the course of technological advancement to the engineers, to the market, and to individual choice.\\nPolitics and government are called upon to take the lead in the debate and the administrative handling of the intimate technology revolution. Nevertheless, there is at this moment no broad political vision addressing the impact of technology on our being and the current political debate is driven largely by random incidents. For such a vision, further knowledge development is necessary. When it comes to our natural environment, the central concept is ecological sustainability. It required many years and the discovery of new knowledge to give qualitative and quantitative meaning to this concept. We think that in the debate over the relation between technology and humanity, the concept of “human sustainability” must play a central role. Human sustainability means the preservation of human individuality: what aspects of humanity and our being-human do we see as malleable, and which do we want to preserve? Think for example of the desire to keep our empathetic capacities working at a high level, or to have children born from a real mother, not an artificial womb. Concepts such as human dignity and human sustainability require much greater research and consideration.\\nFinally, citizens must be able to trust that user interests come first when businesses develop new technological products. The increasing fusion between people and technology forces us to keep in mind the values and norms that we design into products and computer coding. On the subject of privacy, academics have argued for years that organizations should pay attention to privacy measures and data minimization when developing information systems. Privacy by design has become a core principle of new European privacy regulations. Privacy-oriented technology is an example of the broader concept of value-sensitive design, which attempts to incorporate not only privacy but a broad range of relevant collective values, including basic human rights, into the development of technology.',\n",
              "  array([-0.5668505, -1.8239129, -1.3045386], dtype=float32),\n",
              "  0),\n",
              " ('Just about everyone agrees cybersecurity will be paramount in 2020, and governments and regulatory bodies are already taking action. While GDPR allows citizens in Europe to manage their digital footprint and data, the EU’s Cybersecurity Act provides strong support for member nations to alert one another and act against bad actors.\\n\\nStill, cybersecurity is a difficult line of work. It’s dynamic, and IT pros often feel harrowed by the amount of ground they’re expected to cover. Instead of seeing what new cybersecurity trends will develop in 2020, we thought we’d ask the experts.\\n\\nWe polled a massive number of CTOs, VPs, experts, and at least one “InfoSec Personality” on what they thought 2020 would have in store for us. If there’s one core takeaway, it’s the headline for this article could have swapped ‘132’ for ‘5’ and we’d still not have covered all the feedback.\\n\\nCybersecurity professionals have huge jobs, and varied concerns. Still, these are some of the trends we saw dominating our feedback channels.\\n\\n5G is scary\\nWhile carriers like Verizon rile consumers up about 5G, Paul Lipman, CEO of BullGuard, would rather we pump the brakes.\\n\\n“5G is set to be the most sweeping communication revolution we have ever experienced and will usher in an area of innovative new consumer services,” Lipman tells TNW. “Because 5G is a switch to mostly all-software networks, and upgrades will be like the current periodic upgrades to your smartphone, the cyber vulnerabilities of software poses potentially enormous security risks.”\\n\\nThis alarm has been sounded before. Huawei, the leading manufacturer of 5G equipment for carriers, is still embroiled in a trade war with the United States Government over concerns its 5G equipment and consumer devices will allow foreign governments to spy on citizens.\\n\\nThe core concern with 5G is the number of connected devices sending and receiving information – combined with routine network upgrades and the ability to remotely access a network – will create a minefield for cybersecurity professionals.\\n\\nJosh Lemos, VP of Research and Intelligence for BlackBerry Cylance, says a near full-stop is possible:\\n\\n“As cities, towns and government agencies continue to overhaul their networks, sophisticated attackers will begin to tap into software vulnerabilities as expansion of bandwidth that 5G requires creates a larger attack surface. Governments and enterprises will need to retool their network, device and application security, and we will see many lean towards a zero-trust approach for identity and authorization on a 5G network.”\\n\\nPhishing emails will still be big\\nMatt Jakubowski, Director of Cybersecurity at Uptake, says “phishing is still one of the number one ways an attacker will get into a network or infect users.” Though he (naturally) advocates IT teams upskill workers to be vigilant about phishing attempts, hackers are becoming increasingly sophisticated.\\n\\nMoreover, experts think hackers will continue to exploit old avenues, like email. “While there are threats that you can completely mitigate by disabling a service or whitelisting/blacklisting, phishing is not one of those as you have to allow access to email,” says Christopher Hass, Director of Information Security and Research at Automox. “It is also much easier to craft a good-looking phishing email than it is to discover and weaponize a zero-day.”\\n\\nCTO and founder of Illumio, PJ Kimer, takes it even further, saying those around a main target will be at-risk. “Whether it’s the child of an executive, an executive assistant, or even someone with administrative privileges, it only takes one wrong click for them to implant malware on their parent’s phone, opening up the back door for a bad actor to get into the company network.”\\n\\nUnfortunately, there’s no silver-bullet solution. Email requires open lines of communication; educating employees helps, but spoofing Karen’s Sur La Table coupon email is an easy point of entry. In Europe, EU agencies approved the Cybersecurity Act to support member nations against these types of attacks. In addition to providing a framework for combating cybersecurity threats, it allows for wider information sharing on new attacks and vulnerabilities which may be widespread.\\n\\nThe 2020 election should be… interesting\\nHowever you align yourself politically, the 2020 election will be worth watching because cybersecurity pros think we’ll see some hacking – and learn a lot.\\n\\n2016 was the first time we became generally aware of a disinformation campaign from a foreign entity specifically to disrupt the United States’ democratic election process. Social media played a big part in surfacing misleading or outright false information, and it’s just not fixed. Aanand Krishnan, CEO & Founder of Tala Security, says “without more comprehensive website security controls in place, ad networks and session hijack, like those enabled by today’s significant client-side security vulnerability, will continue to put the integrity of these information resources at risk.\\n\\nFrench Caldwell of The Analyst Syndicate says the actual act of voting is in jeopardy, too. “Hackers with ties to Russia did gain access to voter databases in some counties, but they did not alter voter data. The evidence of vulnerability of voter databases could tempt foreign actors to go even further in 2020 – not only gaining access, but perhaps locking down voter databases with ransomware.”\\n\\n“Ransomware attacks in the days just prior to the election would prevent the distribution of voter lists at polling places. Without the voter lists, election judges would not be able to verify registered voters, meaning thousands and maybe millions of people in affected localities would have to use provisional ballots or, if the ballots run out, may even not be able to vote at all.”\\n\\nCasey Ellis, Founder, Chairman and CTO at BugCrowd, says “much of the voter narrative on election security focuses on the cybersecurity elements,” which he predicts will make agencies more accountable. “The good news is, we’re already seeing a move in the right direction with the call for vulnerability disclosure programs across agencies, which would allow whitehat hackers to help surface flaws in election websites and applications in lead up to and through the elections.”\\n\\nBottom line: expect hacking, and expect accountability. If we really do see a more open sharing of information, we’ll learn more about sophisticated hacking at the top level, which is invaluable.\\n\\nWant to work at Rijksoverheid? They’re hiring.\\n\\nA.I. and ML can be hacked, too\\nInfoSec Personality (see, told you we had one!) and Security Advocate Johnny Xmas underscores the real problem with artificial intelligence and machine learning: they’re just plain “resource-intensive,” and that makes them hard to secure. Because A.I. and ML devices or services must be trained, the best ones are birthed and managed off-site, and cybersecurity pros can’t be sure what they’re getting.\\n\\n“Since ML inherently requires ‘training,’ and this training is extremely resource-intensive (thus rendering hardware cost-prohibitive for the customers), the machines must be trained remotely,” says Xmas. “This means customers of ML products are using ML trained on data which is not specifically theirs, in turn meaning they are not reaping the full benefits of the technology as in the end the best the machine can only act on information it recognizes.”\\n\\nHaiyan Song, SVP and General Manager of Security Markets at Splunk, tells TNW: “expect to see attempts to poison the algorithm with specious data samples specifically designed to throw off the learning process of a machine learning algorithm. It’s not just about duping smart technology, but making it so that the algorithm appears to work fine — while producing the wrong results.”\\n\\nAs the training becomes a new vulnerability, the automation of some cybersecurity workloads is in jeopardy. Axel Wirth, Cheif Security Strategist for MedCrypt, notes log and event data reviews and threat analysis modeling are both capable of automation. Further, he says “cyber adversaries” will use A.I. and ML to “uncover new vulnerabilities, to analyze and misguide our defensive tools, or to create realistic false information.”\\n\\nOh, the Cloud\\nRemember that old joke “the cloud is just someone else’s computer”? Well, if someone’s computer can be hacked, why not the ever-present cloud?\\n\\nDarrell Long, VP of Product Management at One Identity, says “there is a ‘gold rush’ for organizations to move their data to the cloud. Large organizations are making rapid moves to the cloud without ensuring their data is secured in transit and once it’s there. In 2020, there will be multiple organizations that deal with data privacy breaches and regulatory fines, as these steps are not being adequately addressed from the beginning of the move. Even with the Shared Responsibility Model and news about vulnerabilities with cloud security, we foresee many organizations failing to conduct due diligence and being burned by leaving their data insecure in the cloud.”\\n\\n“Enterprises should adopt solutions from companies which give cloud visibility, recommend security policy and orchestrate the policies to prevent attacks,” according to Umesh Padval, Partner at Thomvest Ventures. “Secondly, they should accelerate data protection and encryption while data is being transmitted, stored and processed.”\\n\\nHaiyan Song reminds us that once data migration to the loud has occurred, it’s not smooth sailing. “Going forward, cybercriminals will exploit the emerging vectors brought to bear by cloud-native technologies such as containers and Kubernetes, taking advantage of organizations’ learning curves to launch new attacks at a scale and speed we have not seen in the [on-premises] world.”\\n\\nCCPA for all?\\nThe California Consumer Privacy Act (CCPA) is, at its core, the GDPR come stateside. Because the U.S. federal government refuses to act on user privacy in the digital era, California voters went ahead and took matters into their own hands with a GDPR-esque framework that takes effect January 1, 2020.\\n\\nCCPA is significant for one simple reason: ignoring California is nearly impossible. It’s home to Silicon Valley, and is the nation’s largest economy (and would be the fifth-largest economy in the world if the state were a sovereign nation). If a smaller state enacted something akin to the CCPA, it would risk companies choosing to make their services unavailable to netizens residing there. California, though – not so much.\\n\\nSo what is CCPA? It’s a bill that “creates new consumer rights relating to the access to, deletion of, and sharing of personal information that is collected by businesses.” Essentially, if you’re collecting user information, you have to clearly explain why, what exactly you’re collecting, what you’re doing with it, and how they can remove themselves from your database.\\n\\nAanand Krishnan, CEO & Founder of Tala Security, reminds us how powerful the GDPR is. “GDPR went into full effect in May 2018, and already in 2019 the EU has fined Google 50 million Euros, British Airways 180 million Euros, and Marriott 100 million Euros.” GDPR puts the onus on companies to secure your data, or face penalties. Expect CCPA to do the same.\\n\\nGDPR fines are a warning shot to companies doing business in California, which will likely carry over nation-wide – and why wouldn’t they? But cybersecurity professionals need to be proactive about CCPA compliance. Relativity’s Chief Security Officer, Amanda Fennell, reminds us “best practices should include a thorough review of a vendor to ensure they are in accordance with these regulations and a concerted focus on this area with devoted resources to ensure they stay abreast of changes.”\\n\\nExpect other states to simply copy-paste CCPA into legislation, even if the federal government never does. And once other massive or dense states like Texas and New York follow suit, it’s smarter to just comply and push on.\\n\\nConclusion: 2020 will be like 2019’s lingering JIRA ticket\\nThese five topics are simply the most often-recited bits of feedback from experts, and far from the only we’ve received.\\n\\nThey’re also not totally new. Phishing has, in one form or another, been around since the 1980s. The cloud has been problematic since it became a consumer product. We all expected 5G to have issues.\\n\\nIt shows cybersecurity isn’t easy, or simple. If we’ve not solved these issues by now, they’ll continue to surface and morph to suit someone’s needs.',\n",
              "  array([-1.1185797, -0.958681 , -1.2383631], dtype=float32),\n",
              "  1),\n",
              " (\"WASHINGTON (Reuters) - Facebook Inc on Wednesday said it would remove “deepfake” and other manipulated videos from its website to combat the spread of false information ahead of the 2020 presidential race, but lawmakers said those and other changes it has recently announced do not go far enough.\\nFILE PHOTO: Stickers bearing the Facebook logo are pictured at Facebook Inc's F8 developers conference in San Jose, California, U.S., April 30, 2019. REUTERS/Stephen Lam\\nThe comments, made during a hearing held by the House Energy & Commerce subcommittee, marks the latest effort by House lawmakers to probe Facebook’s digital defenses ahead of the November elections four years after Russia used the site to spread misinformation during the 2016 presidential race.\\nSubcommittee Chairwoman Jan Schakowsky, a Democrat, said there is growing evidence that big tech has failed to regulate itself. “I am concerned that Facebook’s latest effort to tackle misinformation leaves a lot out,” she said.\\nOther lawmakers broadly pointed to Facebook's inability to address the issues of data security, misinformation and foreign interference ahead of the elections. An internal company memo sent by a senior executive to employees addressed Facebook's shortcomings and was published by the New York Times on Tuesday. nyti.ms/2QVFjpy\\nRanking Republican member Cathy McMorris Rodgers said consumers are losing faith in sources they can trust online but focus should be on innovation to combat falsified videos and not more regulation.\\nEarlier this week, Facebook said it would remove deepfakes - which use artificial intelligence to create hyper-realistic but fake videos where a person appears to say or do something they did not - as well as other manipulated or misleadingly edited videos from its platform in a move to curb misinformation. It will not remove content deemed to be parody or satire.\\nMonika Bickert, Facebook’s vice president of global policy management, said the social media platform recognizes the risks of manipulated media and that “its latest policy is designed to prohibit the most sophisticated attempts to mislead people.”\\nBickert faced criticism from lawmakers for the company’s decision to not remove a heavily edited video that attempted to make House Speaker Nancy Pelosi seem incoherent by slurring her speech.\\n“Why wouldn’t Facebook simply take down the fake Pelosi video?” Florida Congressman Darren Soto, a Democrat, said.\\nBickert said such videos will be labeled false and are subject to fact-checking. “Our enforcement is not perfect,” but it has gotten better, she said.\\nThe company took down one network that spread false information in 2016. It has removed 50 such networks in 2019, she said.\\nFacebook has been criticized over its content policies by politicians across the spectrum. Democrats have blasted the company for refusing to fact-check political advertisements, while Republicans have accused it of discriminating against conservative views, a charge it has denied.\\nCalifornia Congressman Jerry McKerney asked if Facebook would be ready for an independent third-party audit of its practices by June 1, results of which would be visible to the public.\\nBickert did not answer the question.\",\n",
              "  array([-0.36504102, -1.4978298 , -2.4983947 ], dtype=float32),\n",
              "  0),\n",
              " ('As head of product inclusion at Google, Annie Jean-Baptiste works to ensure that the products and services Google offers are inclusive and reflective of the diverse audience the company serves.\\nSince starting at Google nine years ago, the 31-year-old has served as an account manager and a diversity programs manager before stepping into her current role two years ago. When reflecting on the books that have influenced her the career the most, Jean-Baptiste tells CNBC Make It Adam Grant’s “Originals: How Non-Conformists Move the World,” comes to mind.\\n“Just given that sometimes you’re in a space that’s new and a little bit uncharted, I think that reading about people who have started something from scratch or started something that people didn’t totally get at first is interesting,” she says. “It’s just really interesting to see how they build that consensus up from the ground floor.”\\n\"Originals\" by Adam Grant.\\nIn Grant’s bestselling book, the Wharton School professor uses data and research to show readers what it takes to bring an original idea to life. He also challenges the belief that you have to take a crazy amount of risk in order to birth a great idea.\\n“You don’t have to be a round peg in a square hole to be original,” Grant said on CNBC’s “On the Money” in 2016. “In fact, many originals hate taking risks.”\\nFor example, he says, “If you look at the data, entrepreneurs who avoid risk by saying, ‘You know what, I’m going to keep my day job before I go all in’ are 33% less likely to fail.”\\nThough Grant’s book may be perceived as a read that focuses on entrepreneurship, Jean-Baptiste can relate to the idea of building something from the ground up, considering her current role at Google was non-existent a few years ago.\\n“Product inclusion” includes elements of business, product and diversity, and Jean-Baptiste had worked in all of those areas, she says. But that doesn’t mean the transition was easy. “It’s a culmination of a lot of the work that I’ve been lucky enough to do and learn at Google,” she says. “So seeing there was an opportunity and an opening to do that and to build that out, even though it hadn’t been something that was happening before, can sometimes be scary.”\\nRecommended\\nOPINION\\nBill Barr wants Apple to break their encryption, even if it risks every American being hacked\\nNOW\\nInside the illegal marijuana industry growing on Instagram\\nIn her two years as head of product inclusion, Jean-Baptiste and her team have launched several products including Google Assistant, Gmail’s Smart Compose feature and Google’s Pixel Camera. When developing the Google Assistant, which is an artificial intelligence-powered virtual assistant that can hold a two-way conversation with its user, Jean-Baptiste and her team worked tirelessly to test the product for racially and gender-insensitive language before its launch.\\n“Google has always said focus on the user and all else will follow,” the tech executive says in a video about the product launch of Google Assistant. “If you’re thinking about a challenge or product, you need to make sure that you’re intentional about expanding who your users could and should be.”\\nSimilarly with Gmail’s Smart Compose, Jean-Baptiste and her team tested the product before its launch to ensure that the predictive text feature wouldn’t create any negative or offensive messages. They also tested Google’s Pixel Camera before its launch to ensure that the lens accurately reflected all skin tones.\\n“I think, you know, the crux of this work is to really ensure that everyone feels seen and valued for their differences and feel like they were thought of,” she says. “We know that we have work to do and that we’re on a journey, but we’re really excited and committed to making sure that we’re building for everyone and with everyone.”',\n",
              "  array([-3.260099  , -2.5307686 , -0.12554331], dtype=float32),\n",
              "  2),\n",
              " ('Social media is full of lies, and the Iran attacks show how dangerous they\\'ve become\\nNathan Bomey USA TODAY\\nPublished 11:39 AM EST Jan 9, 2020\\nThe whirlwind of misleading or fabricated reports tied to Iran\\'s retaliatory attack in Iraq on Tuesday night illustrates the depths and dangers of the nation\\'s misinformation crisis.\\nFueled by social media, the false reports generated fears of a wider outbreak of war when international tensions were already high and Americans on edge.\\nAmong the false reports was one widely circulated tweet that 20 American troops died and the nation was on the \"precipice of another horrible war in the Middle East.\" Other posts on Facebook and Twitter, in particular, circulated falsehoods about the attack, using images from unrelated events to suggest a horrifying outcome.\\nIt turned out there were no casualties from the Iranian attack on two Iraqi bases in retaliation for a U.S. strike that killed a top Iranian general.\\nThe barrage of misinformation underscores that despite the pledges and efforts by big tech companies to crack down on falsehoods, fabricated or misleading content remains a significant threat.\\nIs your location on the list?: Macy\\'s is reportedly closing more than two dozen stores.\\nMore store closings: Pier 1 Imports to close up to 450 stores, nearly half of its locations\\n\"You have a system where the potential for people to spread misinformation that gets believed on a wide landscape is huge,\" said Josh Pasek, an associate professor of communication, media and political science at the University of Michigan, who has studied the issue. \\nThe topic of war, long known as a haven for propaganda, is especially ripe for spreading false reports.\\n\"This is the type of story that incites an emotional response from people, so it is a prime target for misinformation and disinformation and bad actors,\" said Katy Byron, editor and manager of MediaWise, a program from the journalism training nonprofit group Poynter that educates youth about how to sort fact from fiction online.\\nSome government officials have misled Americans. Rep. Paul Gosar, R-Ariz., shared a doctored photo appearing to show President Barack Obama shaking hands with Iranian President Hassan Rouhani.\\n\"The world is a better place without these guys in power,\" he tweeted.\\nAfter the photo was flagged as a fabrication, he called reporters \"dim witted\" and wrote, \"No one said this wasn\\'t photoshopped.\"\\nFalse rumors about the prospect of the draft being reinstated circulated widely among young people on social platforms such as TikTok and Twitter, where users shared videos with the hashtag #WWIII. (The reality is that the draft is extremely unlikely to be reinstated, even in the event of war.)\\nThe U.S. Selective Service System\\'s website temporarily crashed Friday after a surge of traffic following the killing of Iran\\'s Gen. Qasem Soleimani. False information reported online about the draft suggested that \"gay Americans are exempt\" and left \"the impression that felons can’t be drafted,\" according to an analysis by fact-checking site PolitiFact.\\nIt\\'s particularly challenging to fact-check information about war, said Aaron Sharockman, executive director of PolitiFact.\\n\"In situations like this, where you have to rely more and more on official government information, it becomes more difficult for us to fact-check,\" he said. \"In many cases, we’ll be able to help a little less because we’re not on the ground and we can’t see things with our own eyes.\"\\nSince he took office, President Donald Trump has made more than 15,400 false or misleading claims, according to The Washington Post\\'s Fact Checker database.\\n\"The fact that there\\'s just not a lot of credibility in what this administration says (means) people are searching for what the real story is here,\" Pasek said. \"You have something ripe for conspiracy theories, misinformation and the like to spread.\"\\nThe bottom line: Anyone with a social media account can mislead – and people in power can do it on a grand scale.\\nIt\\'s unlikely to change anytime soon. There are simply too many users of social media for platforms such as Facebook, YouTube and Twitter to monitor or block all misinformation, absent some form of artificial intelligence or outright censorship that poses its own set of challenges.\\nEach outlet has instituted policies to excise certain forms of false information from their platforms, while allowing others. None of it is enough to stamp out falsehoods altogether.\\nWhen assessing the situation, it helps to take a step back and look at how quickly things have changed. In the pre-social media age, journalists, who are trained on source validation and fact authentication, largely controlled the information that people encountered.\\nToday, social media algorithms have become the new gatekeepers for many people. The algorithms filter information, typically presenting material based on a benchmark for engagement predicated on clicks, shares and likes.\\nPaired with the human nature for confirmation bias – our tendency to seek out information that validates preexisting beliefs and ignore disconfirming material – it\\'s a recipe for political polarization and the spread of misinformation.\\n\"What\\'s apparent is information spreads pretty quickly whether it\\'s right or wrong – and seemingly particularly if it\\'s wrong because it has that additional novelty bonus,\" Pasek said.\\nThe democratic nature of social media, which provides a platform to anyone and everyone, empowers the spread of falsehoods masquerading as insight.\\n\"All of a sudden everyone’s an expert on Iran,\" Sharockman said. \"So you have moments where the publishing platforms – Facebook and Twitter and Reddit – allow anyone to claim expertise on the subject.\"\\nHere are tips on how to validate information you encounter online:\\nVisit primary sources whenever possible. \"Do a little digging\" and seek out the original source of the information to ensure that people aren\\'t misconstruing it online, Byron said.\\nCross-reference information from different sources. Stanford History Education Group\\'s Sam Wineburg, who conducted research for MediaWise on how young people navigate the internet, recommends \"lateral\" reading. That means navigating to unrelated sites to authenticate or disprove information from the original source. One tip to get started is to visit sources listed at the bottom of Wikipedia entries.\\nDon\\'t reflexively trust people in authority. Be skeptical. \"Just because someone has a big blue checkmark next to their name on social media doesn’t mean they’re an expert on that topic,\" Byron said, referring to the logo that verifies a user\\'s identity. \"We always really push people to go to that key question of who’s behind the information.\"\\nBe wary of photos, video and audio. The rising likelihood of fabricated or misleadingly edited video, photos and audio is a source of serious trouble. Known as \"deepfakes,\" this technology is enabled by artificial intelligence and will allow people to create content that makes it look like people did things they never did and said things they never said.\\n\"A lot of people manipulate images for disinformation and negative purposes,\" Byron said.\\nFollow USA TODAY reporter Nathan Bomey on Twitter @NathanBomey. He is the author of a book on the misinformation age, \"After the Fact: The Erosion of Truth and the Inevitable Rise of Donald Trump\" (2018, Prometheus Books).\\nPublished 11:39 AM EST Jan 9, 2020',\n",
              "  array([-0.11295322, -2.422893  , -4.0094805 ], dtype=float32),\n",
              "  0),\n",
              " ('Fake Trump video? How to spot deepfakes on Facebook and YouTube ahead of the presidential election\\nJessica Guynn USA TODAY\\nPublished 9:16 PM EST Jan 8, 2020\\nJust months before the presidential election, Facebook says it\\'s taking steps to combat deepfakes, videos that have been manipulated using artificial intelligence to make it appear that someone has said or done something they haven\\'t. \\nFacebook is banning videos that are \"edited or synthesized\" to fool users but will allow parody and satire.\\nWhy now? Many fear this insidious form of digital disinformation could be used to mislead voters, much the same way that fabricated news stories influenced public opinion in 2016.\\nSo what are deepfakes anyway and just how worried should you be that the video you are watching isn\\'t real?\\nFake but convincing videos\\nDeepfakes are videos doctored using cutting-edge artificial intelligence, or AI, to distort reality. The technology, which analyzes real images to generate fake ones, is a growing form of disinformation and social media platforms have been struggling with how to deal with it.\\nFacebook and other tech companies are sponsoring a \"Deepfake Detection Challenge\" to encourage AI researchers to develop new ways to automatically detect doctored videos.\\nFacebook disinformation: What you can do to stop its spread\\nWait, is that video real? The dangers of manipulated recordings\\nThe ability to create fake videos or manipulate existing videos has been around for decades but typically required some software or skill. In recent years, tools have become popular on social media and elsewhere that allow anyone to manipulate images and video, though this footage usually appears obviously manipulated.\\nToday, powerful new technologies are making it cheaper, faster and easier to produce deepfakes that are “nearly indistinguishable from reality,” says Hao Li, associate professor of computer science at the University of Southern California.\\nThere are also simpler but still effective ways to hoodwink social media users called \"cheap fakes,\" which typically involve editing videos to spread disinformation or propaganda.\\nDigitally manipulated video is dangerous because we tend to have faith in what we see, Li says. “It’s a real problem, and it’s something that is advancing really quickly,” he says. “Regulators and lawmakers are not catching up with this kind of technology.”\\nFacebook\\'s new deepfake policy is too narrow, says Hany Farid, a computer science professor at University of California, Berkeley specializing in digital forensics. \\nUnder the policy, Facebook would not prohibit videos using lower-tech methods of spreading disinformation, such as last year\\'s video of House Speaker Nancy Pelosi which was edited to make it seem as though she was slurring her speech, Monika Bickert, vice president of global policy management, told lawmakers during a Wednesday congressional hearing.\\n\"What they are saying is: If it’s a misleading video of someone saying something they didn’t say and it was created using this specific type of technology, deepfakes, then we reserve the right to take it down, but if it was generated using some other kind of technique, we don’t have a problem with that,” Farid says. “That doesn’t really make sense. The issue is not how it was made, the issue is: Is it misleading and is it harmful?”\\nA Facebook employee entering computer code on his laptop.\\nFacebook\\nWhy you should worry (no, really)\\nRachel Thomas, director of the Center for Applied Data Ethics at University of San Francisco, says she expects \"cheap fakes\" and the more sophisticated deepfakes to proliferate ahead of the presidential election as part of shadowy campaigns to sway public opinion.\\nImagine a fake Donald Trump or a fake Joe Biden simulated saying a racial slur. Then imagine how quickly those videos would spread on Facebook or YouTube.\\n\"The stakes are very high,\" Thomas says. \"This kind of misinformation can have a big impact.\"\\nWhen used to target elected officials or political candidates, doctored video can erode trust. Nearly two-thirds of Americans recently surveyed by Pew Research said altered images and videos caused a great deal of confusion in understanding the basic facts of current events. \\nThe term “deepfakes” refers to videos that have used artificial intelligence techniques to combine and superimpose multiple images or videos onto source material.\\ngn8, Getty Images/iStockphoto\\nHow can you spot deepfakes?\\nHow can you tell if the video you\\'re watching is a deepfake?\\n“That is becoming incredibly difficult. The technology behind fake videos is improving almost on a continuous basis,” says Siwei Lyu, computer science professor at the State University of New York at Albany and a member of the Deepfake Detection Challenge’s advisory group.\\nHe recommends “psychological preparedness,” as in being constantly on the alert for videos, images and audio that have been altered.  \\n“Whenever you see an interesting video showing something that is bizarre or exceptional, a certain vigilance should be raised,” he says.\\nExamine the video carefully before sharing it on social media. Is the video low-resolution or grainy? Is it a single person talking in the video? Is it relatively short, say 30 seconds or 60 seconds long?\\nIs the lighting strange or the face discolored or blurry? Is there blurriness between the face and neck or between the face and the hair? Is the sound not synced with the images?\\nSome of the other tell-tale signs discernible to the naked eye, according to Subbarao Kambhampati, a computer science professor at Arizona State University: different-sized eyes or ill-formed teeth, or more than two eyes, or inconsistencies in the background of the video.\\nBut, says Kambhampati, the rapid improvements in deepfake technology means that we will soon have to rely on AI techniques to detect what the human eye cannot.\\n\"There is not a 100% foolproof way of identifying deepfakes, not even for AI researchers,\" Thomas says. \"Detection is always going to be an arms race. As people develop more accurate detection algorithms, fakers will develop even more sophisticated frauds.\"\\nThere are non-technical ways to sniff out a deepfake, just like other forms of disinformation. Ask yourself: Who is the person publishing this information? Is this person reliable? What else has this person posted? Are the claims in the post backed up by sources you trust? \\nWhen in doubt, turn to nonpartisan fact-checkers who\\'ve spent hours, sometimes days, tracking down accurate information, Farid says.\\n\"You never want to jump to a conclusion one way or another, you never want to say: I looked at this and I’m sure it’s real or I looked at this and I’m sure it’s fake,\" he says. \"The best you can really hope to do is to say: You know what? I should be careful here because I’m not sure, and then research it.”\\nFair warning: It\\'s going to get worse \\nLi says his company, Pinscreen, will demonstrate for attendees of the World Economic Forum in Davos how to livestream fake video.\\nHe says it\\'s part of an effort to raise awareness of the need for technologies that root out deepfakes.\\n“In real-time, you can reenact yourself as Will Smith or some celebrity or a politician. You can be George Bush,\" he says. \"The results are very convincing.\"\\nPublished 9:16 PM EST Jan 8, 2020',\n",
              "  array([-0.09439989, -3.609378  , -2.7644217 ], dtype=float32),\n",
              "  0),\n",
              " ('Storied film company Warner Bros. has signed a deal with Cinelytic, an LA startup that uses machine learning to predict film success. A story from The Hollywood Reporter claims that Warner Bros. will use Cinelytic’s algorithms “to guide decision-making at the greenlight stage,” but a source at the studio told The Verge that the software would only be used to help with marketing and distribution decisions made by Warner Bros. Pictures International.\\n“RIGHT NOW, AN AI CANNOT MAKE ANY CREATIVE DECISIONS”\\nIn an interview with THR, Cinelytic’s CEO Tobias Queisser stressed that AI was only an assistive tool. “Artificial intelligence sounds scary. But right now, an AI cannot make any creative decisions,” Queisser told the publication. “What it is good at is crunching numbers and breaking down huge data sets and showing patterns that would not be visible to humans. But for creative decision-making, you still need experience and gut instinct.”\\nRegardless of what Cinelytic’s technology is being used for, the deal is a step forward for Hollywood’s slow embrace of machine learning. As The Verge reported last year, Cinelytic is just one of a new crop of startups leveraging AI to forecast film performance, but the film world has historically been skeptical about their ability.\\nRELATED\\nHollywood is quietly using AI to help decide which movies to make\\nAndrea Scarso, a film investor and Cinelytic customer, told The Verge that the startup’s software hadn’t ever changed his mind, but “opens up a conversation about different approaches.” Said Scarso: “You can see how, sometimes, just one or two different elements around the same project could have a massive impact on the commercial performance.”\\nCinelytic’s software lets customers play fantasy football with films. Users can model a pitch; inputting genre, budget, actors, and so on, and then see what happens when they tweak individual elements. Does replacing Tom Cruise with Keanu Reeves get better engagement with under-25s? Does it increase box office revenue in Europe? And so on.\\nSTUDIES SHOW ALGORITHMS HAVE LIMITED PREDICTIVE ABILITY\\nMany AI experts are skeptical about the ability of algorithms to make predictions in a field as messy as filmmaking. Because machine learning applications are trained on historical data they tend to be conservative, focusing on patterns that led to past successes rather than predicting what will excite future audiences. Scientific studies also suggest algorithms only produce limited predictive gains, often repeating obvious insights (like “Scarlett Johansson is a bankable film star”) that can be discovered without AI.\\nBut for those backing machine learning in filmmaking, the benefit is simply that such tools produce uncomplicated analysis faster than humans can. This can be especially useful at film festivals, notes THR, when studios can be forced into bidding wars for distribution rights, and have only a few hours to decide how much a film might be worth.\\n“We make tough decisions every day that affect what — and how — we produce and deliver films to theaters around the world, and the more precise our data is, the better we will be able to engage our audiences,” Warner Bros.’ senior vice president of distribution, Tonis Kiis, told THR.\\nUpdate January 8, 11:00AM ET: Story has been updated with additional information from a source at Warner Bros.',\n",
              "  array([-3.5245805, -3.7003148, -0.0557029], dtype=float32),\n",
              "  2),\n",
              " ('LAS VEGAS — Trump administration officials this week promoted a light-touch approach to regulating self-driving cars and artificial intelligence at one of largest technology conferences in the world. \\n\\nTransportation Secretary Elaine Chao unveiled in a speech at CES a new self-driving car policy that largely allows companies to take the wheel on advancing autonomous vehicle technology with limited intervention from the government. \"It should not be the role of the federal government to pick winners or losers,\" she said. \\n\\nMeanwhile, U.S. Chief Technology Officer Michael Kratsios warned federal agencies against over-regulating companies developing artificial intelligence during a pair of appearances here. He touted a series of AI principles that the White House released earlier this week, that stipulate the government should ensure AI is safe and unbiased but engage with industry in any rules it develops to ensure that. \\n\\n\\n“If we’re too heavy-handed with artificial intelligence, we end up stifling entire industries, and we want to make sure to foster the generation in the United States,” Kratsios said during a conference event hosted by Wired. \\n\\nThe industry-friendly announcements and officials’ optimistic focus on the promise of innovation stood in sharp contrast with much of President Trump\\'s typical rhetoric about Silicon Valley. Though the president is known for his attacks and threats of regulation on individual companies, tech companies stand to greatly benefit from the laissez-faire strategy on display here as they invest heavily in artificial intelligence and self-driving cars. \\n\\nAmazon, one of the companies that has been most aggressively targeted by Trump, praised the White House\\'s AI principles in a statement yesterday. (Amazon chief executive Jeff Bezos owns The Washington Post.) \\n\\n\\nGary Shapiro, the president and chief executive of the Consumer Technology Association that represents many large tech giants, said the policy \"is proof government is prioritizing the adoption of self-driving technology.\" \\n\\nBut the White House\\'s hands-off approach could prove controversial amid greater debate in Washington about the tech industry\\'s unchecked rise in power and calls for greater guardrails. \\n\\nThe autonomous vehicle policy sought to place an emphasis on safety, but offered little in way of specifics about how federal agencies would ensure that, my colleague Ian Duncan noted. \\n\\nAnd companies have already been aggressively rolling out test vehicles on the road. Federal investigators called for the government to do more to ensure self-driving vehicles being tested on the road are safe following the investigation of a fatal crash involving an autonomous Uber vehicle in Arizona. \\n\\n\\nNational Transportation Safety Board member Jennifer Homendy told Ian yesterday that the new policy did not address those concerns.\\n\\n“It’s very light on content,” she said. “They say safety is their priority and that’s fantastic, but they don’t back that up.”',\n",
              "  array([-0.45535934, -2.3199024 , -1.3186477 ], dtype=float32),\n",
              "  0),\n",
              " ('Microsoft has developed an automated system to identify when sexual predators are trying to groom children within the chat features of video games and messaging apps, the company announced Wednesday.\\nThe tool, codenamed Project Artemis, is designed to look for patterns of communication used by predators to target children. If these patterns are detected, the system flags the conversation to a content reviewer who can determine whether to contact law enforcement.\\nCourtney Gregoire, Microsoft’s chief digital safety officer, who oversaw the project, said in a blog post that Artemis was a “significant step forward” but “by no means a panacea.”\\n“Child sexual exploitation and abuse online and the detection of online child grooming are weighty problems,” she said. “But we are not deterred by the complexity and intricacy of such issues.”\\nMicrosoft has been testing Artemis on Xbox Live and the chat feature of Skype. Starting Jan. 10, it will be licensed for free to other companies through the nonprofit Thorn, which builds tools to prevent the sexual exploitation of children.\\nThe tool comes as technology companies are developing artificial intelligence programs to combat a variety of challenges posed by both the scale and the anonymity of the internet. Facebook has worked on AI to stop revenge porn, while Google has used it to find extremism on YouTube.\\nGames and apps that are popular with minors have become hunting grounds for sexual predators who often pose as children and try to build rapport with young targets. In October, authorities in New Jersey announced the arrest of 19 people on charges of trying to lure children for sex through social media and chat apps following a sting operation.\\nSecurity camera hacked in Mississippi family’s child’s bedroom\\nDEC. 12, 201901:10\\nMicrosoft created Artemis in conjunction with the online children’s game Roblox, messaging app Kik and the Meet Group, which makes dating and friendship apps including Skout, MeetMe and Lovoo. The collaboration started in November 2018 at a Microsoft hackathon focused on child safety.\\nArtemis builds on an automated system Microsoft started using in 2015 to identify grooming on Xbox Live, looking for patterns of keywords and phrases associated with grooming. These include sexual interactions, as well as manipulation techniques such as detachment from friends and family.\\nThe system analyzes conversations and assigns them an overall score indicating the likelihood that grooming is happening. If that score is high enough, the conversation will be sent to moderators for review. Those employees look at the conversation and decide if there is an imminent threat that needs referring to law enforcement or, if the moderator identifies a request for child sexual exploitation or abuse imagery, the National Center for Missing and Exploited Children is contacted.\\nThe system will also flag cases that might not meet the threshold of an imminent threat or exploitation but violate the company’s terms of services. In these cases, a user could have their account deactivated or suspended.\\nThe way Artemis has been developed and licensed is similar to PhotoDNA, a technology developed by Microsoft and Dartmouth College professor Hany Farid, that helps law enforcement and technology companies find and remove known images of child sexual exploitation. PhotoDNA converts illegal images into a digital signature known as a “hash” which can be used to find copies of the same image when they are uploaded somewhere else. The technology is used by more than 150 companies and organizations including Google, Facebook, Twitter and Microsoft.\\nFor Artemis, developers and engineers from Microsoft and the partners involved fed historical examples of patterns of grooming they had identified on their platforms into a machine learning model to improve its ability to predict potential grooming scenarios, even if the conversation hadn’t yet become overtly sexual. It is common for grooming to start on one platform before moving to a different platform or a messaging app.\\nEmily Mulder from the Family Online Safety Institute, a nonprofit dedicated to helping parents keep kids safe online, welcomed the tool and noted that it would be useful for unmasking adult predators posing as children online.\\n“Tools like Project Artemis track verbal patterns, regardless of who you are pretending to be when interacting with a child online. These sorts of proactive tools that leverage artificial intelligence are going to be very useful going forward.”\\nHowever, she cautioned that AI systems can struggle to identify complex human behavior. “There are cultural considerations, language barriers and slang terminology that make it hard to accurately identify grooming. It needs to be married with human moderation.”',\n",
              "  array([-3.2300835, -3.9119663, -0.0614024], dtype=float32),\n",
              "  2),\n",
              " ('TOKYO — Japanese fashion tycoon Yusaku Maezawa is giving away $9 million to his Twitter followers in what he says is a \"social experiment\" to see if the payment boosts their happiness.\\nMaezawa will give 1 million yen — about $9,000 each — to 1,000 followers selected at random from those who retweeted a Jan. 1 post, with the impact of the money to be tracked through regular surveys.\\n\"It\\'s a serious social experiment,\" said Maezawa on YouTube, adding he hopes to attract interest from academics and economists.\\nMaezawa, who is to be the first private passenger to fly around the moon with Elon Musk\\'s SpaceX, is known for his high spending on art and sports cars but also has a predilection for musing on ideas like a world without money.\\nHe tied the giveaway to the idea of basic income, or the theory of providing a periodic no-strings-attached payment to all citizens, that has gained traction in some political circles and is backed by Democratic U.S. presidential candidate Andrew Yang.\\n\"Basic means a regular minimum amount offering a sense of security, what Maezawa is offering is totally different,\" said Toshihiro Nagahama, senior economist at Dai-ichi Life Research Institute.\\nMaezawa said that given that he \"has the money and free time\" to make the payments, he felt the need to try and inspire greater debate over the merits of the theory in Japan.\\nThe idea of a universal basic income has gained support over fears technology such as artificial intelligence will wipe out large numbers of jobs but that concern is for now less pronounced in Japan with its tight labor market, said Nagahama.\\nIts the second, larger, giveaway by the entrepreneur, who in November secured a $900 million payday through the sale of his online fashion business Zozo Inc to SoftBank Group Corp.\\nMaezawa, who recently grabbed headlines after his split from actress girlfriend Ayame Goriki, has gathered almost 7 million followers on Twitter with his mix of displays of conspicuous consumption and folksy pronouncements on the meaning of life.\\nYouTube is the latest online outlet for the businessman, with videos including a tour of his private jet, a visit to the barber to dye his hair and updating his bank book after November\\'s windfall.\\nThe debate over basic income comes as income inequality continues to grow in the United States, where in recent years some of its wealthiest entrepreneurs, from Microsoft co-founder Bill Gates to investor Warren Buffett, have pledged to give away most of their wealth.',\n",
              "  array([-4.19936   , -2.7493455 , -0.08226801], dtype=float32),\n",
              "  2),\n",
              " (\"LAS VEGAS — Amid the flying taxis, cat exercise machines and companion robots on display this year at the world’s largest consumer tech conference, a different kind of company was cropping up.\\n\\nTake AoAir’s Atmos face mask, a clear plastic bubble that fits over your nose and mouth, framing them with multicolored lights like a dystopian fashion statement.\\n\\nThe battery-powered air filter isn’t something for the future. It’s made for the 95 percent of the world’s population who live, commute and work in areas with polluted air. The two-phase air filtration system can clean smoke from wildfires, such as those ravaging Australia. It can also provide more information about air quality.\\n\\nCES 2020: Anxiety, cats and yet another streaming service, called Quibi\\n\\n“We’re facing realities unlike before,” said Mikal Peveto, U.S. head of AoAir. “Unless people really know what air they’re breathing in, they don’t really act.”\\n\\nAD\\n\\nThe mask was on display at CES, a 53-year-old conference that more typically reflects the shiny utopian future of technology, with robots doing all the hard work while people kick back in their spotless Internet-connected smart homes, watching obscenely large 8K TVs.\\n\\nThese are the coolest and weirdest gadgets at CES 2020\\nThe Washington Post's Geoffrey Fowler and Heather Kelly are at CES 2020 to find the coolest and weirdest gadgets of the future. (James Pace-Cornsilk/The Washington Post)\\nWith each CES, more reality creeps in. For the second consecutive year, the event had a section focused on climate change-related technology with the optimistic name “Resilience.” Other devices on display hinted at general heightened anxiety and helicopter parenting of the digital age, with tech to help you sleep, meditate and track every thing and person in your life to make sure they’re okay.\\n\\nIvanka Trump CES keynote address sparks backlash\\n\\nPrompting even more introspection, smartphones across the conference were lighting up with alerts on unrest in Iran as some muted TVs played the news. It was a serious and stressful interruption that brought a more somber tone to the somewhat silly parade of such gadgets as a robot that delivers a roll of toilet paper.\\n\\nAD\\n\\nIt is not the event’s first collision with reality. Last year, government officials including Ajit Pai, chairman of the Federal Communications Commission, and Transportation Secretary Elaine Chao had to cancel CES appearances because of the U.S. government shutdown. In 2018, security at the event was at an all-time high just months after the mass shooting a few miles away.\\n\\nIn front of the convention center this year, Zero Mass Water set up its hydro-panels to collect moisture from the air and offer attendees samples of the water. Like many people this week, Zero Mass chief executive Cody Friesen was closely following news on the Middle East and Australia. He said he could see connections between what was in the news and the companies at CES.\\n\\nAt CES, Apple, Facebook and Amazon are preaching privacy. Don’t believe the hype.\\n\\n“A huge percentage of the wars we fight now are around water stress, around resource stress. Things we could solve. We could hopefully de-escalate conflicts, not in a utopian way but in a realistic way,” Friesen said.\\n\\nNo single event encapsulated the contrast between the world CES imagines and the one we live in like the keynote talk from Ivanka Trump on Tuesday. During a session on the future of work, Trump talked with Gary Shapiro, president of the Consumer Technology Association, about job retraining and skipping college. She never addressed or acknowledged her father’s pending impeachment trial, the tense military standoff with Iran or climate change.\\n\\nIvanka Trump, daughter and senior adviser to President Trump, speaks at CES. (Ross D. Franklin/AP) \\nIvanka Trump, daughter and senior adviser to President Trump, speaks at CES. (Ross D. Franklin/AP)\\nOverlaps between the unpleasantness of the real world and the optimism of the technology industry is nothing new. While tech giants like Facebook and Google once set out to make the world more connected and better informed, in recent years they’ve struggled with the spread of disinformation and election interference. Companies including Microsoft, Amazon and Google have come under fire for working with law enforcement and pursuing Defense Department contracts. (Amazon CEO Jeff Bezos owns The Washington Post.)\\n\\nAD\\n\\nCES 2020 preview: Surveillance, sex toys and futuristic gadgets\\n\\nOn the show floor, people crowded around self-driving delivery vans, flying taxis, companion robots for the elderly, laptops with foldable screens and computer-generated customer service representatives. They took rides on the next generation of transportation options, like self-balancing scooters and Segway’s odd-looking S-Pod people-mover, which looked like something straight out of “WALL-E” — a cautionary movie about the environment and outsourcing everything to technology.\\n\\nThe Segway S-Pod might soon be how we get around\\nSegway announced its new S-Pod at CES 2020, and it might soon carry you through an airport. (James Pace-Cornsilk, Heather Kelly/The Washington Post)\\nMeanwhile, more of the exhibitors this year were prepping for a bleaker future.\\n\\nThe resilience category, reserved for innovations to help with disasters or such issues as rising sea levels, pollution and water shortages, included almost 40 companies, some grouped together and others spread around the show. Their tech didn’t draw as much attention as the flashy consumer hardware or innovations that reflected the future people want. But they were, perhaps, more realistic about the future to which we’re headed.\\n\\nAD\\n\\nThe most disappointing technologies of the decade: Face computers, space tourism and Juicero\\n\\nThere was Senegalese company Dictaf, which says it uses artificial intelligence to help farmers improve their crops, and Corners, a South Korean company developing evacuation systems to better help people survive such disasters as gas leaks or, for U.S. customers, active-shooter events. Multiple companies, including John Deere and Odd.bot, are working on weeding technology to help farmers use fewer chemical herbicides.\\n\\nA handful of companies were focused on the environment and climate change. There was technology aimed at cities and towns facing pollution and climate issues. The BeachBot is a small, rugged autonomous vehicle from the Netherlands designed to clean up beaches using object detection. Its makers hope to start selling it to local governments next month. Korean company N.thing showed off its modular indoor farm systems in stackable shipping containers, which grow vegetables in locations where the climate has become too dry or hot for some outdoor farming.\\n\\nGigabyte Technology's smart agriculture system is displayed at CES. (Bridget Bennett/Bloomberg News) \\nGigabyte Technology's smart agriculture system is displayed at CES. (Bridget Bennett/Bloomberg News)\\nSome corners of the conference are a prepper’s paradise, with solutions for anyone interested in living off the grid just in case existing infrastructure collapses. There are DIY greenhouses for growing your own food, solar panels for homes and batteries to store enough electricity to weather power outages. Air filtration systems promised to make air more breathable, inside homes and cars or during time outside.\\n\\nAD\\n\\nThe Technology 202: Silicon Valley will face new challenges in 2020. Here's what we're watching.\\n\\nAnother mask — AirBliss Plus’s more traditional-looking smart air pollution mask — was designed to be comfortable for prolonged wear.\\n\\nClimateSeed didn’t have any technology or fun hardware prototypes at its simple booth in the crowded basement of the Sands, the location of the smaller start-ups’ exhibits. The company takes money from businesses hoping to offset their carbon footprint, then invests it in projects, such as preventing deforestation in Brazil. The idea is to cancel out, on paper anyway, the damage businesses might do to the environment through such things as manufacturing.\\n\\nEdoardo Bertin, ClimateSeed’s head of marketing and partnerships, said he hadn’t seen much innovation around addressing climate change.\\n\\nImpossible Foods unveils its new 'pork' products at CES\\n\\n“It’s strange to see the four thousand exhibitors and only a really small part focused on the environment,” Bertin said.\\n\\nZero Mass Water’s Friesen thinks the intelligence on display at CES, and in the technology sector around the world, could be applied to bigger problems and help humans live better lives.\\n\\n“There’s a huge opportunity to create solutions that both provide joy to people and make their lives better, as opposed to just simply another screen with higher resolution.”\",\n",
              "  array([-4.650707  , -4.0713124 , -0.02697032], dtype=float32),\n",
              "  2),\n",
              " ('Segway\\'s prototype wheelchair crashed during a demonstration at the CES tech show.\\n\\nThe S-Pod - a self-balancing electric wheelchair - was being tested by a journalist at the time. The rider had accelerated the vehicle before accidently crashing into a wall.\\n\\nIts maximum speed is 24mph (38km/h). The company said no one was injured.\\n\\nThe crash made the S-Pod unavailable for further demos, but analysts say the company should not face lasting damage.\\n\\n\"In no way is a [malfunction] a total loss. It is still a sign to the public that the company is close to the finished product,\" said Ross Rubin, principal analyst at Reticle Research.\\n\\nThe S-Pod is designed to be driven in enclosed spaces such as airports, theme parks and work campuses.\\n\\nSegway\\'s director of marketing Jeff Wu told the BBC the concept model did not have a safety belt, but that his company intended to add one.\\n\\nThe Chinese firm did not say how much it had spent on the prototype, but companies often spend millions of dollars developing products to have them ready to exhibit at the annual Las Vegas expo.\\n\\nThe S-Pod is expected to go on sale in early 2021.\\n\\nThe demo model had received significant media attention for its design. It is inspired by the geospheres in the film Jurassic World. Many on social media have also compared the vehicle to the hover chairs in the animated Pixar film Wall-E.\\n\\nImage copyrightSEGWAY\\nImage caption\\nSegway\\'s self-balancing electric chair was inspired by the movie Jurassic World\\nSegway is best known for its electric scooters, which are controlled by riders moving forwards, backwards or to the side. However, the S-Pod is steered via a joystick on its armrest.\\n\\nThe crash is the latest in a long history of mishaps at the tech show.\\n\\n\"One reason that companies increasingly release products at their own separate events is because they have [greater] control over the environment,\"commented Mr Rubin.\\n\\nEven so, trade shows can be still a valuable way to show off new concepts to large audiences, especially for smaller brands, he added.',\n",
              "  array([-4.4201694 , -3.6802108 , -0.03796131], dtype=float32),\n",
              "  2),\n",
              " ('Cutting the cord: As prices go up, here\\'s how you can still save money streaming\\nMike Snider USA TODAY\\nPublished 12:01 PM EST Jan 15, 2020\\nFor years, cutting the pay-TV cord has been seen as a way to save money – you don\\'t pay for channels you don\\'t watch and are free from long-term contracts.\\nBut in reality, the landscape and benefits constantly change as content shifts between providers and new streaming players emerge. The latest development: higher prices.\\nAll of the main live TV streaming services – such as AT&T TV Now (formerly DirecTV Now), fubo TV, Hulu + Live TV, Sling TV, and YouTube TV – have seen price increases within the last year.\\nAmong the most recent moves, Sling TV announced a $5 increase to its monthly subscriptions two weeks ago. And earlier in December, Hulu raised the monthly price for its Live TV service, which has more than 60 channels, by $10 monthly.\\nProviding all of the channels and content viewers want comes at a cost. The high price of programming led Sony to decide to shutter its Playstation Vue live TV service, which had operated for five years, at the end of this month.\\nRising prices mean you are likely going to pay about $50 or more for a service to stream live TV including the most popular channels such as Fox News, MSNBC, CNN and ESPN.\\nSo can you really save by opting to cut the cord?\\nThat depends on what you want to watch and how you price it out.\\nAnd another one: New Quibi streaming service to give bite-size shows from J.Lo, Spielberg and The Rock\\nStreaming live and local: You can finally stream your PBS stations online – on YouTube TV\\nUS & Iran: The risk of an Iran cyberattack is up after missile strike on Iraqi military bases\\nBroadband and beyond\\nLet\\'s break down the costs. To cut the cord, first, you need broadband internet service, although some make do with ultimate wireless plans allowing them to stream on mobile devices.\\nMost U.S. homes (82%) already have broadband, paying about $67 monthly for the connection, according to Leichtman Research Group. That\\'s a lot cheaper than the $109.60 that the research firm says pay-TV subscribers pay monthly. But that\\'s not the end of your costs; you still have to pay for the content you stream.\\nSadie Sink, Noah Schnapp, Millie Bobby Brown, Finn Wolfhard, and Caleb McLaughlin in Stranger Things 3 .\\nNetflix.\\nIf you are a pay-TV customer currently paying as much as $100 monthly – not counting your broadband costs – and opt to cut the pay-TV cord, you could likely save money.\\nYou could save as much as $360 a year relying on an antenna, YouTube and free advertising-supported streaming services such as the Roku Channel, Pluto TV, Tubi, Crackle and Xumo, which collectively offer live news and  thousands of movies and TV episodes. The movie offerings aren\\'t first-run, but a sampling includes \"The China Syndrome\" (Crackle), \"Zodiac\" (Pluto TV), \"The Dark Knight\" (Roku Channel) , \"A.I. Artificial Intelligence\" (tubi), and \"Cloverfield\" (Xumo)\\nHowever, if that lineup doesn\\'t suffice and you decide to pay for content, be choosy or you could quickly rack up streaming charges of up to $200 monthly.\\nSavings: $60 or more, if you keep to subscribing to only a few services, such as Netflix, Hulu on demand and one premium network such HBO or Showtime.\\nThe real cost of cutting the cord: What streaming companies don’t want you to know\\nSubscription overload: So many streaming services, so many bills\\nStreaming live TV: What does it cost?\\nIf you prefer something more like a cable service with local TV channels and major cable networks, most services such as AT&T TV Now, fubo TV, Hulu + Live TV, and YouTube TV carry some local channels in most markets. And Sling TV, which carries only Fox or NBC channels, has an affiliate in several markets, too.\\nTo maintain that $50 or so of savings from dropping pay-TV, choose YouTube TV ($49.99 monthly). Each of the others costs a bit more and AT&T TV Now starts at $65 monthly but includes HBO.\\nA note: Before subscribing, you want to check with your internet provider. If you get your internet and TV from the same company, as most customers do, dropping your TV service could result in a higher monthly cost for broadband, too. A very recent development: Verizon Fios customers can now change their pay TV and broadband service without penalty, regardless of any previous contract.\\nIf savings isn\\'t the main goal, you can pay $80 monthly for AT&T TV Now\\'s Max package with 60-plus channels, HBO and Cinemax. Each of the other services lets you add additional channels including premium networks for an additional fee.\\nSometimes, the decision of whether to stream is made for you. If you live somewhere with subpar broadband service, you may not be able to rely on streaming. YouTube TV recommends 7 Mbps for one HD stream and 13 Mbps to do so with other devices on your home network.\\nMy sister and her family in Kansas pay $127 for Dish Network and $35 for internet that delivers only 2.2 Mbps. For them, cutting the cord is dicey because streaming quality will likely suffer.\\nMy parents do stream ESPN+ but their broadband speeds top out at 10 Mbps – another case where streaming multiple devices at one time could lead to quality issues. That – and the ease of use of a remote and programming guide – keeps them tethered to Dish Network, too, at $110 monthly for TV, broadband and home phone.\\nCo-workers and friends pay from $50 to $235 monthly for various pay-TV, broadband and streaming content packages.\\nBut what about news and sports?\\nLive news and sports, especially, can throw a curve into your plans to cut the cord – and to cut costs.\\nFor a full menu of live news and sports (to watch local teams)  AT&T TV Now, Hulu + Live TV, and YouTube TV all have local networks, Fox News, CNN and MSNBC and several sports channels including ESPN, FS1, FS2 and some regional sports networks. Fubo TV compares favorably for news and sports – it has more than 40 channels that carry sports – but does not have ESPN and also recently lost some regional sports networks.\\nOct. 9: Washington Nationals left fielder Juan Soto celebrates after hitting a solo home run during the eighth inning of Game 5 of the NLDS.\\nRobert Hanashiro, USA TODAY Sports\\nBut streaming isn\\'t a solution for all everyone. For instance, Washington, D.C., and Baltimore area fans of the Washington Nationals and Baltimore Orioles cannot stream most of the games locally, so a traditional pay TV service is required. Even an MLB.TV subscription won\\'t solve that problem because local live broadcasts are blacked out, an issue that can hit fans of other sports teams across the U.S.\\nWhere I live in northern Virginia, that means a broadband/TV bundle from Cox Cable ($69.99 and up monthly) or Verizon Fios ($89.99 and up monthly). For pay TV alone, there\\'s satellite service from DirecTV ($60 and up) or Dish Network ($70 and up). Those prices are before any additional fees or taxes.\\nIf you don\\'t need live sports or news, you may be satisfied with on-demand services such as Netflix, Amazon, Hulu, HBO, and Showtime. There\\'s also Philo ($20 monthly), which has 58 channels including AMC, BBC America, Comedy Central, Discovery Channel, Food Network and HGTV.\\nSavings: $45 or more, if you swapped a pay-TV package of $100 or so for a broadband-delivered service of $50 to $55. \\nTips to cut your streaming bill\\nMost likely, you will want a combination of on-demand content from a service like Netflix or Disney+ and some live channels. That can easily push your monthly bill above $100.\\nSo how do you save any money?\\nLearn to game the system. Drop a service when there\\'s nothing vital on there for you – remember, you did not sign a long-term contract. So you can save on monthly fees by dropping a service once you have watched the content you want and re-up when you want to watch again.\\nDisney Plus via AP\\nStreaming providers have begun to expect this behavior. The Disney-controlled Hulu told subscribers in November that if its price increase for live TV hurt their pocketbook, to \"switch to one of Hulu\\'s less expensive on-demand plans\" when a sports season ends or a TV series takes a break.\\nThe practice is \"becoming increasingly commonplace among subscribers,\" wrote Rich Greenfield, tech/media/telecom analyst for LightShed Partners, in a blog post. But it was \"nice to see Disney management embracing change vs. fighting it, even if it hurts their legacy, highly-profitable assets.\"\\nProviders should expect that, fubo TV co-founder and CEO David Gandler told USA TODAY. \"I don’t believe people will maintain all these services 12 months a year,\" he said. \"They will be in and out of these things.\"\\nYou can learn to make money-saving moves. For instance, if you are a fan of \"The Walking Dead,\" rather than pay for live TV service you could wait until the end of the season this spring and then subscribe to Philo for one month. Or perhaps binge the entire season during Philo\\'s free seven-day trial for new members. Or you could wait until the latest season lands on Netflix or another service you already subscribe to.\\nIf you don\\'t need every news or sports channel, you could get the CBS All Access app, which has the CBSN news channel and your local CBS network. Or subscribe to Sling TV\\'s Orange package ($30 monthly after $20 the first month), which has ESPN, ESPN 2, and CNN, or Sling Blue package ($30 monthly after $20 the first month) to get CNN, Fox News and MSNBC.\\nScale it all back. Again, an antenna could be an alternative for some who want to watch local TV channels for local news and sports.\\nThey say sharing is caring. Of course, many of you already use passwords shared with you by family and friends. Many services allow two or three viewers at the same time. So if your daughter or son or parents pay for a service, see if you can piggyback.\\nConsumers expect to spend about $5 more each month for content, but that doesn\\'t mean they will be loyal to a service, says Todd Supplee, entertainment and media partner at PricewaterhouseCoopers US. The consulting firm surveyed 2,016 U.S. adults aged 18 to 59 with annual household incomes above $40,000 conducted in October 2019 and found they spend an average of $76 monthly on video content.\\nSome homes – even those with pay-TV service – may have five to 10 streaming subscriptions and say they are likely to add more, PwC found. More than half (56%) also said they would terminate an existing service to try a new one.\\n\"As the larger studios enter the streaming market with the power of their catalog, the competition will heat up, and consumers will hop services as the content changes,\" Supplee said.\\nFollow USA TODAY reporter Mike Snider on Twitter: @MikeSnider.\\nPublished 12:01 PM EST Jan 15, 2020',\n",
              "  array([-4.0354958 , -3.0875545 , -0.06538188], dtype=float32),\n",
              "  2),\n",
              " ('CES 2020: EVs, 8K TVs and streaming – this is the future Sony says it\\'s optimistic about\\nEdward C. Baig USA TODAY\\nPublished 9:44 AM EST Jan 10, 2020\\nSony CEO Kenichiro Yoshida in front of the company\\'s Vision-S electric vehicle prototype at CES.\\nEdward C. Baig\\nSony unveiled at CES 2020 its Vision-S – an electric concept car – as it looks forward to becoming a driving force on the road to the future. \\nBut don\\'t look to the global tech and entertainment giant to challenge Tesla or GM, at least not anytime soon. Sony is not going to sell you your next car. \\n“At this point in time, we have no plans for mass production,” CEO Kenichiro Yoshida told USA TODAY in an exclusive sit-down at the Sony booth.\\nInstead, Sony is focused on what goes into the car of the future, gathering real-time intelligence about road conditions and the environment in the name of safety. Vision-S has 33 such sensors, including LIDAR and radar.\\nThe car also has Sony’s 360-degree audio format, known as 360 Reality Audio, with speakers built into every seat. It has a custom contiguous display that stretches across the dash, revealing information and controls for the driver and passenger. It has always-on cloud connectivity. And it is built on a newly designed EV platform that Yoshida said can be applied to SUVs or other cars. \\nSony’s industry partners on the prototype include Bosch, BlackBerry, Magna, Nvidia and Qualcomm. \\nVerizon unbundling: Verizon makes it easier to get internet, TV\\n5G service is here: Do you really need to get a 5G phone now?\\nMobility is the megatrend of the new decade, according to Yoshida, fueled by a combination of technologies that include 5G and autonomous driving.\\n“When automotive becomes safer and more autonomous, (the) in-car space will become entertainment space, the time when people can be enjoying content,” he said. “That is a big opportunity for us as a creative entertainment company.”\\nA CES crowd surrounds Sony\\'s Vision-S concept car.\\nEdward C. Baig\\nYoshida also said that the car would be adaptable and largely defined by a \"software-oriented structure\" similar to mobile phones.\\nHere are other highlights from the conversation on other topics, edited for space and clarity:\\nUSA TODAY: A lot of people bought 4K TVs in the last couple of years. Does that suggest a challenge in selling the televisions with 8K resolution that Sony and its rivals have introduced at CES? Is 8K that much of a leap over 4K? And is there anything to watch?\\nYOSHIDA: Sony has businesses in content creation and (TV hardware). The creative side can shift quite quickly to 8K because they can down-convert to 4K. But the distribution side is not easy. For average users, if you have, say, a 50-inch or 65-inch TV, it’s quite difficult to find the difference between 4K and 8K. If you have 100 inches, you can find the difference.\\nUSA TODAY: Who can afford a TV that big?\\nYOSHIDA: That’s the point. I personally think the shift to 8K will be faster on the creation side, but it’s going to take some time for the distribution side. Television quality is not just the number of pixels. Also frame rate is important, particularly in sports, which has very quick movement. If the refresh rate is high enough you can get this very smooth image. Picture resolution is also of course important.\\nUSA TODAY: Let’s talk about the creation side. There’s been consolidation in Hollywood and speculation on where the Sony Pictures studio goes from here. Do you grow it, or sell?\\nYOSHIDA: The current trend is a race to DTC (\"direct-to-consumer\" streaming services): Netflix, Apple TV+, HBO Max, Peacock, Amazon Prime, Hulu and many more. At this point, Sony is in a quite unique position. Paramount is now combined with Viacom, so it seems Sony is the only independent studio which doesn’t have a strong connection with the distribution side.\\nSo the question is \"is that good or bad?\"\\nUSA TODAY: And the answer?\\nYOSHIDA: At this point, we leverage our current position. We can sell actually (to) everybody. We are selling “The Crown” to Netflix. And we are dealing with NBC (“The Blacklist.”) At this point, there is huge demand for content. Recently, (we made a deal selling) “Seinfeld,” a 30-year-old comedy to Netflix. I can’t tell you the amount, but it was quite a good deal.\\nSo there’s no simple answer, but what I can tell you is right now Sony Pictures is in a very unique position. We got a Golden Globe for “Once Upon a Time...In Hollywood.” Basically, I’m optimistic being an independent player. We are investing a lot in the picture side. TV production, movie production. \\nUSA TODAY? What have you learned in your experience as a business person that you can apply to the consumer?\\nYOSHIDA: In the past, monetary capital was capital. But now human capital is more important. And innovation comes from people. How to retain talented people is everything in management. \\nUSA TODAY: Do you worry that artificial intelligence will replace jobs?\\nYOSHIDA: New technologies always create other jobs. I’m optimistic about that.\\nEmail: ebaig@usatoday.com; Follow @edbaig on Twitter\\nPublished 9:44 AM EST Jan 10, 2020',\n",
              "  array([-4.934189  , -3.5961788 , -0.03523827], dtype=float32),\n",
              "  2),\n",
              " ('Traditionally, at the beginning of each year Facebook founder Mark Zuckerberg takes on a personal challenge.\\nThese have ranged from learning Mandarin, to visiting every state in the US, to last year\\'s promise to host public discussions about tech.\\nAfter a difficult year for Facebook, the tech billionaire has decided to ditch his yearly challenges in favour of longer-term reflection.\\nHe added how excited he is about \"a new private social platform\" in the future.\\nLaying out his thoughts in a blogpost, Mr Zuckerberg said: \"Rather than having year-to-year challenges, I\\'ve tried to think about what I hope the world and my life will look like in 2030.\"\\nBy then, he added, \"we\\'ll have the technology to feel truly present with another person no matter where they are, and scientific research will have helped cure and prevent enough diseases to extend our average life expectancy by another two-and-a-half years\".\\nHe also looked back to his own childhood, reflecting on how social networks need to refocus.\\n\"When I grew up in a small town, it was easy to have a niche and sense of purpose. But with billions of people, it\\'s harder to find your unique role. For the next decade, some of the most important social infrastructure will help us reconstruct all kinds of smaller communities to give us that sense of intimacy again.\\n\"Our digital social environments will feel very different over the next five-plus years, re-emphasising private interactions and helping us build the smaller communities we need in our lives.\"\\nMark Zuckerberg\\'s past challenges\\nMedia caption\\nMark Zuckerberg did a 30-minute question-and-answer session in Mandarin\\nIn 2018, after another bad year, he pledged to \"fix important issues\" on Facebook, such as abuse, hate speech and defending against nation state interference\\nHis 2017 challenge - to visit and meet people in every state in the US - sparked rumours he could be planning to run for president.\\nIn 2016 he set out to build an artificial intelligence system in his home\\nReading two books a month was his 2015 challenge\\nIn 2010 he promised to learn Mandarin Chinese and a few years later conducted an interview showing off his skills\\nOther challenges have including running, learning to hunt and cook and becoming more confident at public speaking\\nHe called once again for \"new forms of governance\", asking for governments around the world to establish clearer rules around what was and was not acceptable in online political marketing, among other things.\\nThis week the social network announced that it would not be making any major changes to its political advertising policy, reiterating its view that such decisions should not be made by private companies.\\nIt puts it odds with rivals Twitter and Google who have both pledged to crack down on political ads.\\nIt has been another tough year for Facebook.\\nIt has faced a series of hearings on issues such as privacy and misinformation, with Congress announcing in June that it would investigate Facebook and the big tech firms over anti-competitive behaviour.\\nIn 2019, the firm was also given a record $5bn (£3.8bn) fine over the Cambridge Analytica scandal.\\nAnd Mr Zuckerberg\\'s personal challenge for 2019 was also criticised as being \"a half-baked mission\". It emerged that he had spoken to only nine people in his quest to discuss the future of technology in society with \"leaders, experts and people in our community\". All were white and eight of them were men.',\n",
              "  array([-2.7400105, -0.4687111, -1.1724031], dtype=float32),\n",
              "  1),\n",
              " ('The last thing on Geru Drolma’s mind was becoming an internet celebrity. All she wanted was to make rent.\\nBut the steamed buns Drolma rose at 5 a.m. each morning to make in her village in western China’s Sichuan province just weren’t selling fast enough. So with the bills mounting up, Drolma set off to hunt for wild fungi she hoped to sell at the local market, following the same azalea-strewn mountain paths carved by generations of her fellow ethnic Tibetans before her.\\nFinding the best fungi varieties—like the sought-after matsutake, or pine mushroom—is not easy. The finest specimens only grow around the roots of pine trees from August to September at an elevation above 13,000 feet. But Drolma grew up foraging on the frigid Tibetan Plateau, and was well-trained by her father how to spot that telltale bulge of the earth, the loosened topsoil, which betrays a fattened mushroom ripe for picking.\\n“Matsutakes can only be found by experienced people,” Drolma, 22, tells TIME, adding with a laugh, “My husband, for example, hasn’t dug out a single one so far!”\\nMatsutakes are one of the world’s most valuable mushrooms. Impossible to commercially farm, they can command up to $1,000 per kilo in the tony delis of Tokyo or Shanghai. Not that people in Drolma’s remote village on the Tibetan plateau had any idea. That was until Drolma posted a cellphone video to live-streaming app Kuaishou of her trip for picking fungi.\\nThat post received 600,000 views; commenters swamped Drolma with positive feedback and requests for matsutake mushrooms and cordyceps, another fungus native to the region that grows on the bodies of caterpillars and is used in Traditional Chinese Medicine, and can fetch a whopping $20 per gram.\\nBefore long, Drolma had sold the family’s small shop and dedicated herself full time to live-streaming. As her celebrity grew, so did demand for fungi, leading Drolma to set up a collective with local villagers and farmers. Last year, the group generated $500,000 in revenue over the five-month picking season—an enormous windfall in what’s historically one of China’s most impoverished regions. Other than foraging, her posts showcase other aspects of traditional Tibetan life, such as roasting meat over campfires, dancing in traditional garb, and herding black pigs across the snow-capped landscape. Just over two years since she posted that initial viral clip, Drolma now has 1.9 million followers.\\n“My family strongly opposed our decision to concentrate on live-streaming at the beginning,” she recalls with a smile. “They didn’t understand online money you cannot see or touch, and said that I acted like a beggar by taking videos during private times like meals. But I never thought about giving up.”\\nIt’s a success story that highlights how even people in the most far-flung communities are being lifted out of poverty by technology—particularly artificial intelligence, or AI. Since China embraced economic reforms in the late 1970s and embarked upon an export-led boom, rural people sought fortunes in coastal manufacturing hubs, which often decimated the communities they left behind. But now, modern delivery services and AI-powered online marketing are allowing people like Drolma to develop flourishing businesses that showcase and enhance traditional life without leaving home. Eradicating poverty by 2020 is a key policy priority of China’s strongman, President Xi Jinping.\\nFor a while, it was Drolma’s natural charm that clicked with harried urbanites eager to rediscover China’s forgotten cultures. But key to connecting content provider and consumer were the algorithms employed by the Kuaishou live-streaming app that she uses, which has garnered 200 million daily active users since its launch in 2011.\\nAll uploaded content is forensically parsed: the facial expressions of those featured, any objects included or action taking place, what background music is playing, even the style of a protagonist’s dancing. Any words uttered are automatically transcribed by embedded voice recognition software and mined for keyword tags.\\nUsers, too, are evaluated depending on their location, whether they connect by Wi-Fi or 4G, and their behaviors on the platform, such as how often they click, comment and share videos. Kuaishou doesn’t only show users content that directly correlates to their interests, but also attempts to broader the topics they see depending on what works with similar profile types. That, the firm says, enhances users’ experience. The firm recorded close to $78 million in gross revenue in the second quarter of 2019, according to data analytics firm Sensor Tower, a 57% year-on-year rise.\\n“We want to give as many people as possible the opportunity to be seen by the world,” says Zheng Wen, head of AI for Kuaishou.\\nWhile most social media is dominated by a tiny number of celebrities and viral videos, what sets Kuaishou apart, says the firm, is the “democratization” of the 15 million videos uploaded every day by its 700 million registered users. The app helps laypeople without any photography training produce professional-style video clips—including Hollywood-esque special effects—with just a click of a button, and ensures that those clips are seen by a wide range of people.\\nKuaishou’s archive of 13 billion videos is filled with surprise stars, ranging from wacky amateur inventors, rural schoolchildren who must scale cliffs via rickety ladders to reach class, to chefs who specialize in giant seafood. An online store and gift application allows easy monetization.',\n",
              "  array([-3.598406  , -2.433315  , -0.12229506], dtype=float32),\n",
              "  2),\n",
              " ('LAS VEGAS (Reuters) - Futuristic sex toys that allow the user to control a vibrator from across the country or achieve “blended orgasms” got some visitors at the Las Vegas Consumer Electronics Show hot under the collar this week.\\nThe blueMotion Nex3 vibrator by OhMiBod debuted at the Consumer Electronics Show (CES) in Las Vegas, Nevada, U.S., January 9, 2020. REUTERS/Nathan Frandino\\nSome 170,000 visitors were expected at the annual show in Nevada, which showcases cutting edge consumer technology, with artificial intelligence and 5G applications at the forefront in 2020.\\nCompanies who sell sexual wellness-related products said they have not always found it easy to find a home at the show.\\nSuki Dunham, founder of OhMiBod, which makes a music-driven vibrator, said that in the past her booth was put in the wireless communications part of the show.\\n“We did have wireless communications, but we were a little bit, let’s say, out of place there. But people over time have really embraced our presence,” Dunham said from her booth in the Health and Wellness section.\\nThe vibrator is Bluetooth enabled and connects to an app, she explained.\\n“You could be here at Las Vegas and your partner can be in New York and you can control it remotely,” she said.\\nNearby, Avery Smith, an engineering assistant at Lora DiCarlo, explained the company’s new ‘Ose’ product, which uses microrobotics, to an enthusiastic-looking show attendee.\\n“It’s basically a dual stimulation sexual wellness product, so it’s a personal massager for internal and external stimulation to achieve a blended orgasm,” she said.\\nLora DiCarlo, the company’s founder, said sex tech was intended to help people.\\n“It’s done in a very healthy way. It’s done tastefully. And we really just want to see innovation that is truly innovative... and is still not demoralizing female bodies or objectifying anyone,” she said.\\nOther passers-by looked politely embarrassed. But there was no need for shame, said Candace Thome, a marketing assistant at Ergo-Fit, a company that debuted an inflatable, strapless strap-on device.\\n“No one likes to talk about things they don’t understand or they don’t know. But I think the more sex tech that is here, people are going to get comfortable with that,” she said.\\nThe Consumer Technology Association, which owns CES, said in a statement: “CES 2020 included tech-based sexual products as part of the Health & Wellness product category or in the Health & Wellness startup area of Eureka Park. Products had to be innovative and include new or emerging tech to qualify.”',\n",
              "  array([-4.8099504 , -4.1962023 , -0.02347428], dtype=float32),\n",
              "  2),\n",
              " ('The last decade was a big one for artificial intelligence but researchers in the field believe that the industry is about to enter a new phase.\\nHype surrounding AI has peaked and troughed over the years as the abilities of the technology get overestimated and then re-evaluated.\\nThe peaks are known as AI summers, and the troughs AI winters.\\nThe 10s were arguably the hottest AI summer on record with tech giants repeatedly touting AI\\'s abilities.\\nAI pioneer Yoshua Bengio, sometimes called one of the \"godfathers of AI\", told the BBC that AI\\'s abilities were somewhat overhyped in the 10s by certain companies with an interest in doing so.\\nThere are signs, however, that the hype might be about to start cooling off.\\n\\nKatja Hofmann, a principal researcher at Microsoft Research\\'s Game Intelligence group\\n\"I have the sense that AI is transitioning to a new phase,\" said Katja Hofmann, a principal researcher at Microsoft Research in Cambridge.\\nGiven the billions being invested in AI and the fact that there are likely to be more breakthroughs ahead, some researchers believe it would be wrong to call this new phase an AI winter.\\nRobot Wars judge Noel Sharkey, who is also a professor of AI and robotics at Sheffield University, told the BBC that he likes the term \"AI autumn\" - and several others agree.\\n\\'Feeling of plateau\\'\\nAt the start of the 2010s, one of the world leaders in AI, DeepMind, often referred to something called AGI, or \"artificial general intelligence\" being developed at some point in the future.\\nMachines that possess AGI - widely thought of as the holy grail in AI - would be just as smart as humans across the board, it promised.\\nDeepMind\\'s lofty AGI ambitions caught the attention of Google, who paid around £400m for the London-based AI lab in 2014 when it had the following mission statement splashed across its website: \"Solve intelligence, and then use that to solve everything else.\"\\nSeveral others started to talk about AGI becoming a reality, including Elon Musk\\'s $1bn AI lab, OpenAI, and academics like MIT professor Max Tegmark.\\nIn 2014, Nick Bostrom, a philosopher at Oxford University, went one step further with his book Superintelligence. It predicts a world where machines are firmly in control.\\nBut those conversations were taken less and less seriously as the decade went on. At the end of 2019, the smartest computers could still only excel at a \"narrow\" selection of tasks.\\nGary Marcus, an AI researcher at New York University, said: \"By the end of the decade there was a growing realisation that current techniques can only carry us so far.\"\\nHe thinks the industry needs some \"real innovation\" to go further.\\n\"There is a general feeling of plateau,\" said Verena Rieser, a professor in conversational AI at Edinburgh\\'s Heriot Watt University.\\nOne AI researcher who wishes to remain anonymous said we\\'re entering a period where we are especially sceptical about AGI.\\n\"The public perception of AI is increasingly dark: the public believes AI is a sinister technology,\" they said.\\nFor its part, DeepMind has a more optimistic view of AI\\'s potential, suggesting that as yet \"we\\'re only just scratching the surface of what might be possible\".\\n\"As the community solves and discovers more, further challenging problems open up,\" explained Koray Kavukcuoglu, its vice president of research.\\n\"This is why AI is a long-term scientific research journey.\\n\"We believe AI will be one of the most powerful enabling technologies ever created - a single invention that could unlock solutions to thousands of problems. The next decade will see renewed efforts to generalise the capabilities of AI systems to help achieve that potential - both building on methods that have already been successful and researching how to build general-purpose AI that can tackle a wide range of tasks.\"\\n\\'Far to go\\'\\nWhile AGI isn\\'t going to be created any time soon, machines have learned how to master complex tasks like:\\nplaying the ancient Chinese board game Go\\nidentifying human faces\\ntranslating text into practically every language\\nspotting tumours\\ndriving cars\\nidentifying animals.\\nThe relevance of these advances was overhyped at times, says ex-DeepMinder Edward Grefenstette, who now works in the Facebook AI Research group as a research scientist.\\n\\nEdward Grefenstette is a research scientist at Facebook in London\\n\"The field has come a very long way in the past decade, but we are very much aware that we still have far to go in scientific and technological advances to make machines truly intelligent,\" he said.\\n\"One of the biggest challenges is to develop methods that are much more efficient in terms of the data and compute power required to learn to solve a problem well. In the past decade, we\\'ve seen impressive advances made by increasing the scale of data and computation available, but that\\'s not appropriate or scalable for every problem.\\n\"If we want to scale to more complex behaviour, we need to do better with less data, and we need to generalise more.\"\\nNeil Lawrence, who recently left Amazon and joined the University of Cambridge as the first DeepMind-funded professor of machine learning, thinks that the AI industry is very much still in the \"wonder years\".\\nReality check\\nSo what will AI look like at the end of the 20s, and how will researchers go about developing it?\\n\"In the next decade, I hope we\\'ll see a more measured, realistic view of AI\\'s capability, rather than the hype we\\'ve seen so far,\" said Catherine Breslin, an ex-Amazon AI researcher.\\nThe term \"AI\" became a real buzzword through the last decade, with companies of all shapes and sizes latching onto the term, often for marketing purposes.\\n\"The manifold of things which were lumped into the term \"AI\" will be recognised and discussed separately,\" said Samim Winiger, a former AI researcher at Google in Berlin.\\n\"What we called \\'AI\\' or \\'machine learning\\' during the past 10-20 years, will be seen as just yet another form of \\'computation\\'\".',\n",
              "  array([-0.32370368, -2.100129  , -1.8701879 ], dtype=float32),\n",
              "  0),\n",
              " ('Artificial intelligence: How to measure the ‘I’ in AI\\nby BEN DICKSON — in SYNDICATION\\n20\\nSHARES\\nLast week, Lee Se-dol, the South Korean Go champion who lost in a historical matchup against DeepMind’s artificial intelligence algorithm AlphaGo in 2016, declared his retirement from professional play.\\n“With the debut of AI in Go games, I’ve realized that I’m not at the top even if I become the number one through frantic efforts,” Lee told the Yonhap news agency. “Even if I become the number one, there is an entity that cannot be defeated.”\\nEarly birds get all the good stuff\\nLike a great price on TNW2020 tickets\\nGIMME\\nPredictably, Se-dol’s comments quickly made the rounds across prominent tech publications, some of them using sensational headlines with AI dominance themes.\\nSince the dawn of AI, games have been one of the main benchmarks to evaluate the efficiency of algorithms. And thanks to advances in deep learning and reinforcement learning, AI researchers are creating programs that can master very complicated games and beat the most seasoned players across the world. Uninformed analysts have been picking up on these successes to suggest that AI is becoming smarter than humans.\\nBut at the same time, contemporary AI fails miserably at some of the most basic that every human can perform.\\nThis begs the question, does mastering a game prove anything? And if not, how can you measure the level of intelligence of an AI system?\\nTake the following example. In the picture below, you’re presented with three problems and their solution. There’s also a fourth task that hasn’t been solved. Can you guess the solution?\\nThe Abstraction Reasoning Corpus (ARC), introduced by AI scientist François Chollet, tests intelligence systems with few training examples. (Source: Arxiv.org)\\nYou’re probably going to think that it’s very easy. You’ll also be able to solve different variations of the same problem with multiple walls, and multiple lines, and lines of different colors, just by seeing these three examples. But currently, there’s no AI system, including the ones being developed at the most prestigious research labs, that can learn to solve such a problem with so few examples.\\nThe above example is from “The Measure of Intelligence,” a paper by François Chollet, the creator of Keras deep learning library. Chollet published this paper a few weeks before Le-sedol declared his retirement. In it, he provided many important guidelines on understanding and measuring intelligence.\\nIronically, Chollet’s paper did not receive a fraction of the attention it needs. Unfortunately, the media is more interested in covering exciting AI news that gets more clicks. The 62-page paper contains a lot of invaluable information and is a must-read for anyone who wants to understand the state of AI beyond the hype and sensation.\\nBut I will do my best to summarize the key recommendations Chollet makes on measuring AI systems and comparing their performance to that of human intelligence.\\nWhat’s wrong with current AI?\\n“The contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks, such as board games and video games,” Chollet writes, adding that solely measuring skill at any given task falls short of measuring intelligence.\\nIn fact, the obsession with optimizing AI algorithms for specific tasks has entrenched the community in narrow AI. As a result, work in AI has drifted away from the original vision of developing “thinking machines” that possess intelligence comparable to that of humans.\\n“Although we are able to engineer systems that perform extremely well on specific tasks, they have still stark limitations, being brittle, data-hungry, unable to make sense of situations that deviate slightly from their training data or the assumptions of their creators, and unable to repurpose themselves to deal with novel tasks without significant involvement from human researchers,” Chollet notes in the paper.\\nChollet’s observations are in line with those made by other scientists on the limitations and challenges of deep learning systems. These limitations manifest themselves in many ways:\\nAI models that need millions of examples to perform the simplest tasks\\nAI systems that fail as soon as they face corner cases, situations that fall outside of their training examples\\nNeural networks that are prone to adversarial examples, small perturbations in input data that cause the AI to behave erratically\\nHere’s an example: OpenAI’s Dota-playing neural networks needed 45,000 years’ worth of gameplay to reach a professional level. The AI is also limited in the number of characters it can play, and the slightest change to the game rules will result in a sudden drop in its performance.\\nThe same can be seen in other fields, such as self-driving cars. Despite millions of hours of road experience, the AI algorithms that power autonomous vehicles can make stupid mistakes, such as crashing into lane dividers or parked firetrucks.\\nWhat is intelligence?\\nOne of the key challenges that the AI community has struggled with is defining intelligence. Scientists have debated for decades on providing a clear definition that allows us to evaluate AI systems and determine what is intelligent or not.\\nChollet borrows the definition by DeepMind cofounder Shane Legg and AI scientist Marcus Hutter: “Intelligence measures an agent’s ability to achieve goals in a wide range of environments.”\\nThe key here is “achieve goals” and “wide range of environments.” Most current AI systems are pretty good at the first part, which is to achieve very specific goals, but bad at doing so in a wide range of environments. For instance, an AI system that can detect and classify objects in images will not be able to perform some other related tasks, such as drawing images of objects.\\nChollet then examines the two dominant approaches in creating intelligence systems: symbolic AI and machine learning.\\nSymbolic AI vs machine learning\\nEarly generations of AI research focused on symbolic AI, which involves creating an explicit representation of knowledge and behavior in computer programs. This approach requires human engineers to meticulously write the rules that define the behavior of an AI agent.\\n“It was then widely accepted within the AI community that the ‘problem of intelligence’ would be solved if only we could encode human skills into formal rules and encode human knowledge into explicit databases,” Chollet observes.\\nBut rather than being intelligent by themselves, these symbolic AI systems manifest the intelligence of their creators in creating complicated programs that can solve specific tasks.\\nThe second approach, machine learning systems, is based on providing the AI model with data from the problem space and letting it develop its own behavior. The most successful machine learning structure so far is artificial neural networks, which are complex mathematical functions that can create complex mappings between inputs and outputs.\\nFor instance, instead of manually coding the rules for detecting cancer in x-ray slides, you feed a neural network with many slides annotated with their outcomes, a process called “training.” The AI examines the data and develops a mathematical model that represents the common traits of cancer patterns. It can then process new slides and outputs how likely it is that the patients have cancer.\\nAdvances in neural networks and deep learning have enabled AI scientists to tackle many tasks that were previously very difficult or impossible with classic AI, such as natural language processing, computer vision and speech recognition.\\nNeural network-based models, also known as connectionist AI, are named after their biological counterparts. They are based on the idea that the mind is a “blank slate” (tabula rasa) that turns experience (data) into behavior. Therefore, the general trend in deep learning has become to solve problems by creating bigger neural networks and providing them with more training data to improve their accuracy.\\nChollet rejects both approaches because none of them has been able to create generalized AI that is flexible and fluid like the human mind.\\n“We see the world through the lens of the tools we are most familiar with. Today, it is increasingly apparent that both of these views of the nature of human intelligence—either a collection of special-purpose programs or a general-purpose Tabula Rasa—are likely incorrect,” he writes.\\nTruly intelligent systems should be able to develop higher-level skills that can span across many tasks. For instance, an AI program that masters Quake 3 should be able to play other first-person shooter games at a decent level. Unfortunately, the best that current AI systems achieve is “local generalization,” a limited maneuver room within their own narrow domain.\\nFrançois Chollet breaks down intelligence into a hierarchy of three layers. Current AI systems are still struggling at the bottom rung of this ladder. (Source: Arxiv.org)\\nThe requirements of broad and general AI\\nIn his paper, Chollet argues that the “generalization” or “generalization power” for any AI system is its “ability to handle situations (or tasks) that differ from previously encountered situations.”\\nInterestingly, this is a missing component of both symbolic and connectionist AI. The former requires engineers to explicitly define its behavioral boundary and the latter requires examples that outline its problem-solving domain.\\nChollet also goes further and speaks of “developer-aware generalization,” which is the ability of an AI system to handle situations that “neither the system nor the developer of the system has encountered before.”\\nThis is the kind of flexibility you would expect from a robo-butler that could perform various chores inside a home without having explicit instructions or training data on them. An example is Steve Wozniak’s famous coffee test, in which a robot would enter a random house and make coffee without knowing in advance the layout of the home or the appliances it contains.\\nElsewhere in the paper, Chollet makes it clear that AI systems that cheat their way toward their goal by leveraging priors (rules) and experience (data) are not intelligent. For instance, consider Stockfish, the best rule-base chess-playing program. Stockfish, an open-source project, is the result of contributions from thousands of developers who have created and fine-tuned tens of thousands of rules. A neural network-based example is AlphaZero, the multi-purpose AI that has conquered several board games by playing them millions of times against itself.\\nBoth systems have been optimized to perform a specific task by making use of resources that are beyond the capacity of the human mind. The brightest human can’t memorize tens of thousands of chess rules. Likewise, no human can play millions of chess games in a lifetime.\\n“Solving any given task with a beyond-human level performance by leveraging either unlimited priors or unlimited data does not bring us any closer to broad AI or general AI, whether the task is chess, football, or any e-sport,” Chollet notes.\\nThis is why it’s totally wrong to compare Deep Blue, Alpha Zero, AlphaStar or any other game-playing AI with human intelligence.\\nLikewise, other AI models, such as Aristo, the program that can pass an eighth-grade science test, does not possess the same knowledge as a middle school student. It owes its supposed scientific abilities to the huge corpora of knowledge it was trained on, not its understanding of the world of science.\\n(Note: Some AI researchers, such as computer scientist Rich Sutton, believe that the true direction for artificial intelligence research should be methods that can scale with the availability of data and compute resources.)\\nThe Abstraction Reasoning Corpus\\nIn the paper, Chollet presents the Abstraction Reasoning Corpus (ARC), a dataset intended to evaluate the efficiency of AI systems and compare their performance with that of human intelligence. ARC is a set of problem-solving tasks that tailored for both AI and humans.\\nOne of the key ideas behind ARC is to level the playing ground between humans and AI. It is designed so that humans can’t take advantage of their vast background knowledge of the world to outmaneuver the AI. For instance, it doesn’t involve language-related problems, which AI systems have historically struggled with.\\nOn the other hand, it’s also designed in a way that prevents the AI (and its developers) from cheating their way to success. The system does not provide access to vast amounts of training data. As in the example shown at the beginning of this article, each concept is presented with a handful of examples.\\nThe AI developers must build a system that can handle various concepts such as object cohesion, object persistence, and object influence. The AI system must also learn to perform tasks such as scaling, drawing, connecting points, rotating and translating.\\nA training example in the ARC dataset: The red object must be made adjacent to the blue box.\\n  In this example, the test taker must denoise the image, removing the blue dots (or any other color they might have) and keep the main objects intact.\\nAlso, the test dataset, the problems that are meant to evaluate the intelligence of the developed system, are designed in a way that prevents developers from solving the tasks in advance and hard-coding their solution in the program. Optimizing for evaluation sets is a popular cheating method in data science and machine learning competitions.\\nAccording to Chollet, “ARC only assesses a general form of fluid intelligence, with a focus on reasoning and abstraction.” This means that the test favors “program synthesis,” the subfield of AI that involves generating programs that satisfy high-level specifications. This approach is in contrast with current trends in AI, which are inclined toward creating programs that are optimized for a limited set of tasks (e.g., playing a single game).\\nIn his experiments with ARC, Chollet has found that humans can fully solve ARC tests. But current AI systems struggle with the same tasks. “To the best of our knowledge, ARC does not appear to be approachable by any existing machine learning technique (including Deep Learning), due to its focus on broad generalization and few-shot learning,” Chollet notes.\\nWhile ARC is a work in progress, it can become a promising benchmark to test the level of progress toward human-level AI. “We posit that the existence of a human-level ARC solver would represent the ability to program an AI from demonstrations alone (only requiring a handful of demonstrations to specify a complex task) to do a wide range of human-relatable tasks of a kind that would normally require human-level, human-like fluid intelligence,” Chollet observes.\\nThis story is republished from TechTalks, the blog that explores how technology is solving problems… and creating new ones. Like them on Facebook here and follow them down here:\\nRead next: Cisco certification is getting simpler and this $39 training is now all you need.\\nTECHARTIFICIAL INTELLIGENCEHUMANDATA\\nSHARE ON FACEBOOK (0)\\nSHARE ON TWITTER (0',\n",
              "  array([-0.27849558, -1.9997104 , -2.2283692 ], dtype=float32),\n",
              "  0),\n",
              " ('Computers can now drive cars, beat world champions at board games like chess and Go, and even write prose. The revolution in artificial intelligence stems in large part from the power of one particular kind of artificial neural network, whose design is inspired by the connected layers of neurons in the mammalian visual cortex. These “convolutional neural networks” (CNNs) have proved surprisingly adept at learning patterns in two-dimensional data—especially in computer vision tasks like recognizing handwritten words and objects in digital images.\\nOriginal story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research develop\\xadments and trends in mathe\\xadmatics and the physical and life sciences.\\nBut when applied to data sets without a built-in planar geometry—say, models of irregular shapes used in 3D computer animation, or the point clouds generated by self-driving cars to map their surroundings—this powerful machine learning architecture doesn’t work well. Around 2016, a new discipline called geometric deep learning emerged with the goal of lifting CNNs out of flatland.\\nNow, researchers have delivered, with a new theoretical framework for building neural networks that can learn patterns on any kind of geometric surface. These “gauge-equivariant convolutional neural networks,” or gauge CNNs, developed at the University of Amsterdam and Qualcomm AI Research by Taco Cohen, Maurice Weiler, Berkay Kicanaoglu and Max Welling, can detect patterns not only in 2D arrays of pixels, but also on spheres and asymmetrically curved objects. “This framework is a fairly definitive answer to this problem of deep learning on curved surfaces,” Welling said.\\nAlready, gauge CNNs have greatly outperformed their predecessors in learning patterns in simulated global climate data, which is naturally mapped onto a sphere. The algorithms may also prove useful for improving the vision of drones and autonomous vehicles that see objects in 3D, and for detecting patterns in data gathered from the irregularly curved surfaces of hearts, brains or other organs.\\nTaco Cohen, a machine learning researcher at Qualcomm and the University of Amsterdam, is one of the lead architects of gauge-equivariant convolutional neural networks.PHOTOGRAPH: ORK DE ROOIJ\\nThe researchers’ solution to getting deep learning to work beyond flatland also has deep connections to physics. Physical theories that describe the world, like Albert Einstein’s general theory of relativity and the Standard Model of particle physics, exhibit a property called “gauge equivariance.” This means that quantities in the world and their relationships don’t depend on arbitrary frames of reference (or “gauges”); they remain consistent whether an observer is moving or standing still, and no matter how far apart the numbers are on a ruler. Measurements made in those different gauges must be convertible into each other in a way that preserves the underlying relationships between things.\\nFor example, imagine measuring the length of a football field in yards, then measuring it again in meters. The numbers will change, but in a predictable way. Similarly, two photographers taking a picture of an object from two different vantage points will produce different images, but those images can be related to each other. Gauge equivariance ensures that physicists’ models of reality stay consistent, regardless of their perspective or units of measurement. And gauge CNNs make the same assumption about data.\\n“The same idea [from physics] that there’s no special orientation—they wanted to get that into neural networks,” said Kyle Cranmer, a physicist at New York University who applies machine learning to particle physics data. “And they figured out how to do it.”\\nEscaping Flatland\\nMichael Bronstein, a computer scientist at Imperial College London, coined the term “geometric deep learning” in 2015 to describe nascent efforts to get off flatland and design neural networks that could learn patterns in nonplanar data. The term—and the research effort—soon caught on.\\nBronstein and his collaborators knew that going beyond the Euclidean plane would require them to reimagine one of the basic computational procedures that made neural networks so effective at 2D image recognition in the first place. This procedure, called “convolution,” lets a layer of the neural network perform a mathematical operation on small patches of the input data and then pass the results to the next layer in the network.\\n“You can think of convolution, roughly speaking, as a sliding window,” Bronstein explained. A convolutional neural network slides many of these “windows” over the data like filters, with each one designed to detect a certain kind of pattern in the data. In the case of a cat photo, a trained CNN may use filters that detect low-level features in the raw input pixels, such as edges. These features are passed up to other layers in the network, which perform additional convolutions and extract higher-level features, like eyes, tails or triangular ears. A CNN trained to recognize cats will ultimately use the results of these layered convolutions to assign a label—say, “cat” or “not cat”—to the whole image.\\nADVERTISEMENT\\nILLUSTRATION: LUCY READING-IKKANDA/QUANTA MAGAZINE\\nBut that approach only works on a plane. “As the surface on which you want to do your analysis becomes curved, then you’re basically in trouble,” said Welling.\\nKeep Reading\\nThe latest on artificial intelligence, from machine learning to computer vision and more\\nPerforming a convolution on a curved surface — known in geometry as a manifold — is much like holding a small square of translucent graph paper over a globe and attempting to accurately trace the coastline of Greenland. You can’t press the square onto Greenland without crinkling the paper, which means your drawing will be distorted when you lay it flat again. But holding the square of paper tangent to the globe at one point and tracing Greenland’s edge while peering through the paper (a technique known as Mercator projection) will produce distortions too. Alternatively, you could just place your graph paper on a flat world map instead of a globe, but then you’d just be replicating those distortions—like the fact that the entire top edge of the map actually represents only a single point on the globe (the North Pole). And if the manifold isn’t a neat sphere like a globe, but something more complex or irregular like the 3D shape of a bottle, or a folded protein, doing convolution on it becomes even more difficult.\\nBronstein and his collaborators found one solution to the problem of convolution over non-Euclidean manifolds in 2015, by reimagining the sliding window as something shaped more like a circular spiderweb than a piece of graph paper, so that you could press it against the globe (or any curved surface) without crinkling, stretching or tearing it.\\nChanging the properties of the sliding filter in this way made the CNN much better at “understanding” certain geometric relationships. For example, the network could automatically recognize that a 3D shape bent into two different poses—like a human figure standing up and a human figure lifting one leg—were instances of the same object, rather than two completely different objects. The change also made the neural network dramatically more efficient at learning. Standard CNNs “used millions of examples of shapes [and needed] training for weeks,” Bronstein said. “We used something like 100 shapes in different poses and trained for maybe half an hour.”\\nAt the same time, Taco Cohen and his colleagues in Amsterdam were beginning to approach the same problem from the opposite direction. In 2015, Cohen, a graduate student at the time, wasn’t studying how to lift deep learning out of flatland. Rather, he was interested in what he thought was a practical engineering problem: data efficiency, or how to train neural networks with fewer examples than the thousands or millions that they often required. “Deep learning methods are, let’s say, very slow learners,” Cohen said. This poses few problems if you’re training a CNN to recognize, say, cats (given the bottomless supply of cat images on the internet). But if you want the network to detect something more important, like cancerous nodules in images of lung tissue, then finding sufficient training data — which needs to be medically accurate, appropriately labeled, and free of privacy issues — isn’t so easy. The fewer examples needed to train the network, the better.\\nADVERTISEMENT\\nCohen knew that one way to increase the data efficiency of a neural network would be to equip it with certain assumptions about the data in advance — like, for instance, that a lung tumor is still a lung tumor, even if it’s rotated or reflected within an image. Usually, a convolutional network has to learn this information from scratch by training on many examples of the same pattern in different orientations. In 2016, Cohen and Welling co-authored a paper defining how to encode some of these assumptions into a neural network as geometric symmetries. This approach worked so well that by 2018, Cohen and co-author Marysia Winkels had generalized it even further, demonstrating promising results on recognizing lung cancer in CT scans: Their neural network could identify visual evidence of the disease using just one-tenth of the data used to train other networks.\\nThe Amsterdam researchers kept on generalizing. That’s how they found their way to gauge equivariance.\\nExtending Equivariance\\nPhysics and machine learning have a basic similarity. As Cohen put it, “Both fields are concerned with making observations and then building models to predict future observations.” Crucially, he noted, both fields seek models not of individual things — it’s no good having one description of hydrogen atoms and another of upside-down hydrogen atoms — but of general categories of things. “Physics, of course, has been quite successful at that.”\\nEquivariance (or “covariance,” the term that physicists prefer) is an assumption that physicists since Einstein have relied on to generalize their models. “It just means that if you’re describing some physics right, then it should be independent of what kind of ‘rulers’ you use, or more generally what kind of observers you are,” explained Miranda Cheng, a theoretical physicist at the University of Amsterdam who wrote a paper with Cohen and others exploring the connections between physics and gauge CNNs. Or as Einstein himself put it in 1916: “The general laws of nature are to be expressed by equations which hold good for all systems of coordinates.”\\nMiranda Cheng, a physicist at the University of Amsterdam.PHOTOGRAPHER: ILVY NJIOKIKTJIEN/QUANTA MAGAZINE\\nConvolutional networks became one of the most successful methods in deep learning by exploiting a simple example of this principle called “translation equivariance.” A window filter that detects a certain feature in an image — say, vertical edges — will slide (or “translate”) over the plane of pixels and encode the locations of all such vertical edges; it then creates a “feature map” marking these locations and passes it up to the next layer in the network. Creating feature maps is possible because of translation equivariance: The neural network “assumes” that the same feature can appear anywhere in the 2D plane and is able to recognize a vertical edge as a vertical edge whether it’s in the upper right corner or the lower left.\\n“The point about equivariant neural networks is [to] take these obvious symmetries and put them into the network architecture so that it’s kind of free lunch,” Weiler said.\\nBy 2018, Weiler, Cohen and their doctoral supervisor Max Welling had extended this “free lunch” to include other kinds of equivariance. Their “group-equivariant” CNNs could detect rotated or reflected features in flat images without having to train on specific examples of the features in those orientations; spherical CNNs could create feature maps from data on the surface of a sphere without distorting them as flat projections.\\nThese approaches still weren’t general enough to handle data on manifolds with a bumpy, irregular structure — which describes the geometry of almost everything, from potatoes to proteins, to human bodies, to the curvature of space-time. These kinds of manifolds have no “global” symmetry for a neural network to make equivariant assumptions about: Every location on them is different.\\nADVERTISEMENT\\nILLUSTRATION: LUCY READING-IKKANDA/QUANTA MAGAZINE\\nThe challenge is that sliding a flat filter over the surface can change the orientation of the filter, depending on the particular path it takes. Imagine a filter designed to detect a simple pattern: a dark blob on the left and a light blob on the right. Slide it up, down, left or right on a flat grid, and it will always stay right-side up. But even on the surface of a sphere, this changes. If you move the filter 180 degrees around the sphere’s equator, the filter’s orientation stays the same: dark blob on the left, light blob on the right. However, if you slide it to the same spot by moving over the sphere’s north pole, the filter is now upside down — dark blob on the right, light blob on the left. The filter won’t detect the same pattern in the data or encode the same feature map. Move the filter around a more complicated manifold, and it could end up pointing in any number of inconsistent directions.\\nLuckily, physicists since Einstein have dealt with the same problem and found a solution: gauge equivariance.\\nThe key, explained Welling, is to forget about keeping track of how the filter’s orientation changes as it moves along different paths. Instead, you can choose just one filter orientation (or gauge), and then define a consistent way of converting every other orientation into it.\\nThe catch is that while any arbitrary gauge can be used in an initial orientation, the conversion of other gauges into that frame of reference must preserve the underlying pattern — just as converting the speed of light from meters per second into miles per hour must preserve the underlying physical quantity. With this gauge-equivariant approach, said Welling, “the actual numbers change, but they change in a completely predictable way.”\\nCohen, Weiler and Welling encoded gauge equivariance — the ultimate “free lunch” — into their convolutional neural network in 2019. They did this by placing mathematical constraints on what the neural network could “see” in the data via its convolutions; only gauge-equivariant patterns were passed up through the network’s layers. “Basically you can give it any surface” — from Euclidean planes to arbitrarily curved objects, including exotic manifolds like Klein bottles or four-dimensional space-time — “and it’s good for doing deep learning on that surface,” said Welling.\\nA Working Theory\\nThe theory of gauge-equivariant CNNs is so generalized that it automatically incorporates the built-in assumptions of previous geometric deep learning approaches — like rotational equivariance and shifting filters on spheres. Even Michael Bronstein’s earlier method, which let neural networks recognize a single 3D shape bent into different poses, fits within it. “Gauge equivariance is a very broad framework. It contains what we did in 2015 as particular settings,” Bronstein said.\\nADVERTISEMENT\\nA gauge CNN would theoretically work on any curved surface of any dimensionality, but Cohen and his co-authors have tested it on global climate data, which necessarily has an underlying 3D spherical structure. They used their gauge-equivariant framework to construct a CNN trained to detect extreme weather patterns, such as tropical cyclones, from climate simulation data. In 2017, government and academic researchers used a standard convolutional network to detect cyclones in the data with 74% accuracy; last year, the gauge CNN detected the cyclones with 97.9% accuracy. (It also outperformed a less general geometric deep learning approach designed in 2018 specifically for spheres — that system was 94% accurate.)\\nLEARN MORE\\nThe WIRED Guide to Artificial Intelligence\\nMayur Mudigonda, a climate scientist at Lawrence Berkeley National Laboratory who uses deep learning, said he’ll continue to pay attention to gauge CNNs. “That aspect of human visual intelligence” — spotting patterns accurately regardless of their orientation — “is what we’d like to translate into the climate community,” he said. Qualcomm, a chip manufacturer which recently hired Cohen and Welling and acquired a startup they built incorporating their early work in equivariant neural networks, is now planning to apply the theory of gauge CNNs to develop improved computer vision applications, like a drone that can “see” in 360 degrees at once. (This fish-eye view of the world can be naturally mapped onto a spherical surface, just like global climate data.)\\nMeanwhile, gauge CNNs are gaining traction among physicists like Cranmer, who plans to put them to work on data from simulations of subatomic particle interactions. “We’re analyzing data related to the strong [nuclear] force, trying to understand what’s going on inside of a proton,” Cranmer said. The data is four-dimensional, he said, “so we have a perfect use case for neural networks that have this gauge equivariance.”\\nRisi Kondor, a former physicist who now studies equivariant neural networks, said the potential scientific applications of gauge CNNs may be more important than their uses in AI.\\n“If you are in the business of recognizing cats on YouTube and you discover that you’re not quite as good at recognizing upside-down cats, that’s not great, but maybe you can live with it,” he said. But for physicists, it’s crucial to ensure that a neural network won’t misidentify a force field or particle trajectory because of its particular orientation. “It’s not just a matter of convenience,” Kondor said—“it’s essential that the underlying symmetries be respected.”\\nBut while physicists’ math helped inspire gauge CNNs, and physicists may find ample use for them, Cohen noted that these neural networks won’t be discovering any new physics themselves. “We’re now able to design networks that can process very exotic kinds of data, but you have to know what the structure of that data is” in advance, he said. In other words, the reason physicists can use gauge CNNs is because Einstein already proved that space-time can be represented as a four-dimensional curved manifold. Cohen’s neural network wouldn’t be able to “see” that structure on its own. “Learning of symmetries is something we don’t do,” he said, though he hopes it will be possible in the future.\\nCohen can’t help but delight in the interdisciplinary connections that he once intuited and has now demonstrated with mathematical rigor. “I have always had this sense that machine learning and physics are doing very similar things,” he said. “This is one of the things that I find really marvelous: We just started with this engineering problem, and as we started improving our systems, we gradually unraveled more and more connections.”\\nOriginal story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.',\n",
              "  array([-2.887748  , -3.7584412 , -0.08231863], dtype=float32),\n",
              "  2),\n",
              " ('New archaeology findings are redefining human history faster than ever\\nby THE CONVERSATION — 8 days ago in SYNDICATION\\n5\\nSHARES\\nIn 1924, a 3-year-old child’s skull found in South Africa forever changed how people think about human origins.\\nThe Taung Child, our first encounter with an ancient group of proto-humans or hominins called australopithecines, was a turning point in the study of human evolution. This discovery shifted the focus of human origins research from Europe and Asia onto Africa, setting the stage for the last century of research on the continent and into its “Cradles of Humankind.”\\nVolume 0%\\n*chirp chirp*\\nWho’s there? It’s early bird tickets to TNW2020\\nCOME IN\\nFew people back then would’ve been able to predict what scientists know about evolution today, and now the pace of discovery is faster than ever. Even since the turn of the 21st century, human origins textbooks have been rewritten over and over again. Just 20 years ago, no one could have imagined what scientists know two decades later about humanity’s deep past, let alone how much knowledge could be extracted from a thimble of dirt, a scrape of dental plaque or satellites in space.\\nHuman fossils are outgrowing the family tree\\nIn Africa, there are now several fossil candidates for the earliest hominin dated to between 5 and 7 million years ago, when we know humans likely split off from other Great Apes based on differences in our DNA.\\nAlthough discovered in the 1990s, publication of the 4.4 million year old skeleton nicknamed “Ardi” in 2009 changed scientists’ views on how hominins began walking.\\nRounding out our new relatives are a few australopithecines, including Australopithecus deryiremeda and Australopithecus sediba, as well as a potentially late-surviving species of early Homo that reignited debate about when humans first began burying their dead.\\nFossils like that of Australopithecus sediba, discovered in South Africa by a 9-year-old boy, are reshaping the human family tree. Photo by Brett Eloff. Courtesy Prof Berger and Wits University, CC BY-SA\\nPerspectives on our own species have also changed. Archaeologists previously thought Homo sapiens evolved in Africa around 200,000 years ago, but the story has become more complicated. Fossils discovered in Morocco have pushed that date back to 300,000 years ago, consistent with ancient DNA evidence. This raises doubts that our species emerged in any single place.\\nThis century has also brought unexpected discoveries from Europe and Asia. From enigmatic “hobbits” on the Indonesian island of Flores to the Denisovans in Siberia, our ancestors may have encountered a variety of other hominins when they spread out of Africa. Just this year, researchers reported a new species from the Philippines.\\nAnthropologists are realizing that our Homo sapiens ancestors had much more contact with other human species than previously thought. Today, human evolution looks less like Darwin’s tree and more like a muddy, braided stream.\\nThe rise of biomolecular archaeology means new opportunities for interdisciplinary collaboration among field- and lab-based scientists. Christina Warinner, CC BY-ND\\nAncient DNA reveals old relationships\\nMany recent discoveries have been made possible by the new science of ancient DNA.\\nSince scientists fully sequenced the first ancient human genome in 2010, data from thousands of individuals have shed new insights on our species’ origins and early history.\\nOne shocking discovery is that although our lineages split up to 800,000 years ago, modern humans and Neanderthals mated a number of times during the last Ice Age. This is why many people today possess some Neanderthal DNA.\\nThe 2010 excavation in the East Gallery of Denisova Cave, where the ancient hominin species known as the Denisovans were discovered. Bence Viola. Dept. of Anthropology, University of Toronto, CC BY-ND\\nAncient DNA is how researchers first identified the mysterious Denisovans, who interbred with us and Neanderthals. And while most studies are still conducted on bones and teeth, it is now possible to extract ancient DNA from other sources like cave dirt and 6,000-year-old chewing gum.\\nGenetic methods are also reconstructing individual and family relationships, and connecting ancient individuals to living peoples to end decadeslong debates.\\nThe applications go far beyond humans. Paleogenomics is yielding surprising discoveries about plants and animals from ancient seeds and skeletons hidden in the backrooms of museums.\\nNatural history museums hold a wealth of information, some of which can only be tapped through new biomolecular methods. Scientists analyze modern and fossil animal skeletons to ask questions about the past using ancient proteins. Mary Prendergast at National Museums of Kenya, CC BY-ND\\nBiomolecules are making the invisible visible\\nDNA is not the only molecule revolutionizing studies of the past.\\nPaleoproteomics, the study of ancient proteins, can determine the species of a fossil and recently linked a 9-foot tall, 1,300-pound extinct ape that lived nearly 2 million years ago to today’s orangutans.\\nDental calculus – the hardened plaque that your dentist scrapes off your teeth – is particularly informative, revealing everything from who was drinking milk 6,000 years ago to the surprising diversity of plants, some likely medicinal, in Neanderthal diets. Calculus can help scientists understand ancient diseases and how the human gut microbiome has changed over time. Researchers even find cultural clues – bright blue lapis lazuli trapped in a medieval nun’s calculus led historians to reconsider who penned illuminated manuscripts.\\nScientists unexpectedly found lazurite pigment in calcified plaque clinging to a 11th- to 12th-century woman’s tooth, challenging the assumption that male monks were the primary makers of medieval manuscripts. Christina Warinner, CC BY-ND\\nLipid residues trapped in pottery have revealed the origins of milk consumption in the Sahara and showed that oddly shaped pots found throughout Bronze and Iron Age Europe were ancient baby bottles.\\nResearchers use collagen-based “barcodes” of different animal species to answer questions ranging from when Asian rats arrived as castaways on Africa-bound ships to what animals were used to produce medieval parchment or even to detect microbes left by a monk’s kiss on a page.\\nBig data is revealing big patterns\\nWhile biomolecules help researchers zoom into microscopic detail, other approaches let them zoom out. Archaeologists have used aerial photography since the 1930s, but widely available satellite imagery now enables researchers to discover new sites and monitor existing ones at risk. Drones flying over sites help investigate how and why they were made and combat looting.\\nArchaeologists increasingly use technology to understand how sites fit into their environment and to document sites at risk. Here, a drone captured a tell (a mound indicating build-up of ancient settlements) in the Kurdistan Region of Iraq. Jason Ur, CC BY-ND\\nOriginally developed for space applications, scientists now use LIDAR – a remote sensing technique that uses lasers to measure distance – to map 3D surfaces and visualize landscapes here on Earth. As a result, ancient cities are emerging from dense vegetation in places like Mexico, Cambodia and South Africa.\\nTechnologies that can peer underground from the surface, such as Ground Penetrating Radar, are also revolutionizing the field – for example, revealing previously unknown structures at Stonehenge. More and more, archaeologists are able to do their work without even digging a hole.\\nGeophysical survey methods enable archaeologists to detect buried features without digging large holes, maximizing knowledge while minimizing destruction. Mary Prendergast and Thomas Fitton, CC BY-ND\\nTeams of archaeologists are combining big datasets in new ways to understand large-scale processes. In 2019, over 250 archaeologists pooled their findings to show that humans have altered the planet for thousands of years, for example, with a 2,000-year-old irrigation system in China. This echoes other studies that challenge the idea that the Anthropocene, the current period defined by human influences on the planet, only began in the 20th century.\\nNew connections are raising new possibilities\\nThese advances bring researchers together in exciting new ways. Over 140 new Nazca Lines, ancient images carved into a Peruvian desert, were discovered using artificial intelligence to sift through drone and satellite imagery. With the wealth of high-resolution satellite imagery online, teams are also turning to crowdsourcing to find new archaeological sites.\\nAlthough new partnerships among archaeologists and scientific specialists are not always tension-free, there is growing consensus that studying the past means reaching across fields.\\nThe Open Science movement aims to makes this work accessible to all. Scientists including archaeologists are sharing data more freely within and beyond the academy. Public archaeology programs, community digs and digital museum collections are becoming common. You can even print your own copy of famous fossils from freely available 3D scans, or an archaeological coloring book in more than 30 languages.\\nArchaeologists are increasingly reaching out to communities to share their findings, for example at this school presentation in Tanzania. Agness Gidna, CC BY-ND\\nEfforts to make archaeology and museums more equitable and engage indigenous research partners are gaining momentum as archaeologists consider whose past is being revealed. Telling the human story requires a community of voices to do things right.\\nStudying the past to change our present\\nAs new methods enable profound insight into humanity’s shared history, a challenge is to ensure that these insights are relevant and beneficial in the present and future.\\nIn a year marked by youth-led climate strikes and heightened awareness of a planet in crisis, it may seem counterproductive to look back in time.\\nYet in so doing, archaeologists are providing empirical support for climate change and revealing how ancient peoples coped with challenging environments.\\nAs one example, studies show that while industrial meat production has serious environmental costs, transhumance – a traditional practice of seasonally moving livestock, now recognized by UNESCO as intangible cultural heritage – is not only light on the land today, but helped promote biodiversity and healthy landscapes in the past.\\nArchaeologists today are contributing their methods, data and perspectives toward a vision for a less damaged, more just planet. While it’s difficult to predict exactly what the next century holds in terms of archaeological discoveries, a new focus on “usable pasts” points in a positive direction.',\n",
              "  array([-3.5007927 , -3.4177353 , -0.06502941], dtype=float32),\n",
              "  2),\n",
              " ('SEOUL (Reuters) - In cram school-obsessed South Korea, students fork out for classes in everything from K-pop auditions to real estate deals. Now, top Korean firms are rolling out artificial intelligence in hiring - and jobseekers want to learn how to beat the bots.\\nFrom his basement office in downtown Gangnam, careers consultant Park Seong-jung is among those in a growing business of offering lessons in handling recruitment screening by computers, not people. Video interviews using facial recognition technology to analyze character are key, according to Park.\\n“Don’t force a smile with your lips,” he told students looking for work in a recent session, one of many he said he has conducted for hundreds of people. “Smile with your eyes.”\\nClasses in dealing with AI in hiring, now being used by major South Korean conglomerates like SK Innovation (096770.KS) and Hyundai Engineering & Construction (000720.KS), are still a tiny niche in the country’s multi-billion dollar cram school industry. But classes are growing fast, operators like Park’s People & People consultancy claim, offering a three-hour package for up to 100,000 won ($86.26).\\nThere’s good reason to see potential. As many as eight out of every 10 South Korean students are estimated to have used cram schools, and rampant youth unemployment in the country - nearly one in four young people are not in the workforce by certain measures, according to Statistics Korea - offers a motive not present in other countries where cram schools are popular, like Japan.\\n“The AI won’t be naturally asking personal questions,” said Yoo Wan-jae, a 26-year-old looking for work in the hospitality industry. “That will make it a bit uncomfortable ... I’ll need to sign up for cram schools for the AI interview,” said Yoo, speaking in Seoul’s Noryangjin district, known as ‘Exam Village’, packed with cram schools and study rooms.\\n‘FEAR, JOY AND GAMIFICATION’\\nBusinesses around the world are experimenting with increasingly advanced AI techniques for whittling down applicant lists.\\nKim Seok-wu, a university senior majoring in management, demonstrates an AI interview program in Sungnam, South Korea, November 20, 2019. Picture taken November 20, 2019. REUTERS/Kim Hong-Ji\\nBut Lee Soo-young, a director of Korea Advanced Institute of Science and Technology (KAIST) Institute for Artificial Intelligence, told Reuters by telephone the new technology is being more widely embraced in South Korea, where large employers wield much influence in a tightening job market.\\nAccording to Korea Economic Research Institute (KERI), nearly a quarter of the top 131 corporations in the country currently use or plan to use AI in hiring.\\nOne AI video system reviewed by Reuters asks candidates to introduce themselves, during which it spots and counts facial expressions including ‘fear’ and ‘joy’ and analyses word choices. It then asks questions that can be tough: “You are on a business trip with your boss and you spot him using the company (credit) card to buy himself a gift. What will you say?”\\nAI hiring also uses ‘gamification’ to gauge a candidate’s personality and adaptability by putting them through a sequence of tests.\\n“Through gamification, employers can check 37 different capabilities of an applicant and how well the person fits into a position,” said Chris Jung, a chief manager of software firm Midas IT in Pangyo, a tech hub dubbed South Korea’s Silicon Valley.\\nPreparing for such tests doesn’t necessarily involve simply memorizing answers. “Some games don’t even have a ‘right answer’, as they are aimed to spot the problem-solving attitude of the applicant,” Jung said.\\n‘HOPELESS’\\nAt People & People, consultant Park said he gave AI hiring talks to over 700 university students, graduates and lecturers in 2019.\\n“Students are struggling from the emergence of AI interviews. My goal is to help them be fully prepared for what they will be dealing with,” said Park.\\nIn an online chat room monitored by Park, with more than 600 participants, numerous messages indicate thanks for the classes and success in AI interview quests.\\nSlideshow (2 Images)\\nBut elsewhere, some who haven’t yet taken lessons have already given up.\\nKim Seok-wu, a 22-year-old senior at a top university, recently failed to get beyond an AI interview for a management position at a retail company, and decided to pursue graduate school instead of trying to find a job.\\n“I think I will feel hopeless if all companies go AI for hiring,” Kim said. “The AI interview is too new, so job applicants don’t know what to prepare for and any preparations seem meaningless since the AI will read our faces if we make something up.”',\n",
              "  array([-3.3226662 , -2.8260088 , -0.10015795], dtype=float32),\n",
              "  2),\n",
              " ('HBO’s Westworld will return for a puzzle-filled third season on March 15, the network announced tonight.\\nThe network announced the release date with a new teaser trailer that includes a series of ominous moments in the near future that spell apocalyptic disaster. It starts with the impeachment of the 45th president, a second civil war breaking out in Russia, ecological disasters around the world, and a few more events that eventually collapse into one mega issue. Show co-creator Jonathan Nolan said at last year’s San Diego Comic-Con that the new season was partially inspired by Ridley Scott’s Blade Runner, and will focus on a dystopian future.\\n“Dystopia can look pretty beautiful in the world,” Nolan said, according to Deadline. “Just because the world is corrupt inside, doesn’t mean it can’t be smothered over and pretty. We wanted to find a version of dystopia that we hadn’t seen before.”\\nWestworld’s second season wrapped up with some answers to some pretty big questions — and opened few more doors to even bigger mysteries. HBO’s official recap of the second season finale does an excellent job of breaking down everything you need to remember heading into the third season.\\nThe show’s third season will see many of its stars return, including Evan Rachel Wood as Dolores, Thandie Newton as Maeve, Ed Harris as the Man in Black, Jeffrey Wright as Bernard, Tessa Thompson as Charlotte, and Luke Hemsworth as Stubbs, according to Deadline.\\nVox Media has affiliate partnerships. These do not influence editorial content, though Vox Media may earn commissions for products purchased via affiliate links. For more information, see our ethics policy.',\n",
              "  array([-3.3123217 , -1.4784786 , -0.30709028], dtype=float32),\n",
              "  2),\n",
              " ('By now, you’ve probably seen a host of industry experts and talking heads decree that this will be the year of artificial intelligence. But beyond the buzz-cycle of robotic dogs, ‘future of work’ coffee machines, and virtual assistants, where do the true advances really lie?\\n\\nToday, a lot of what we see labeled ‘AI’ is actually closer to automation. Automated responses can give the impression of intelligence, but in reality they are predetermined. \\n\\nThis use of automation, in part, is due to the data boom. Organizations have found currency in data and have fought to fuel products with this knowledge. But the problem with a vast data set is that obsolete knowledge can be muddled with the correct, useful, information. \\n\\nOne of the things that makes the human brain so incredible is how it is able to rapidly re-learn and replace knowledge. It helps people process new information and make decisions more effectively. \\n\\nWhile it may sound counter-intuitive, AI needs to learn to forget to become more intelligent. \\n\\nJust like us. \\n\\nAI needs to be more human — looking at neural networks \\xa0\\nWe have greater access to information than ever before, but much of it is siloed or irrelevant. The beauty of the human brain is that it can strategically filter through large data and make sense of the chaos. Outdated or unused data is eliminated, preventing generalizations and making space for new intel.\\n\\nBut the brain processes, retrieves, and stores knowledge very differently to a machine. \\n\\nFor example, a computer might process thousands of images of an animal to determine if it’s a cat or a dog. But a human can look at a cat — and understand the core principles of it — forever understanding what a cat is. True intelligence is not related to big data; it’s related to the small, but correct, data.\\n\\n\\nIs this a cat?\\nThe key for AI to function effectively is in creating a neural network that can process and learn to strategically forget data while keeping the concepts stored, just like a human, versus overwrite all the information it has acquired. \\n\\nArtificial intelligence that constantly reevaluates previously learned information in the same way the human brain does, will help AI intelligently identify the most relevant data and use that in its decision making process. This human-inspired AI can then apply insights to new situations in real time, with minimal computational power.\\n\\nIt works both ways — using AI can make people more intelligent \\nInnovation in AI has fueled concern that people’s jobs will become redundant or worse, pose an existential threat. \\n\\nMcKinsey estimates that AI will lift productivity and economic growth which will see people switching occupations or upgrading skills. \\n\\nYes, this means that the way people work will change but it doesn’t mean AI will replace people’s jobs. Just as AI needs human input, using AI can enhance people’s intelligence and help them gain new skills. \\n\\nAn easy way to demonstrate this is that a business may only have access to 20 percent of the knowledge available across an organization. The further 80 percent of knowledge could be undocumented — it’s recorded no',\n",
              "  array([-0.77172357, -3.255379  , -0.6947121 ], dtype=float32),\n",
              "  2),\n",
              " ('Technology takes over in Joanna Kavenna’s distressingly dystopian, relevant \\'Zed\\'\\nDon Oldenburg Special for USA TODAY\\nPublished 7:00 AM EST Jan 13, 2020\\nThe future is complicated. But that isn’t why Joanna Kavenna’s new dystopian novel “Zed” (Doubleday, 336 pp., ★★★½ out of four stars) triggers an unsettling buzz inside your brain that lasts long after the last page. What’s troubling is that this technology-gone-wrong nightmare feels like a plausible eventuality of today’s most intrusive technological issues.   \\nThe story begins a few years from now in London, where corporate exec George Mann murders his wife and two young sons before heading to a nearby pub. The city\\'s vast network of security cameras locates Mann, but the robotic law enforcement Anti-Terror Droid (ANT) sent to arrest him mistakenly kills an innocent bystander.\\nThis is not supposed to happen in the orderly technotopia run by the mega-corporation Beetle, whose ubiquitous Lifechain’s algorithms predict everyone’s next minute, hour, day, etc., calculating likely behaviors and events. Mann’s bloody rampage and the droid’s botched arrest blindside the system and set off a domino effect of unanticipated glitches that threaten the corporate goliath’s grip on life in the Western world.\\n\"Zed,\" by Joanna Kavenna.\\nDoubleday\\nBeetle’s genius owner Guy Matthias originally envisioned creating a benevolent society built upon technological conveniences and artificial intelligence innovations. But Matthias became a greedy, philandering egomaniac, and Beetle (think Amazon, Google, Apple and Facebook, plus the CIA) became intrusive and authoritarian – for everyone’s own good, of course.\\nMost everyone works for Beetle or its subsidiaries and must wear super smartwatches – Beetlebands – that monitor and report every thought, feeling and action. Beetlebands contain Very Intelligent Personal Assistants (Veeps), like Alexa on steroids, though personable. Beetle cryptocurrency is the only legal currency. Cars are driverless. AI is embedded in everything: Talking refrigerators recommend healthier foods; brushing your teeth sends physiological data to analyze your health. BeetleInspire nonstop suggests better alternatives. Beetle executives send lifelike avatars to virtual boardroom meetings.\\nLifechain, so accurate that predicted criminal behavior is a punishable offense, is crashing. More glitches, called “zed events,” are messing with the system’s predictions, causing Veeps to malfunction and befuddling Matthias’ language-censoring Bespoke program.\\nAuthor Joanna Kavenna.\\nAlexander Michaelis\\nThe author of four previous novels, including the award-winning debut “Inglorious” and “A Field Guide to Reality,” British novelist Joanna Kavenna cleverly combines dark humor and Pynchonesque storytelling in this insightful, unsettling look at how technology impacts our lives. With philosophical underpinnings (determinism versus free will), she crowds this tale with imaginative real and virtual characters, offering hope against techno-tyranny, for instance, through an idealistic newspaper editor who owns a musty paperback of  “All the President’s Men” and off-the-grid hackers who crack Beetle’s encryption system.\\nEarly chapters cause some technological claustrophobia as Kavenna draws readers into this bleak dystopian existence with the narrative’s deadpan Orwellian double speak, but she breaks through that setup to deliver a crazy, convoluted plot that’s riveting and relevant. So relevant that when an email asks you to “accept” a new privacy policy you’ll think “zed.” When an app asks you to share your location, you’ll pause and mumble “zed.” Kavenna has skillfully made our present feel like dystopian fiction.\\n\\'Consider This\\': \\'Fight Club\\' author Chuck Palahniuk takes readers to writing school\\nPublished 7:00 AM EST Jan 13, 2020',\n",
              "  array([-1.0159878, -1.517711 , -0.8704996], dtype=float32),\n",
              "  2),\n",
              " ('Bitcoin isn’t exactly en vogue in 2020, but its underlying technology (blockchain) is apparently still cool.\\n\\nAt least, that’s according to employment service LinkedIn, which listed “blockchain” as tech’s most sought after “hard skill” this year.\\n\\nA LinkedIn blog published last week notes that “blockchain” is the most in-demand skill in the United States, the United Kingdom, Australia, France, and Germany — more popular than cloud computing, artificial intelligence, and UX design.\\n\\n“Blockchain has emerged from the once shadowy world of cryptocurrency to become a business solution in search of problems,” said LinkedIn. “Which means that you don’t have to be in financial services to be seeking new hires who have background and expertise in putting blockchain to use.”\\n\\nThe Microsoft-owned company then urged recruiters to start familiarizing themselves with how “blockchain” works and what its perceived benefits are.\\n\\n“[…] Companies seem to be saying that the potential [of blockchain] is worth the gamble,” LinkedIn continued. “Blockchain has become a line of business for a who’s who of the corporate world — IBM, Oracle, JPMorgan Chase, Microsoft (LinkedIn’s parent company), Amazon, and American Express, to name just a few.”\\n\\nLinkedIn says devs should learn Solidity to work in blockchain\\nLinkedIn explains that there are two different kinds of skills: hard and soft.\\n\\nHard skills are those that pertain to an employee’s aptitude for executing specific tasks, mostly technical abilities and specialized knowledge. For software developers, proficiency in the C++ programming language would be a “hard” skill.\\n\\nSoft skills relate to the way in which those employees perform those tasks. How one behaves, how they think, and their cognitive skills are all examples of soft skills.\\n\\nWhile soft skills are difficult to measure, and perhaps harder to learn, LinkedIn did offer some advice to those looking to break into blockchain this year: learn the programming language used to write Ethereum smart contracts, Solidity.\\n\\nIn November last year, Hard Fork reported that job vacancy site Indeed had noted decreasing candidate interest in blockchain-related roles, but that employer demand in the industry had skyrocketed.\\n\\nSo, while it might be relatively difficult to make money from investing in or trading cryptocurrency this year, working with its underlying tech still seems mighty lucrative — for now.',\n",
              "  array([-4.4314766 , -4.1428423 , -0.02816757], dtype=float32),\n",
              "  2),\n",
              " ('Why machines should have rights, just like humans\\nby THE CONVERSATION — 7 days ago in SYNDICATION\\n4\\nSHARES\\nLike it or loathe it, the robot revolution is now well underway and the futures described by writers such as Isaac Asimov, Frederik Pohl, and Philip K. Dick are fast turning from science fiction into science fact. But should robots have rights? And will humanity ever reach a point where human and machine are treated the same?\\nAt the heart of the debate is the most fundamental question: what does it mean to be human? Intuitively, we all think we know what this means – it almost goes without saying. And yet, as a society, we regularly dehumanize others, and cast them as animal or less than human – what philosopher Giorgio Agamben describes as “bare life.”\\nEarly birds get all the good stuff\\nLike a great price on TNW2020 tickets\\nGIMME\\nTake the homeless for example. People who the authorities treat much like animalsor less than animals (like pests) who need to be guarded against with anti-homeless spikes and benches designed to prevent sleep. A similar process takes place within a military setting, where enemies are cast as less than human to make them easier to fight and easier to kill.\\nHumans also do this to other “outsiders” such as immigrants and refugees.\\nWhile many people may find this process disturbing, these artificial distinctions between insider and outsider reveal a key element in the operation of power. This is because our very identities are fundamentally built on assumptions about who we are and what it means to be included in the category of “human.” Without these wholly arbitrary distinctions, we risk exposing the fact that we’re all a lot more like animals than we like to admit.\\nBeing human\\nOf course, things get a whole lot more complicated when you add robots into the mix. Part of the problem is that we find it hard to decide what we mean by “thought” and “consciousness” and even what we mean by “life” itself. As it stands, the human race doesn’t have a strictly scientific definition of when life begins and ends.\\nSimilarly, we don’t have a clear definition of what we mean by intelligent thought and how and why people think and behave in different ways. If intelligent thought is such an important part of being human (as some would believe), then what about other intelligent creatures such as ravens and dolphins? What about biological humans with below-average intelligence?\\nThese questions cut to the heart of the rights debate and reveal just how precarious our understanding of the human really is. Up until now, these debates have solely been the preserve of science fiction, with the likes of Flowers for Algernon and Do Androids Dream of Electric Sheep? exposing just how easy it is to blur the line between the human and non-human other. But with the rise of robot intelligence, these questions become more pertinent than ever, as now we must also consider the thinking machine.\\nMachines and the rule of law\\nBut even assuming that robots were one day to be considered “alive” and sufficiently intelligent to be thought of in the same way as human beings, then the next question is how might we incorporate them into society and how might we hold them to account when things go wrong?\\nTraditionally, we tend to think about rights alongside responsibilities. This comes as part of something known as social contract theory, which is often associated with political philosopher Thomas Hobbes. In a modern context, rights and responsibilities go hand-in-hand with a system of justice that allows us to uphold these rights and enforce the rule of law. But these principles simply cannot be applied to a machine. This is because our human system of justice is based on a concept of what it means to be human and what it means to be alive.\\nSo, if you break the law, you potentially forfeit some part of your life through incarceration or (in some nations) even death. However, machines cannot know mortal existence in the same way humans do. They don’t even experience time in the same way as humans. As such, it doesn’t matter how long a prison sentence is, as a machine could simply switch itself off and remain essentially unchanged.\\nFor now, at least, there’s certainly no sign of robots gaining the same rights as human beings and we’re certainly a long way off from machines thinking in a way that might be described as “conscious thought.” Given that we still haven’t quite come to terms with the rights of intelligent creatures such as ravens, dolphins, and chimpanzees, the prospect of robot rights would seem a very long way off.\\nThe question then really, is not so much whether robots should have rights, but whether we should distinguish human rights from other forms of life such as animal and machines. It may be that we start to think about a cybernetic Bill of Rights that embraces all thinking beings and recognizes the blurred boundaries between human, animal, and machine.\\nWhatever the case, we certainly need to move away from the distinctly problematic notion that we humans are in some way superior to every other form of life on this planet. Such insular thinking has already contributed to the global climate crisis and continues to create tension between different social, religious, and ethnic groups. Until we come to terms with what it means to be human, and our place in this world, then the problems will persist. And all the while, the machines will continue to gain intelligence.',\n",
              "  array([-0.08076335, -3.198348  , -3.3033886 ], dtype=float32),\n",
              "  0),\n",
              " ('Salesforce announced some new developer tools today, designed to make it easier for programmers to build applications on top of Commerce Cloud in what is known in industry parlance as a “headless” system.\\nWhat that means is that developers can separate the content from the design and management of the site, allowing companies to change either component independently.\\nTo help with this goal, Salesforce announced some new and enhanced APIs that enable developers to take advantage of features built into the Commerce Cloud platform without having to build them from scratch. For instance, they could take advantage of Einstein, Salesforce’s artificial intelligence platform, to add elements like next-best actions to the site, the kind of intelligent functionality that would typically be out of reach of most developers.\\nDevelopers also often need to connect to other enterprise systems from their e-commerce site to share data with these tools. To fill that need, Salesforce is taking advantage of MuleSoft, the company it purchased almost two years ago for $6.5 billion. Using MuleSoft’s integration technology, Salesforce can help connect to other systems like ERP financial systems or product management tools and exchange information between the two systems.\\nBrent Leary, founder at CRM Essentials, whose experience with Salesforce goes back to its earliest days, says this about helping give developers the tools they need to create the same kind of integrated shopping experiences consumers have grown to expect from Amazon.\\n“These tools give developers real-time insights delivered at the ‘moment of truth’ to optimize conversion opportunities, and automate processes to improve ordering and fulfillment efficiencies. This should give developers in the Salesforce ecosystem what they need to deliver Amazon-like experiences while having to compete with them,” he said.\\nTo help get customers comfortable with these tools, the company also announced a new Commerce Cloud Development Center to access a community of developers who can discuss and share solutions with one another, an SDK with code samples and Trailhead education resources.\\nSalesforce made these announcement as part of the National Retail Foundation (NRF) Conference taking place in New York City this week.',\n",
              "  array([-4.929501  , -3.468681  , -0.03914445], dtype=float32),\n",
              "  2),\n",
              " ('Facebook says that it is banning “deepfakes,” those high-tech doctored videos and audios that are essentially indistinguishable from the real thing.\\n\\nThat’s excellent news — an important step in the right direction. 1 But the company didn’t go quite far enough, and important questions remain.\\n\\nQuicktake\\nHow Deepfakes Make Disinformation Real\\n\\nPolicing deepfakes isn’t simple. As Facebook pointed out in its announcement this week, media can be manipulated for benign reasons, for example to make video sharper and audio clearer. Some forms of manipulation are clearly meant as jokes, satires, parodies or political statements — as, for example, when a rock star or politician is depicted as a giant. That’s not Facebook’s concern.\\n\\nFacebook says that it will remove “misleading manipulative media” only if two conditions are met:\\n\\n“It has been edited or synthesized — beyond adjustments for clarity or quality — in ways that aren’t apparent to an average person and would likely mislead someone into thinking that a subject of the video said words that they did not actually say.”\\n“It is the product of artificial intelligence or machine learning that merges, replaces or superimposes content onto a video, making it appear to be authentic.”\\nThose conditions are meant to be precisely tailored to Facebook’s concern: Use of new or emerging technologies to mislead the average person into thinking that someone said something that they never said.\\n\\nFacebook’s announcement also makes it clear that even if a video is not removed under the new policy, other safeguards might be triggered. If, for example, a video contains graphic violence or nudity, it will be taken down. And if it is determined to be false by independent third-party fact-checkers, those who see it or share it will see a warning informing them that it is false. Its distribution will also be greatly reduced in Facebook’s News Feed.\\n\\nThe new approach is a major step in the right direction, but two problems remain.\\n\\nThe first is that even if a deepfake is involved, the policy does not apply if it depicts deeds rather than words. Suppose that artificial intelligence is used to show a political candidate working with terrorists, engaging in sexual harassment, beating up a small child or using heroin.\\n\\nNothing in the new policy would address those depictions. That’s a serious gap.\\n\\nThe second problem is that the prohibition is limited to products of artificial intelligence or machine learning. But why?\\n\\nSuppose that videos are altered in other ways — for example, by slowing down them down so as to make someone appear drunk or drugged, as in the case of an infamous doctored video of Nancy Pelosi.\\n\\nOr suppose that a series of videos, directed against a candidate for governor, are produced not with artificial intelligence or machine learning, but nonetheless in such a way as to run afoul of the first condition; that is, they have been edited or synthesized so as to make the average person think that the candidate said words that she did not actually say. What matters is not the particular technology used to deceive people, but whether unacceptable deception has occurred.\\n\\nFacebook must fear that a broader prohibition would create a tough line-drawing problem. In its public explanation, it also noted that if it “simply removed all manipulated videos flagged by fact-checkers as false,” the videos would remain available elsewhere online. By labeling them as false, the company said, “We’re providing people with important information and context.” Facebook seems to think that removal does less good, on balance, than a clear warning: “False.”\\n\\nMaybe so, but in the context of deepfakes, Facebook has now concluded that removal is better than a warning. In terms of human psychology, that’s almost certainly the right conclusion. If you actually see someone saying or doing something, some part of your brain will think that they said or did it, even if you’ve been explicitly told that they didn’t.',\n",
              "  array([-0.23494516, -3.046417  , -1.8210425 ], dtype=float32),\n",
              "  0),\n",
              " ('As Google Cloud looks to convince more enterprises to move to its platform, it needs to be able to give businesses an onramp for their existing legacy infrastructure and workloads that they can’t easily replace or move to the cloud. A lot of those workloads run on IBM Power Systems with their Power processors, and, until now, IBM was essentially the only vendor that offered cloud-based Power systems. Now, however, Google is also getting into this game by partnering with IBM to launch IBM Power Systems on Google Cloud.\\nUpdate: Seattle-based Skytap also offers support for IBM Power systems and makes them available in its own cloud, as well as Azure and IBM Cloud.\\n“Enterprises looking to the cloud to modernize their existing infrastructure and streamline their business processes have many options,” writes Kevin Ichhpurani, Google Cloud’s corporate VP for its global ecosystem, in today’s announcement. “At one end of the spectrum, some organizations are re-platforming entire legacy systems to adopt the cloud. Many others, however, want to continue leveraging their existing infrastructure while still benefiting from the cloud’s flexible consumption model, scalability, and new advancements in areas like artificial intelligence, machine learning, and analytics.”\\nPower Systems support obviously fits in well here, given that many companies use them for mission-critical workloads based on SAP and Oracle applications and databases. With this, they can take those workloads and slowly move them to the cloud, without having to re-engineer their applications and infrastructure. Power Systems on Google Cloud is obviously integrated with Google’s services and billing tools.\\nThis is very much an enterprise offering, without a published pricing sheet. Chances are, given the cost of a Power-based server, you’re not looking at a bargain, per-minute price here.\\nBecause IBM has its own cloud offering, it’s a bit odd to see it work with Google to bring its servers to a competing cloud — though it surely wants to sell more Power servers. The move makes perfect sense for Google Cloud, though, which is on a mission to bring more enterprise workloads to its platform. Any roadblock the company can remove works in its favor, and, as enterprises get comfortable with its platform, they’ll likely bring other workloads to it over time.',\n",
              "  array([-4.5835795 , -3.291761  , -0.04856712], dtype=float32),\n",
              "  2),\n",
              " (\"HONG KONG/BEIJING (Reuters) - China is working to finalize its first rules to cover online-only banking operations in a push to minimize risk in the financial sector and attract players including foreign lenders, three people with direct knowledge of the matter said.\\nA man stands on the Bund in front of Shanghai's financial district of Pudong in Shanghai, China February 26, 2018. REUTERS/Aly Song\\nThe guidelines could also bolster foreigners with existing China operations such as Citigroup Inc, HSBC Holdings PLC and Standard Chartered PLC by allowing them to set up separate digital banking platforms, two of them said.\\nThe framing of rules come as companies’ use of data as well as digital and artificial intelligence technologies have transformed China’s financial services landscape, from processing payments to selling investment products.\\nBut foreigners have struggled to make money in mainland retail banking, with many yet to break even despite years of onshore presence as they compete with the vast physical networks of domestic rivals.\\nAbout a dozen groups including foreigners are in talks with Chinese regulators over the new rules and have shown interest in launching digital banking operations, said one person who has been involved in such discussions with the banking watchdog.\\nThe rules would allow them to partner tech firms for independent digital banking platforms, the person said.\\nBanks are expected to be allowed to own majority stakes in online-only banking ventures, the people said, as the government pushes ahead with its strategy of easing access for foreigners to China’s vast financial markets.\\nThe framework, which will cover the existing online banking units of Alibaba Group Holding Ltd and Tencent Holdings Ltd among others, will form China’s first comprehensive move to standardize oversight of the fast-growing digital banking sector.\\nOther Asian economies including Hong Kong and Singapore are also ushering in digital-only banks.\\nChina has licensed four online-only banks since 2014 including Tencent-backed WeBank, Alibaba offshoot MYbank, and AiBank, backed by search engine operator Baidu Inc and China Citic Bank Corp Ltd.\\nThe People’s Bank of China and the China Banking and Insurance Regulatory Commission did not respond to Reuters’ faxed requests for comment. The people declined to be identified as the regulatory plans are confidential.\\nA spokesman for Citi said the bank would review the rules when they are published, and that its own China consumer banking business had already been digitized.\\nHSBC declined to comment on the rules but said it would welcome any move that continued to open the financial markets in mainland China, and would always look for new opportunities to expand there.\\nStanChart, AiBank, MYbank and WeBank declined to comment.\\nIn August, the central bank announced it was developing a three-year plan for the financial technology sector. It has yet to publish plans or give a timetable for doing so.\\nSo far the online sector has made little inroad into day-to-day banking, with assets of the four online banks standing at $56 billion at the end of 2018, latest official data showed, accounting for 0.15% of China’s total banking assets.\",\n",
              "  array([-3.8002436 , -1.7564396 , -0.21694264], dtype=float32),\n",
              "  2),\n",
              " ('Comcast NBCUniversal believes its can access startup innovation while supporting future Olympic gold-medalists.\\nThe American mass media company launched its new SportsTech accelerator today, based in part, on that impetus.\\nTechCrunch attended a briefing with Comcast execs at 30 Rock NYC to learn more about the initiative.\\nThe SportsTech accelerator is a partnership across Comcast NBCUniversal’s sports media brands: NBC Sports, Sky Sports and the Golf Channel.\\nThe program brings in industry partners NASCAR, U.S. Ski & Snowboard and USA Swimming — all of which are broadcast on Comcast NBC channels.\\nStarting today, pre-Series A sports technology startups can apply to become part of a 10-company cohort.\\nAccepted ventures will gain $50,000 in equity-based funding and enter SportsTech’s three-month accelerator boot camp — with sports industry support and mentorship — to kick off at Comcast’s Atlanta offices August 2020.\\nBoomtown Accelerators will join Comcast in managing the SportsTech program, with both sharing a minimum of 6% equity in selected startups.\\nIndustry partners, such as NASCAR and U.S. Ski & Snowboard, will play an advisory role in startup selection, but won’t add capital.\\nAn overarching objective for SportsTech emerged during conversations with execs and Jenna Kurath, Comcast’s VP for Startup Partner Development, who will run the new accelerator.\\nComcast and partners aim to access innovation that could advance the business and competitive aspects of each organization.\\nFrom McDonald’s McD Tech Labs to Mastercard’s Start Path, corporate incubators and accelerators have become common in large cap America, where companies look to tap startup ingenuity and deal-flow to and hedge disruption.\\nToward its own goals, SportsTech has designated several preferred startup categories. They include Business of Sports, Team and Coach Success and Athlete and Player Performance.\\nSportsTech partners, such as NASCAR, hope to access innovation to drive greater audience engagement. The motorsport series (and its advertising-base) has become more device-distributed, and NASCAR streams more race-day data live, from the pits to the driver’s seat.\\n“The focus has grown into what are we going to do to introduce more technology in the competition side of the sport…the fan experience side and how we operate as a business,” said NASCAR Chief Innovation Officer Craig Neeb.\\n“We’re confident we’re going to get access to some incredibly strong and innovative companies,” he said of NASCAR’s SportsTech participation.\\nU.S. Ski & Snowboard — the nonprofit that manages America’s snowsport competition teams  — has an eye on performance and medical tech for its athletes.\\n“Wearable technology [to measure performance]…is an area of interest…and things like computer vision and artificial intelligence for us to better understand technical elements, are quite interesting,” said Troy Taylor, U.S. Ski & Snowboard’s Director of High Performance.\\nCredit: U.S. Ski & Snowboard\\nSome of that technology could boost prospects of U.S. athletes, such as alpine skiers Tommy Ford and Mikaela Shiffrin, at the 2022 Beijing Winter Olympics.\\nIn a $7.75 billion deal inked in 2014, Comcast NBCUniversal purchased the U.S. broadcast rights for Olympic competition —  summer and winter —  through 2032.\\n“We asked ourselves, ‘could we do more?’ The notion of an innovation engine that runs before, during and after the Olympics. Could that give our Team USA a competitive edge in their pursuit for gold?,” said Jenna Kurath.\\nThe answer came up in the affirmative and led to the formation of Comcast’s SportsTech accelerator.\\nBeyond supporting Olympic achievement, there is a strategic business motivation for Comcast and its new organization.\\n“The early insights we gain from these companies could lead to other commercial relationships, whether that’s licensing or even acquisition,” Will McIntosh, EVP for NBC Sports Digital and Consumer Business, told TechCrunch.\\nSportsTech is Comcast’s third accelerator, and the organization has a VC fund, San Francisco-based Comcast Ventures — which has invested in the likes of Lyft, Vimeo and Slack and racked up 67 exits, per Crunchbase data.\\nAfter completing the SportsTech accelerator, cohort startups could receive series-level investment or purchase offers from Comcast, its venture arm or industry partners, such as NASCAR.\\n“Our natural discipline right now is…to have early deliverables. But overtime, with our existing partners, we’ll have conversations about who else could be a logical value-add to bring into this ecosystem,” said Bill Connors, Comcast Central Division President.',\n",
              "  array([-5.1974325 , -3.6981893 , -0.03076761], dtype=float32),\n",
              "  2),\n",
              " ('HANOI (Reuters) - Vietnam’s largest conglomerate Vingroup JSC said on Tuesday it has scrapped a plan to launch an airline and will instead focus on technology development and industrial production.\\nThe company last year applied for a license to launch an airline in 2020 but had yet to order any planes.\\nIt has now notified the Ministry of Transport that it will withdraw from the aviation transport services sector.\\n“We need to focus our resources on the development of technologies and industrial production, and therefore we have decided to withdraw,” Chief Executive Nguyen Viet Quang said in a statement.\\nQuang said Vietnam’s aviation market has been developing rapidly but there are already several airlines, and that the group’s move to launch another carrier would lead to oversupply.\\nADVERTISEMENT\\nVingroup, once a real estate and retail conglomerate, has grown to become Vietnam’s largest listed firm with a market capitalization of over $16 billion. It now also sells vehicles, television sets and smartphones and is looking to enter the artificial intelligence sector.\\nVietnam’s aviation market has been growing at double-digit pace, attracting new entrants.\\nProperty and leisure firm FLC Group JSC’s Bamboo Airways conducted its maiden commercial flight a year ago to become the country’s fifth airline.\\nOther companies, including Thien Minh Group and tour operator Vietravel, also aim to launch airlines this year.\\nVingroup said it will continue to train pilots at its VinAviation academy.',\n",
              "  array([-3.3366191, -1.5192385, -0.2936136], dtype=float32),\n",
              "  2),\n",
              " ('LONDON (Reuters) - As the 1st Brigade Combat Team of the U.S. 82nd Airborne Division departed for the Middle East amid rising tensions with Iran, their divisional commander gave a simple order. All personnel entering the region were told to leave smartphones and personal devices in the United States.\\nIt was a clear sign of growing official nervousness over the potential vulnerability of items that had become an unquestioned fact of life for soldiers and civilians alike, but which Washington fears potential foes could track, exploit and use for targeting. Such concerns are far from new, but were regarded less seriously when America’s primary enemies were seen as non-state groups such as Islamic State, the Taliban and al Qaeda. Now Washington is worried about other nations – not just Iran, but Russia and China – which are seen as a much more existential threat.\\nIt also points to a much greater trend. Across the board, the communications revolution – and the vast sea of data it produces – has made surveillance much easier, a trend likely to be magnified by the growth of artificial intelligence. It has also facilitated the mass leaking of phenomenal amounts of information, as demonstrated by NSA contractor Edward Snowden. And simultaneously, it has overturned decades of tradecraft in espionage and associated fields, where despite the rise in “fake news” and online trickery, spy agencies like the CIA now reportedly find it almost impossible to maintain the multiple false identities on which they once relied.\\n“The foundations of the business of espionage have been shattered,” former CIA official Duyane Norman said in a Yahoo news report, which outlined how foreign governments have become much better at tracking real and covert U.S. identities through phone and bank records, facial recognition and even the records of off-the-shelf DNA tests. “The debate [within the intelligence community] is like the one surrounding climate change. Anyone who says otherwise just isn’t looking at the facts.”\\nOPTIONS LIMITED\\nFor military commanders, the options are also becoming limited. In Russia’s war with Ukraine, Moscow’s forces have shown remarkable skill in targeting counterparts on the battlefield as soon as they use their phones or radios. According to the U.S.-based Military Times, the U.S. Marine Corps already bans troops from taking personal devices on Middle East combat deployments. The U.S. Navy says it is reconsidering its rules, while the Army says such decisions - as with the 82nd Airborne - are at the personal discretion of commanders.\\nDecisions are inevitably compromises. Taking away devices reduces the ability of personnel not just to talk to their families, but can complicate communications and organization. But concerns are growing fast. This month, the Pentagon also demanded personnel stop using the Chinese-owned TikTok application, with other similar platforms including WhatsApp also added to some blacklists.\\nReducing “careless talk” and unnecessary radio and other emissions is hardly new. As far back as World War One, British commanders discovered telephone systems in forward trenches had often been compromised by German signallers and did everything they could to ensure the most sensitive messages were instead carried by hand or word-of-mouth. Naval vessels, military aircraft and particularly submarines have long done everything possible to mask their signatures, particularly near enemy territory. Recent years, however, have seen growing lapses, including from those who might have been expected to know better.\\nFITNESS APP\\nIn early 2018, data released by fitness app Strava identified assorted U.S., Russian and even Iranian secret bases in Syria where military personnel and contractors appeared to have recorded their exercise runs without realizing they would be highlighted and widely shared. The U.S. military has now gone so far as to incorporate such mistakes into training exercises, killing off an entire unit in one drill after a soldier posted a selfie photo whose geo-tagging gave away their position.\\nAuthorities are also nervous about non-accidental release of information. This November, White House and military staff removed smartphones from reporters and presidential aides for the duration of President Donald Trump’s unannounced Thanksgiving trip to Afghanistan, which appeared as much about ensuring the news did not leak as worries the phones themselves might be tracked.\\nIn terms of the latter, the greatest threat will come when artificial intelligence and voice recognition software reach the point where phones can be used to monitor nearby conversations without use of a human analyst or translator. That may come sooner rather than later – one reason why some security experts are extremely nervous about Chinese firm Huawei being at the heart of 5G phone networks in several European countries. That may include Britain, due to make its own choice soon. This week, the head of Britain’s Security Service told the Financial Times he believed that risk can be managed without barring the Chinese firm altogether. U.S. counterparts, however, are much more cautious.\\nFor authoritarian states like China and Iran, both witnessing a major spike in often smartphone-coordinated protest and unrest, being able to access and track electronic devices – and the population at large – is seen as a priority. Most notably in Xinjiang province but also across the country, Beijing is turning China into the most sophisticated surveillance state in human history. Within its borders, China already has considerable, sometimes almost exhaustive, access to data and devices. Faster and more incisive artificial intelligence and machine learning will dramatically extend that reach.\\nThe question for Western states will be how effectively their potential foes can repurpose that technology to gather information outside their borders. The United States and its allies have become used to being able to use whatever devices and communications they wished since the Berlin wall fell. Those days are ending fast.\\n*** Peter Apps is a writer on international affairs, localization, conflict and other issues. He is the founder and executive director of the Project for Study of the 21st Century; PS21, a non-national, non-partisan, non-ideological think tank. Paralysed by a war-zone car crash in 2006, he also blogs about his disability and other topics. He was previously a reporter for Reuters and continues to be paid by Thomson Reuters. Since 2016, he has been a member of the British Army Reserve and the UK Labour Party, and is an active fundraiser for the party.\\n(The opinions expressed here are those of the author, a columnist for Reuters)\\nEditing by Giles Elgood\\nOur Standards:The Thomson Reuters Trust Principles',\n",
              "  array([-1.0932566, -0.4563886, -3.4638486], dtype=float32),\n",
              "  1),\n",
              " ('For the past few years, volatility in British politics has seemed the new normal.\\nBut barring unforeseen events, it looks likely Boris Johnson will be in residence in Downing Street until 2024, at least.\\nThe Conservatives\\' resounding election victory has given him real political authority, but the challenges facing him are also considerable.\\nWhat are the key decisions that could come to define the Johnson years?\\nHS2: Will it go ahead or be stopped it in its tracks?\\n\\nREUTERS\\nThe fate of the UK\\'s largest infrastructure project is hanging in the balance.\\nBoris Johnson must soon decide whether HS2 will go full steam ahead, be scaled back or cancelled entirely. His choice - as well as the government\\'s broader prescription for the UK\\'s faltering rail network - will say a lot about his vision for post-Brexit Britain.\\nThe idea of a high-speed rail link connecting London, the West Midlands and the North of England is a long-held dream for its supporters. For its detractors, the gargantuan project - currently estimated to cost between £81bn and £88bn - has become the stuff of nightmares.\\nWill HS2 ever get built?\\nThe PM has a political tightrope to walk, as well as a business decision to make.\\nCan he satisfy Tory activists in the eye of the route who are vociferously opposed, as well as his new cohort of MPs in the Midlands, Yorkshire and Greater Manchester who desperately want better local services and more investment?\\nThe PM has sent varying signals so far, insisting costs cannot continue to spiral while also hinting that, temperamentally, he is in favour of a scheme that could help rebalance the UK\\'s lopsided economy and address regional disparities.\\nThere are two reports sitting on his desk - a review of the project by Doug Oakervee, likely to recommend it goes ahead pretty much as planned, and a dissenting view from his deputy, Lord Berkeley, which warns of mounting costs and delays unless it is completely redesigned.\\nIf it does proceed, HS2 trains could potentially be running on the first section of the line by 2029. This, if he was still in power, would be the tenth anniversary of Mr Johnson becoming PM.\\nThe union: Will there be another Scottish independence referendum?\\n\\nGETTY IMAGES\\nBoris Johnson and Nicola Sturgeon are on a collision course.\\nThe SNP leader has her heart set on another Scottish independence referendum and the prime minister is determined to thwart her, or at least, postpone the day of reckoning for as long as possible.\\nThe prime minister has rejected a request from Scotland\\'s first minister for the necessary powers, known as a Section 30 order, to hold another vote on the union.\\nHe says all sides agreed before the 2014 referendum, which resulted in a clear No vote, that it was a \"once in a generation\" event. The decision taken by the Scottish people then must be upheld, he insists.\\nScottish independence: What might happen next?\\nEnd of story? Well, not quite. Independence is the SNP\\'s central driving force and Ms Sturgeon believes she has the wind in her sails after her party did better than expected in the general election.\\nShe has said she will consider her next steps but once the UK has left the EU on 31 January - a Brexit which Scotland did not vote for - she is likely to step up the pressure on Downing Street.\\nShe has, so far, ruled out Scotland following the example of Catalonia and holding a referendum without Westminster\\'s agreement. But other options, including some form of legal challenge or getting the Scottish Parliament to symbolically legislate itself for a referendum, remain open.\\nThe ultimate showdown could well be delayed until May 2021, when Ms Sturgeon will hope to win a direct electoral mandate for another referendum by getting a majority in Holyrood elections.\\nIf that transpired, could Mr Johnson continue to stand in her way? Or will he end up taking the same calculated gamble as David Cameron that a unionist victory might settle the issue for good?\\nSocial care: Will there be a long-term fix or more delay?\\n\\nSCIENCE PHOTO LIBRARY\\nCan Boris Johnson succeed where his two predecessors failed and find a lasting, affordable solution to funding care for the elderly?\\nSupporting the growing number of people living longer but with chronic conditions is a huge challenge, and one that, despite political good will and the work of endless experts, eluded both David Cameron and Theresa May.\\nThe PM has a head start with a large Commons majority and the public finances healthier than at any time in the past decade.\\nBut he knows that mere tinkering with the current means-tested system for helping people pay for basic tasks, such as getting washed and dressed, will not be acceptable.\\nWho gets social care and who pays for it?\\nDoes free personal care solve the crisis?\\nHe raised expectations when first becoming PM by suggesting he had a plan to fix the issue once and for all. He has offered to work with other parties on a way forward and much will depend on whether Labour\\'s new leader - whoever they are - is willing to invest political capital in the process.\\nWhile promising a blueprint in 2020, Mr Johnson has said it could take four years to implement. He has given little away about his own thinking, beyond stressing the importance of maintaining dignity in old age and stopping people having to sell their own homes to pay for care.\\nPressed during the election on what it would cost to meet such a guarantee, the PM - who has already committed to ploughing record sums into the NHS without raising personal taxes - struggled to come up with a figure.\\nThere is wide support for a cap on individuals\\' lifetime contributions combined with more state funding, while on the left and right there are proponents of a free universal system and a savings-based model for those who can afford it.\\nWhatever his government decides, achieving the kind of political consensus that underpins the NHS will be hard.\\nClimate change: Can there be a green revolution without big tax rises?\\n\\nGETTY IMAGES\\nClimate climate is one of the \"grand challenges\" of the next decade, according to the Conservatives, along with finding a cure for dementia and harnessing artificial intelligence.\\nDuring the election, the party resurrected the \"vote blue, go green\" slogan first used by David Cameron in 2010.\\nMr Cameron\\'s record in government did not always live up to the early \"hug a huskie\" hype and Boris Johnson - whose partner Carrie Symonds is a passionate conservationist - will be expected to be more ambitious.\\nClimate change: Where we are in seven charts\\nWill the government pay you to go green?\\nThe UK is committed to net zero greenhouses gas emissions by 2050. While there will inevitably be pressure to bring this date forward, the current focus is on how the target will be met and what the economic effects will be.\\nThere\\'s a commitment to create two million new jobs in the industries of the future while building on the UK\\'s natural strengths in areas such as offshore wind power. But will the £500m earmarked to help energy-intensive industries transition to a low-carbon future be enough?\\nThe PM will want the market to take the lead and for the state to be seen to facilitate, rather than dictate, consumer choices. Hence, the promise to consult on the \"earliest date\" for phasing out diesel and petrol cars rather than mandating a cut-off point.\\nMarch\\'s Budget is expected to have a green tinge, with tax breaks for de-carbonisation schemes and facilities making electric vehicle batteries. But will the PM be content for the tax system to encourage innovation or bow to those who want it to be an instrument to change behaviour?\\nThe government has promised \"strict\" new laws on air quality but can it deliver meaningful pollution targets without asking motorists - who have enjoyed a nine-year freeze in fuel duty - or air passengers to pay more?\\nAnd will the £4bn in extra cash promised to prevent flooding be remotely enough to shore up the UK\\'s defences?\\nThe Northern Powerhouse: Will the dream become a reality?\\n\\nTRANSPORT FOR GREATER MANCHESTER\\nFor the former Chancellor George Osborne, giving the North of England the tools and resources it needs to compete with the south was an economic imperative.\\nFor Boris Johnson, it has arguably become a political necessity.\\nOn 12 December, the Conservatives pulled off scores of victories in towns such as Burnley, Wakefield, Redcar, Sedgefield and Workington they could scarcely have dreamt of.\\nAfter annexing large sections of Labour\\'s \"red wall\", Mr Johnson will have to deliver on Brexit, his main campaign promise, if his party is to retain its allegiance next time around.\\nBut he also has the opportunity to consolidate his party\\'s surprise gains by delivering on his pledge to \"level up\" opportunities across the UK and ensure the North is no longer the poor relation when it comes to spending on transport, industrial and digital infrastructure.\\nCan the Treasury computer say \\'Yes\\' to the north?\\nWhat is the Northern Powerhouse?\\nExisting projects, such as the extension of the Manchester tram network, could be accelerated while others in development, such as the first phase of the Northern Powerhouse Rail line between Leeds and Manchester and the dualling of the A66, will be given greater impetus.\\nThere is talk of a Department for the North, while there are plans afoot to overhaul the way the Treasury calculates value for money in public spending, to address the historic bias towards faster-growing parts of the country.\\nMr Johnson, the former MP for leafy Henley who ran London for eight years, seems an unlikely standard bearer for the North. But with his chief adviser Dominic Cummings set on refashioning how Whitehall works, the rules of the game may be about to change.',\n",
              "  array([-1.1837076, -0.3955925, -3.883561 ], dtype=float32),\n",
              "  1),\n",
              " (\"If you\\xa0can't beat Big Tech, you might want to join it.\\xa0\\nTechnology is already pervasive in our lives – from smartphones to social media to buying goods and services online -- a reality that some embrace while others shun.\\nNow, technology jobs are increasingly being viewed as the best occupations in the country.\\nTech positions make up the top seven of Glassdoor’s 50 best jobs for 2020, an annual ranking based on job satisfaction, median base salary and number of openings. Last year, tech jobs comprised three of the top seven and four of the top 10. The job satisfaction ratings are based on Glassdoor’s popular employee reviews.\\nWhile tech giants like Apple, Amazon and Facebook may be the most prominent providers of technology products and jobs, virtually every sector – from retail to financial to health care – depends heavily on software and computers.\\n“Every company is a technology company,” says Glassdoor community expert Sarah Stoddard. “Everybody has a website. Everybody has an application these days.” Not to mention a social media presence and Big Data team that crunches reams of numbers to better target customers.\\nThis year, front-end engineer, essentially a web developer that creates the look and feel of websites, displaced data scientist as the top best job. Several non-technology roles in the top 10 of Glassdoor’s 50 best jobs ranking last year fell further down in the top 20, including nursing manager, occupational therapist, program manager (who oversees several projects at a company) and human resources manager.\\nWhile health care jobs may have slipped a bit, they remain in high demand as the population ages and a source of gratification for workers.\\n\\n4. Data Scientist &nbsp; &nbsp; &bull; National median base pay: $96,338 &nbsp; &nbsp; &bull; Highest median pay: $137,439 in San Francisco &nbsp; &nbsp; &bull; Lowest median pay: $98,485 in Atlanta &nbsp; &nbsp; &bull; 1-year pay growth: +1.8% &nbsp; &nbsp; ALSO READ: Most Common Last Names in the US\\nThinkstock\\n“Folks who work in health care feel they have the opportunity to help people and they’re drawn by that,” Stoddard says.\\nMarketing manager, ranked third last year, didn’t show up in the top 50 this year, but that’s because employees didn’t provide enough ratings to meet Glassdoor’s criteria, Stoddard says.\\nLast week, the\\xa0Labor Department reported a disappointing 145,000 job gains but unemployment remained at a 50-year low of 3.5%. That means workers generally continue to enjoy bargaining power over employers, a dynamic that's prompting many workers to switch jobs and even careers. And January is a prime month for switching, Stoddard says.\\nHere’s a look at the top 10 of Glassdoor's 50 best jobs list: (Ratings are out of 5. Job openings and median base salary are based on ads on Glassdoor’s site in 2019)\\n1. Front end engineer\\nJob satisfaction rating: 3.9\\nNumber of job openings: 13,122\\nMedian base salary: $105,240\\nThe popular position didn’t appear in last year’s rankings because there weren’t enough employee ratings.\\n2. Java developer\\nJob satisfaction rating: 3.9\\nNumber of job openings: 16,136\\nMedian base salary: $83,589\\nDevelopers who write code for Java are in growing demand because the software lets programs run on different types of computers. Moves up from No. 22 last year.\\n3. Data scientist\\nJob satisfaction rating: 4.0\\nNumber of job openings: 6,542\\nMedian base salary: $107,801\\nA data scientist, who analyzes vast amounts of data to spot trends, has been touted as the hottest job in the country in recent years. But the website towarddatascience.com says some professionals have become disillusioned and left the field, in part because some companies don’t have the technology in place to get the most out of artificial intelligence.\\n4. Product manager\\nJob satisfaction rating: 3.8\\nNumber of job openings: 12,173\\nMedian base salary: $117,713\\nWhile there are product managers in a wide variety of fields, Stoddard says those advertised on Glassdoor’s site generally oversee technology products. Moves up from No. 5.\\n5. Devops engineer\\nJob satisfaction rating: 3.9\\nNumber of job openings: 6,603\\nMedian base salary: $107,310\\nDevops engineers oversee the release of new computer code. Moves up from No. 6.\\n6. Data engineer\\nJob satisfaction rating: 3.9\\nNumber of job openings: 6,941\\nMedian base salary: $102,472\\nData engineers build\\xa0infrastructure that data scientists use for their analysis. Moves up from No. 8.\\n7. Software engineer\\nJob satisfaction rating: 3.6\\nNumber of job openings: 50,438 (most in the top 50)\\nMedian base salary: $105,563\\nSoftware engineers design the software to meet a company’s needs. Programmers write the code. Moves up from No. 10\\n8. Speech language pathologist\\nJob satisfaction rating: 3.8\\nNumber of job openings: 29,167\\nMedian base salary: $71,867\\nThe top-ranked healthcare job moves up from No. 19.\\n9. Strategy manager\\nJob satisfaction rating: 4.3\\nNumber of job openings: 3,515\\nMedian base salary: $133,067 (highest salary in the top 50)\\nA strategy manager assesses how best to achieve the company’s goals and works with different departments to do so. Moves up from No. 16.\\n10. Business development manager\\nJob satisfaction rating: 4.0\\nNumber of job openings: 6,560\\nMedian base salary: $78,480\\nA business development manager identifies new opportunities and attracts new customers.\",\n",
              "  array([-3.3068469 , -3.3633788 , -0.07391528], dtype=float32),\n",
              "  2),\n",
              " ('The venture-backed insurance world is more than the\\xa0Lemonades\\xa0and\\xa0MetroMiles\\xa0of the world. There’s more room in the industry for startups to shake things up. One such company, Cambridge-based\\xa0Insurify, is out today with a new venture round that greatly expands its capital base.\\nThe startup, which had accepted just $6.6 million over two rounds before its latest investment, has raised $23 million in a Series A led by\\xa0MTECH Capital\\xa0and\\xa0VIOLA FinTech. Prior investors\\xa0MassMutual Ventures\\xa0and\\xa0Nationwide\\xa0took part in the new investment.\\xa0(Update:\\xa0Hearst Ventures\\xa0also took part.)\\nTechCrunch hasn’t caught up with the company since our own\\xa0Sarah Buhr covered its first $2 million deal\\xa0back in early 2016. As you’d expect, a lot has changed in the last four years.\\nWhat’s Insurify?\\nTo get under the skin of the new round, TechCrunch caught up with\\xa0Insurify’s\\xa0\\xa0CEO and founder,\\xa0Snejina Zacharia.\\nZacharia, formerly of Gartner, came up with the idea for Insurify after she had an accident years ago. Following an unsatisfying experience working with the insurance industry after the fact, she discovered that consumers “have very, very little idea of how much coverage they need,” and that insurance providers (Insurify started out working with car insurance and is expanding to life and home insurance, as well) were “really struggling to [access] digital consumers because they have very poor UIs, [and] their APIs [were] not up to date.”\\nEnter Insurify, which bridges that gap. Working to build “automation behind insurance,” Insurify wants to help people find the coverage that they need, online, at a fair price; it’s a good business for the startup, which gets paid when consumers buy new insurance through its tooling.\\nInsurify, according to Zacharia, operates as a licensed agent for the various types of insurance it helps consumers find.\\nIt’s more than a middleperson, however. The startup wants to bring the buying of insurance more firmly into the digital world. Today, Insurify completes 65% of its new policies online, and provides pre-loaded information to carriers when it passes a consumer over to their side of things.\\nInsurify is also building out its own technology products that exist a little past insurance, including a “wallet” that lets users manage multiple policies in one place.\\nNew capital\\nTechCrunch asked Zacharia why she decided to raise capital now. According to the CEO, after doing “a lot with almost nothing,” her company is ready to accelerate its go-to-market motion.\\nIn practical terms, the new capital will help Insurify with “horizontal expansion,” like “launching new verticals” that will include home, rental and other types of insurance, she said. Even more interesting, the Series A will also be used to fuel the startup’s marketing arm, which Zacharia says is run like a “hedge fund.” Insurify’s marketing efforts are “automated through [an] artificial intelligence model,” she told TechCrunch, which estimates “the value of every click” through a set of algorithms that it tunes regularly.\\n(We’ll avoid\\xa0making a joke about hedge fund returns\\xa0at this juncture.)\\nThe CEO went on to say that “putting more money and more fuel behind [Insurify’s] marketing engine could really help us tremendously at this point,” helping to explain why Insurify decided to take on more capital when it did.\\nThe startup had options when it came to investor selection, with Zacharia telling TechCrunch that her firm “had multiple, different term sheets” from which to choose. Why MTECH and VIOLA as lead investors? Zacharia emphasized venture\\xa0partner\\xa0selection as key, also highlighting the experiences and expertise of each firm (insurance with MTECH, and fintech with VIOLA).\\nIt will be fascinating to see what happens at the meeting point of new capital, an operating marketing engine and an expanding set of products. Presumably Insurify can grow like heck from that confluence of factors. We’ll ask in a few months.',\n",
              "  array([-5.1083627 , -2.9159176 , -0.06208851], dtype=float32),\n",
              "  2),\n",
              " ('The recent commercial AI revolution has been largely driven by deep neural networks. \\xa0 First invented in the 1960s, deep NNs came into their own once fueled by the combination of internet-scale datasets and distributed GPU farms.\\xa0\\xa0\\xa0\\nBut the field of AI is much richer than just this one type of algorithm. Symbolic reasoning algorithms such as artificial logic systems, also pioneered in the ’60s, may be poised to emerge into the spotlight — to some extent perhaps on their own, but also hybridized with neural networks in the form of so-called “neural-symbolic” systems.\\nWeaknesses of deep neural networks\\nDeep neural nets have done amazing things for certain tasks, such as image recognition and machine translation. However, for many more complex applications, traditional deep learning approaches cannot match the ability of hybrid architecture systems that additionally leverage other AI techniques such as probabilistic reasoning, seed ontologies, and self-reprogramming ability.\\nDeep neural networks, by themselves, lack strong generalization, i.e. discovering new regularities and extrapolating beyond training sets. Deep neural networks interpolate and approximate on what is already known, which is why they cannot truly be creative in the sense that humans can, though they can produce creative-looking works that vary on the data they have ingested.\\nThis is why large training sets are required to teach deep neural networks and also why data augmentation is such an important technique for deep learning, which needs humans to specify known data transformations. Even interpolation cannot be done perfectly without learning underlying regularities, which is vividly demonstrated by well-known adversarial attacks on deep neural networks.\\nThe slavish adherence of deep neural nets to the particulars of their training data also makes them poorly interpretable. Humans cannot completely rely or interpret their results, especially in novel situations.\\xa0\\nCombining the strengths of neural and symbolic AI methods\\nWhat is interesting is that, for the most part, the disadvantages of deep neural nets are strengths of symbolic systems (and vice versa), which inherently possess compositionality, interpretability, and can exhibit true generalization. Prior knowledge can also be easily incorporated into symbolic systems in contrast to neural nets.\\nNeural net architectures are very powerful at certain types of learning, modeling, and action — but have limited capability for abstraction. That is why they are compared with the Ptolemaic epicycle model of our solar system — they can become more and more precise, but they need more and more parameters and data for this, and they, by themselves, cannot discover Kepler’s laws and incorporate them into the knowledge base, and further infer Newton’s laws from them.\\nSymbolic AI is powerful at manipulating and modeling abstractions, but deals poorly with massive empirical data streams.\\nThis is why we believe that deep integration of neural and symbolic AI systems is the most viable path to human-level AGI on modern computer hardware.\\nIt’s worth noting in this light that many recent “deep neural net” successes are actually hybrid architectures, e.g. the AlphaGo architecture from Google DeepMind integrates two neural nets with one game tree. Their recent MuZero architecture, which can master both board and Atari games, goes further along this path using deep neural nets together with planning with a learned model.\\nThe highly successful ERNIE architecture for Natural Language Processing question-answering from Tsinghua University integrates knowledge graphs into neural networks.\\xa0 The symbolic sides of these particular architectures are relatively simplistic, but they can be seen as pointing in the direction of more sophisticated neural-symbolic hybrid systems.\\nCisco’s successes with neural-symbolic street scene analysis\\nThe integration of neural and symbolic methods relies heavily on what has been the most profound revolution in AI in the last 20 years — the rise of probabilistic methods: e.g. neural generative models, Bayesian inference techniques, estimation of distribution algorithms, probabilistic programming.\\nAs an example of the emerging practical applications of probabilistic neural-symbolic methods, at the Artificial General Intelligence\\xa0(AGI) 2019 conference in Shenzhen\\xa0last August, Hugo Latapie from Cisco Systems described work his team has done in collaboration with our AI team at SingularityNET Foundation, using the\\xa0OpenCog AGI engine together with deep neural networks to analyze street scenes.\\nThe OpenCog framework provides a neural-symbolic framework that is especially rich on the symbolic side, and interoperates with popular deep neural net frameworks. It features a combination of probabilistic logic networks (PLNs), probabilistic evolutionary program learning (MOSES), and probabilistic generative neural networks.\\xa0\\xa0\\xa0\\xa0\\xa0\\nThe traffic analytics system demonstrated by Latapie deploys OpenCog-based symbolic reasoning on top of deep neural models for street scene cameras, enabling feats such as semantic anomaly detection (flagging collisions, jaywalking, and other deviations from expectation), unsupervised scene labeling for new cameras, and single-shot transfer learning (e.g. learning about new signals for bus stops with a single example).\\nThe difference between a pure deep neural net approach and a neural-symbolic approach in this case is stark. With deep neural nets deployed in a straightforward way, each neural network models what is seen by a single camera. Forming a holistic view of what’s happening at a given intersection, let alone across a whole city, is much more of a challenge.\\nIn the neural-symbolic architecture, the symbolic layer provides a shared ontology, so all cameras can be connected for to an integrated traffic management system. If an ambulance needs to be routed in a way that will neither encounter nor cause significant traffic, this sort of whole-scenario symbolic understanding is exactly what one needs.\\nThe same architecture can be applied to many other related use cases where one can use neural-symbolic AI to both enrich local intelligence and connect multiple sources/locations into a holistic view for reasoning and action.\\xa0\\nIt may not be impossible to crack this particular problem using a more complex deep neural net architecture, with multiple neural nets working together in subtle ways. However, this is an example of something that is easier and more straightforward to address using a neural-symbolic approach. And it is quite close to machine vision, one of deep neural nets’ great strengths.\\nIn other, more abstract application domains such as mathematical theorem-proving or biomedical discovery the critical value of the symbolic side of the neural-symbolic hybrid is even more dramatic.\\n2020: The year of neural-symbolic hybrid AI\\nDeep neural nets have done amazing things over the last few years, bringing applied AI to a whole new level. We’re betting that the next phase of incredible AI achievements are going to be delivered via hybrid AI architectures such as neural-symbolic systems. This trend has already started in 2019 in a relatively quiet way and in 2020 we expect it will pick up speed dramatically.',\n",
              "  array([-3.0888257 , -4.0753384 , -0.06458316], dtype=float32),\n",
              "  2),\n",
              " ('Global\\xa0risks\\xa0that were once on the horizon are now at the doorstep.\\nThe effects of climate change are more pronounced: the last five years have been the\\xa0hottest\\xa0on record and extreme weather events—such as super storms, droughts, and heatwaves—have occurred across every continent.\\nWhat were warning signs on the economy are now reality. The steady global recovery in the aftermath of the 2008 financial crisis has given way to what the International Monetary Fund (IMF) has\\xa0referred\\xa0to as a “synchronized slowdown,” driven in large measure by rising trade barriers and growing geopolitical tensions. Last year, the IMF warned of a 3% global growth rate for 2019, the lowest level in a decade. All this while citizens around the world protest systems they feel have failed to offer means for advancement.\\nOn technology, cyberattacks continue to rise as artificial intelligence poses questions to our leaders about surveillance, weapons systems, and video authenticity that are in need of immediate answers. According to one\\xa0study, the number of “deep fake” videos on the Internet rose by 84% last year, and there is no expectation that this rate will slow down.',\n",
              "  array([-0.6064437, -1.2822675, -1.7298794], dtype=float32),\n",
              "  0),\n",
              " ('Straight out of the 1990s and the plot of Jurassic Park, a team of scientists have successfully created an entirely new life-form using stem cells derived from frog embryos.\\nThe new species is called a xenobot and it’s unlike anything else in nature. The researchers call it a “living robot” because, though it’s made entirely of organic material, it’s not so much grown as “developed.”\\n\\nEarly birds get all the good stuff\\nLike a great price on TNW2020 tickets\\nGIMME\\nJoshua Bongard, a researcher from University of Vermont who co-led the research,\\xa0said:\\nThese are novel living machines. They’re neither a traditional robot nor a known species of animal. It’s a new class of artifact: a living, programmable organism.\\nXenobots are made using an artificial intelligence system that relies on, aptly enough, evolutionary algorithms. The scientists program the cells using a supercomputer at the University of Vermont and then researchers at Tufts assemble the creatures and… life finds a way.\\nPer\\xa0a press release\\xa0from University of Vermont:\\nAssembled into body forms never seen in nature, the cells began to work together. The skin cells formed a more passive architecture, while the once-random contractions of heart muscle cells were put to work creating ordered forward motion as guided by the computer’s design, and aided by spontaneous self-organizing patterns—allowing the robots to move on their own.\\nThe robots can be assembled into numerous configurations and, according to the team’s\\xa0research paper, they’re capable of self-healing, locomotion, and working together.\\n\\n\\nAn error occurred.\\nTry watching this video on www.youtube.com, or enable JavaScript if it is disabled in your browser.\\n\\nTheoretically, the xenobots could function as biodegradeable nanobots. This would make them useful for functions like delivering drugs inside the human body.\\nHere’s another theory: Jurassic Park. In the 1993 film (spoilers ahead, but it’s been almost 30 years) a team of scientists cobble together some dinosaur DNA with stem cells from modern creatures including, you guessed it: frogs.\\nAll hell broke loose in the movie once the frog cells started going haywire.\\nHowever, we’re quite sure this won’t be the case here. Here’s the\\xa0full study\\xa0just in case you want to check for yourself.',\n",
              "  array([-4.6904273 , -4.276389  , -0.02334595], dtype=float32),\n",
              "  2),\n",
              " ('Apple has\\xa0quietly acquired Xnor.ai, a Seattle-based artificial intelligence startup, for approximately $200 million, GeekWire reports. But that doesn\\'t mean a smarter Siri is on the roadmap—at least not yet.\\n\"Apple buys smaller technology companies from time to time and we generally do not discuss our purpose or plans,\" an Apple spokesperson told\\xa0Fortune\\xa0in a statement.\\nFounded in 2017, Xnor.ai was born inside the Allen Institute for Artificial Intelligence, a startup incubator founded by\\xa0Microsoft\\xa0co-founder Paul Allen. Using the startup\\'s technology, a device can call on a local chipset—or the cloud—to answer A.I. queries, as well as improve images by removing blemishes or blurriness, among other functions.\\n\\nCurrently, most smartphone virtual assistant functions rely on cloud-based servers that identify a need for artificial intelligence and send directions back to the requesting device. So, if Siri is asked to turn off the lights, for instance, that query is sent over the internet to Apple\\'s cloud server, which interprets the request and sends a command back to the software to activate a function that turns off the desired lights. While it may all take milliseconds (ideally), it\\'s still a delayed process that burns battery life.\\nKen Hyers, a director at Strategy Analytics in the research firm\\'s Emerging Device Technologies division, says Apple\\'s acquisition reflects where smartphone artificial intelligence is going.\\n\"It\\'s a real trend in the industry to move artificial intelligence to the device,\" Hyers says. \"Google is focused on this, Samsung is working on this as well.\\xa0Qualcomm\\xa0with their latest Snapdragon chipsets is also doing it.\"\\nBut that doesn\\'t mean Siri will be any smarter as a result of the Xnor.ai acquisition. While it\\'s possible that the startup\\'s technology could improve the latency on Apple\\'s virtual personal assistant, Xnor.ai won\\'t be able to improve Siri\\'s abilities or boost its accuracy, Hyers says.\\nInstead, Hyers believes Apple will use Xnor.ai\\'s artificial intelligence to analyze pictures users take and make on-the-fly enhancements to make them look better. Recently, Xnor.ai had driven the\\xa0people detection on low-cost Wyze cameras, a service it stopped providing in November, The Verge reports.\\nApple, however, could easily use Xnor.ai to make iPhones more attractive to shoppers. Strategy Analytics regularly polls consumers on their most-desired smartphone features, and Hyers says camera quality is always in the top three. \"Anything that improves picture-taking is going to enhance the saleability of the device,\" he says.\\nIt typically takes companies about 18 months to integrate a new chipset into a phone, and since Hyers believes Apple will unveil new iPhones in September, there won\\'t likely be enough time for this year\\'s Apple products to take advantage of Xnor\\'s technology. But Hyers expects to see it integrated into iPhones, iPads, and other devices next year.',\n",
              "  array([-4.5298724 , -3.9209332 , -0.03108268], dtype=float32),\n",
              "  2),\n",
              " ('TAIPEI (Reuters) - Taiwan Semiconductor Manufacturing Co Ltd (TSMC) forecast an up to 45% spike in January-March revenue and raised its capex plan for the year, betting robust demand for 5G smartphones would dial up profits at the world’s top contract chipmaker.\\nThe promising outlook from TSMC - a proxy for global tech demand given it has clients like Apple, Qualcomm and Huawei [HWT.UL] - comes as a strong uptake of 5G smartphones is fuelling an overall recovery in the global smartphone market that has shrunk for the past three years.\\nTSMC said it expects revenue over January-March to reach $10.2 billion-$10.3 billion, versus $7.1 billion a year ago.\\n“Moving into first quarter 2020, despite mobile product seasonality, we anticipate our business to be supported by the continued ramp up of 5G smartphones,” Chief Financial Officer Wendell Huang said at a briefing on Thursday, after TSMC earlier reported a better-than-expected profit for the fourth quarter.\\nHowever, the outlook assumes no disruption caused by the China-U.S. trade war. The world’s two largest economies signed an initial trade deal on Wednesday, but with numerous thorny issues still unresolved.\\nThe United States has in particular pressured Huawei, believing it a potential security threat, despite the Chinese company’s repeated denials.\\n“The company sees any disruption will be short-lived and for example commented that smaller telco infrastructure suppliers can quickly pick up the shortfall if Huawei can’t deploy 5G as planned,” analysts at Bernstein wrote in a note.\\n\\n“We find that too optimistic and believe the short-term impact will be notable.”\\nTSMC’s October-December net profit rose 16.1% year-on-year to T$116.035 billion ($3.88 billion), above an average forecast of T$111.41 billion drawn from 19 analysts by Refinitiv.\\nRevenue rose 10.6% to $10.39 billion, versus the company’s estimate of $10.2 billion to $10.3 billion and an average $10.55 billion estimate from 21 analysts.\\n“Our fourth-quarter business benefited from strong demand for high-end smartphones, initial 5G deployment and high performance computing related applications using TSMC’s industry-leading 7-nanometer technology,” Huang said.\\nThe 7-nanometer is one of the most advanced technology TSMC uses for its high-end semiconductors.\\nTSMC expects the global market for foundry chipmaking - contract chip manufacturing - to grow 17% this year, outstripping an 8% rise in the semiconductor market worldwide.\\nIt raised its estimate for capital expenditure to $15-16 billion for 2020, versus an earlier forecast of a level similar to its estimated capex for 2019, which was $14-15 billion. In the end it spent $14.9 billion in 2019\\n\\nRISING SMARTPHONE DEMAND\\nThe higher capex allocation comes amid forecasts for rising smartphone demand. Industry tracker IDC sees global smartphone shipments topping 1.4 billion units in 2020, up 1.5% on year, which should bode well for chipmaker TSMC.\\nReflecting growing optimism for the tech sector, TSMC shares hit a record high this month after gaining more than 50% in 2019.\\nInvestor sentiment should get a further boost from a Phase 1 trade deal agreed this week between the United States and China that is expected to defuse their 18-month trade war, which has weighed on the global economy and the tech industry.\\n“Looking into 2020-21, we believe TSMC’s position in advanced technology nodes remains solid given its greater capacity, yield rate control, execution and diversified customer base,” KGI analyst Laura Chen wrote in a January report prior to the earnings announcement.\\nJapan’s Nikkei reported this week that the United States had increased pressure on TSMC to make military-use chips there, free from any potential Chinese interference. China claims Taiwan as its own territory.\\nTSMC declined to comment on the report, reiterating that the company is always evaluating the idea of making chips everywhere.\\nCompany chairman Mark Liu said a decision will be made based on the “best interest” of their clients.\\n\\n\\n“We listen to our customers as our priority,” he said. “In the future, right now, it’s too early to say. Our customers prefer us to have the lowest (cost) production site in doing business with us.”\\nTo overcome geopolitical changes, TSMC will continue to be the industry’s leader so that “people will have to come to you”, he added. “That’s how we maintain being everyone’s foundry.”\\nShares in TSMC closed down 1.62%, versus a 0.21% fall in the wider market, valuing the company at almost $295 billion, bigger than U.S. rival Intel Corp’s $256 billion. It reported results after the market closed.',\n",
              "  array([-4.901425  , -3.1709116 , -0.05066324], dtype=float32),\n",
              "  2),\n",
              " ('WASHINGTON/BEIJING (Reuters) - From Huawei to the South China Sea, deep political rifts between Beijing and Washington are set to persist, despite a trade relations breakthrough, as the United States pushes back against an increasingly powerful and assertive China.\\n\\nRelations between the world’s two largest economies have deteriorated sharply since U.S. President Donald Trump imposed punitive tariffs in 2018, igniting a trade war.\\n“The broader, darkening picture is not going to be brightened much by this deal,” Bates Gill, an expert on Chinese security policy at Macquarie University in Sydney, said of the initial trade deal signed on Wednesday.\\n\\nThis backdrop spans China’s militarization of the South China Sea; rising tensions over Taiwan, which Beijing claims as its own; U.S. criticism over human rights in Hong Kong and Xinjiang, and a backlash against telecoms gear provider Huawei.\\nWhile the initial deal defuses an 18-month row that has hit global growth, experts say it is unlikely to provide much balm for broader frictions rooted in U.S. fears over an economically and technologically powerful China with a modernizing military.\\n“We can see Phase 1 as an emergency treatment to lower the temperature, but it has not addressed the fundamental problems,” said Wang Heng, a professor at the University of New South Wales in Sydney, who studies the China-U.S. economic relationship.\\n\\nHOSTILITY\\nWashington is increasingly alarmed about the security implications of Chinese technology, and has tightened its rules to keep better tabs on the acquisition of key technology by China, setting in motion changes to the global supply chain.\\n“The Chinese leadership are not naive about this,” said Gill. “They are already making moves to be more autonomous and thinking about a future... in an environment of hostility.”\\nThe Trump administration put Chinese telecoms equipment giant Huawei Technologies Co [HWT.UL] on a trade blacklist on national security concerns in May, banning it from buying supplies from American firms without U.S. government approval.\\n\\n\\nIt has also taken measures to crimp exports of artificial intelligence software.\\nNew U.S. rules tightening scrutiny of foreign investment go into effect on Feb. 13, extending the powers of the Committee on Foreign Investment in the United States (CFIUS), which has increasingly flexed its muscle against Chinese firms.\\nIntervention by CFIUS, which reviews mergers and stock purchases to ensure they do not harm national security, has dramatically slowed Chinese investment in the United States. Chinese foreign direct investment into U.S. fell 90% to $1.9 billion in 2019 from its peak in 2016, according to Refinitiv data.\\nThe two countries are also at odds over Taiwan, which counts the United States as its biggest weapons supplier but which China sees as one of its provinces.\\n\\n\\u2028\\nTaiwan’s President Tsai Ing-wen was re-elected on Saturday, vowing not to submit to Chinese pressure or control.\\nTsai’s campaign was helped by seven months of anti-government protests in Hong Kong, which Beijing accuses Washington of helping to foment, eroding China’s case for a “one country, two-systems model” similar to Hong Kong’s for Taiwan.\\nU.S. Army Secretary Ryan McCarthy said last week that China “will emerge as America’s strategic threat” and that the United States planned to deploy two task forces to the Pacific over the next two years capable of information, electronic, cyber and missile operations against Beijing.\\nTreasury Secretary Steven Mnuchin told CNBC on Wednesday that the United States was concerned about other issues involving China but these should be dealt with separately.\\n“You have to negotiate different pieces at different times,” he said.',\n",
              "  array([-1.0367459 , -0.71460986, -1.8578331 ], dtype=float32),\n",
              "  1),\n",
              " ('Foxconn announced it has invested in Israel-based startup\\xa0Nanox, to produce futurist and affordable X-ray machines. Apart from leading a round of $26 million investment, the Taiwanese company will also help with manufacturing these machines.\\nThe X-ray machine, called Nanox.arc, looks quite cool and only weighs around 70 kg. So, it’s very portable as compared to traditional X-ray machines that usually weigh a couple of hundred kilos\\nNanox said the idea is to make available where the traditional hospitals or clinics are not available. Apart from X-ray, the device will also support other scans\\xa0such as CT, mammography, fluoroscopy, and angiogram.\\n[Read:\\xa0Google’s new AI detects breast cancer just by scanning X-ray]\\xa0\\nThe machine is paired with the company’s proprietary software\\xa0Nanox.Cloud, that provides end-to-end medical imaging service offering image repository, radiologist matching, online and offline diagnostics review and annotation, connectivity to diagnostic assistive artificial intelligence systems, billing, and reporting.\\xa0\\n\\n\\nRan Poliakine, Founder & CEO of Nanox, talking to TNW on a call, said his aim is to make preventive scanning available to more people by making affordable and portable X-ray machines:\\xa0\\nNanox.arc is an affordable X-ray solution that will be priced around $10,000. That’ll help medical institutions invest into other equipments instead of spending thousands and millions of dollars on traditional scanning machines.\\nPoliakine said the system uses AI for better diagnosis: Nanox’s in-house AI works on making the best image available to radiologists using 3D reconstruction, and a third-party AI — such as Google’s\\xa0recent model\\xa0that catches breast cancer through X-rays —can assist experts making the right diagnosis.\\nPoliakine\\xa0added another advantage of this machine is that it can startup and shut down immediately unlike traditional machines.\\nThe company aims to start shipping the product by the first half of 2020. It aims to ship close to 15,000 machines in the next two years.\\xa0Poliakine said a network of over 15,000 machines will also boost medical research with over 300,000 images being collected daily. He said currently, research on medical AI suffers from the lack of data, that’s where Nanox can step in and provide ample data.',\n",
              "  array([-4.9136    , -3.2697887 , -0.04642141], dtype=float32),\n",
              "  2),\n",
              " ('TLDR:\\xa0Understand how machine learning works and what it can mean for your professional future with this four-course AI training bundle at just $39.99.\\nAnother day, another example of how artificial intelligence has infiltrated every aspect of human life…\\nBirds are cool\\nEarly birds are even cooler. Get cheap tickets to TNW2020 right now\\nYEAH\\nAlways looking for a competitive edge, film giant Warner Bros. has now enlisted an AI-powered system designed to\\xa0predict whether a particular mix of stars, budget and branding will lead to a box office success.\\xa0\\nAdvances in AI and machine learning have everyday implications from Hollywood to your hometown and beyond, which is why resources like\\xa0The Essential AI and Machine Learning Certification Training Bundle\\xa0($39.99, over 90 percent off\\xa0from TNW Deals) can be a huge lift for anyone’s professional skill set.\\nThe package includes four courses featuring 24 hours of content that introduces students to ways engineers are teaching machines to think for themselves as well as how those new abilities can impact your business or projects.\\nOnce you’ve gotten your feet wet with basic concepts in the\\xa0Artificial Intelligence and Machine Learning Foundation Course, you’ll not only understand the ideas behind convolutional and neural networks and other Deep Architectures, but you’ll start using that technology yourself through practice modules and real-time projects.\\nNext, the\\xa0Computer Vision Training Course\\xa0explains how machines can mine usable information from digital images and video; and the\\xa0Natural Language Processing Training Course\\xa0digs into ways artificial intelligence can interpret and understand human language in everything from emails to advertisements and websites.\\nThere’s also the\\xa0Data Visualization with Python and Matplotlib Training Course, which reveals methods for displaying data visually for a deeper understanding of what that data actually means.',\n",
              "  array([-4.458105  , -4.194002  , -0.02703217], dtype=float32),\n",
              "  2),\n",
              " ('The Allen Institute for AI (AI2)\\xa0started its incubator two years ago, helping launch companies like\\xa0Xnor.ai,\\xa0Blue Canoe\\xa0and\\xa0WellSaidLabs. Their success has attracted funding from not just local Seattle VC outfit Madrona, but Sequoia, Kleiner Perkins and Two Sigma Ventures as well, resulting in a new $10 million fund that should help keep the lights on.\\nThe AI2 Incubator, led by Jacob Colker since its inception in 2017, has focused on launching a handful of companies every year that in some way leverage a serious AI advantage. Blue Canoe, for instance, does natural language processing with a focus on accent modification; Xnor.ai is working on ultra-low-power implementations of machine learning algorithms, and was just\\xa0acquired yesterday by Apple for a reported $200 million.\\n\\n“We think the next generation of so-called AI-first companies are going to have to graduate into building long-term, successful businesses that start with an AI edge,” said the program’s new managing director, Bryan Hale. “And the people who can help do this are the ones who have helped build iconic companies.”\\nHence the involvement of household names (in the startup community, anyhow) Sequoia and Kleiner Perkins, and Two Sigma Ventures from New York. Seattle-based Madrona also recently invested in AI2 company\\xa0Lexion. It’s a pretty solid crowd to be running with, and as Colker pointed out, “they don’t often come together.”\\n“But also, they looked up into the northwest and said, what’s going on up there?” added Hale. Indeed, Seattle has over the last few years blossomed into a haven for AI research, with many major tech companies establishing or expanding satellite offices here at least partly concerned with the topic: Apple, Google, Nvidia and Facebook among others, and, of course, local standbys Amazon, Microsoft and Adobe.\\nPractically speaking, the new fund will let the incubator continue on its current path, but with a bit more runway and potentially bigger investments in the startups it works with.\\n\\n“We just have a lot more resources now to help our companies succeed,” said Colker. “Previously we were able to write up to about a $250,000 check, but now we can write up to maybe $800,000 per company. That means they have a lot more time to build out their team, aggregate training data, test their models, all these points that are important for a team to raise a bigger, better VC funding round.”\\nAI2 prides itself on its large staff of PhDs and open research strategy, publishing pretty much everything publicly in order to spur the field onwards. Access to these big brains, many of which have bred successful startups of their own, is no less a draw than the possibility of more general business mentorship and funding.\\nColker said the incubator will continue to produce three to five startups per year, each one taking “about 12-18 months, from whiteboard to venture funding.” AI, he pointed out, often needs more time than a consumer app or even enterprise play, since it’s as much research as it is development. But so far the model seems to work quite well.\\n“There are very few places in the world where an entrepreneur can come to take advantage of the brain power of a hundred PhDs and support staff. We’ve got a new research center with 70 desks, we’ve got plenty of space for those teams to grow,” he said. “We’re incredibly well-positioned to support the next wave of AI companies.”',\n",
              "  array([-4.9584603 , -3.3268018 , -0.04388029], dtype=float32),\n",
              "  2),\n",
              " ('BRUSSELS (Reuters) - The European Union is considering banning facial recognition technology in public areas for up to five years, to give it time to work out how to prevent abuses, according to proposals seen by Reuters.\\nThe plan by the EU’s executive - set out in an 18-page white paper - comes amid a global debate about the systems driven by artificial intelligence and widely used by law enforcement agencies.\\nThe EU Commission said new tough rules may have to be introduced to bolster existing regulations protecting Europeans’ privacy and data rights.\\n“Building on these existing provisions, the future regulatory framework could go further and include a time-limited ban on the use of facial recognition technology in public spaces,” the EU document said.\\n\\nDuring that ban, of between three to five years, “a sound methodology for assessing the impacts of this technology and possible risk management measures could be identified and developed.”\\nExceptions to the ban could be made for security projects as well as research and development, the paper said.\\nThe document also suggested imposing obligations on both developers and users of artificial intelligence and that EU countries should appoint authorities to monitor the new rules\\n\\nThe Commission will seek feedback on its white paper before making a final decision, officials said. EU digital and antitrust chief Margrethe Vestager is expected to present her proposals next month.\\nThe U.S. government earlier this month announced regulatory guidelines on artificial intelligence technology aimed at limiting authorities’ overreach and urged Europe to avoid aggressive approaches.',\n",
              "  array([-0.12911552, -2.3124886 , -3.8115878 ], dtype=float32),\n",
              "  0),\n",
              " ('(Reuters) - Experian (EXPN.L), the world’s largest credit data company, on Friday posted a 7% rise in third-quarter organic revenue, driven by increasing popularity of its analytics products for businesses.\\nThe company, which competes with U.S. peers TransUnion (TRU.N) and Equifax (EFX.N), said revenue from its North American market, rose 11% for the three months ended Dec. 31. The region accounts for more than half of Experian’s sales.\\nExperian has benefited from an increased demand for its Ascend product, which is a platform that integrates client data, industry-specific data feeds and analytics, machine learning and artificial intelligence.\\nThe blue-chip company, which runs credit score checks for individuals and companies who seek to take out loans, currently has 27 million customers on its free membership platform in the United States and 42 million in Brazil.\\nADVERTISEMENT\\n\\nExperian, which maintains credit information for 40 million active businesses in the U.S., saw a 11% rise in its B2B unit’s organic revenue in the region.\\n\\nThe company said it saw strength in credit data volumes and mortgage in North America.\\nThat helped offset falls of 13% and 3% in organic revenues from Experian’s EMEA/Asia Pacific and UK & Ireland businesses, respectively.\\n“Across EMEA/Asia Pacific, we’ve seen growing pipelines for affordability, decisioning software and marketplace offers and we expect a return to growth in Q4 FY20,” the company said.',\n",
              "  array([-4.7741632 , -3.8729498 , -0.02967821], dtype=float32),\n",
              "  2),\n",
              " ('JAKARTA (Reuters) - SoftBank has offered to invest up to $40 billion in the new capital city Indonesia plans to build on Borneo island, a minister said on Friday, though the Japanese tech conglomerate said no figure had been suggested yet.\\n\\n\\nIndonesian President Joko Widodo announced in August plans to move the administrative capital to East Kalimantan province, on Borneo, to relieve Jakarta from “a heavy burden” due to overcrowding and pollution.\\nIndonesia has previously put the cost of moving the capital at $33 billion, but Luhut Pandjaitan, Indonesia’s coordinating minister for maritime affairs and investment, said Softbank Group Corp had offered up to $40 billion.\\n“So we’ll negotiate over the structure (of the investment). It could amount to $30-40 billion,” he told reporters.\\nA spokeswoman for SoftBank said the group had not suggested specific numbers, echoing comments by CEO and founder Masayoshi Son last week.\\n\\n\\n“We’re not discussing the specific numbers yet, but a new smart city, newest technology, clean city, with a lot of AI (artificial intelligence), that’s what I’m interested in supporting,” Son said during a visit to Jakarta last week.\\nPandjaitan said he would discuss the potential investment further with Son further on Monday at the World Economic Forum gathering in the Swiss resort of Davos and that President Joko Widodo will likely decide on the agreement in February.\\nWidodo said on Thursday that a planned sovereign wealth fund to finance infrastructure projects in Southeast Asia’s biggest economy will raise at least $20 billion once established, though Pandjaitan said it was still under discussion whether investment would be channeled through the sovereign wealth fund.\\nWidodo discussed plans to establish the fund with Abu Dhabi’s Crown Prince Sheikh Mohammed Bin Zayed al-Nahyan last weekend during an official visit to the United Arab Emirates.\\nSovereign wealth funds are typically used by countries to direct funds from resource wealth such as oil, though Indonesia also plans to invite foreign investment into its fund.',\n",
              "  array([-3.8576412, -1.9862543, -0.1723627], dtype=float32),\n",
              "  2),\n",
              " ('The European Commission has revealed it is considering a ban on the use of facial recognition in public areas for up to five years.\\nRegulators want time to work out how to prevent the technology being abused.\\nThe technology allows faces captured on CCTV to be checked in real time against watch lists, often compiled by police.\\nExceptions to the ban could be made for security projects as well as research and development.\\nThe Commission set out its plans in an 18-page document, suggesting that new rules will be introduced to bolster existing regulation surrounding privacy and data rights.\\nIt proposed imposing obligations on both developers and users of artificial intelligence, and urged EU countries to create an authority to monitor the new rules.\\n\\tEmotion-detecting tech should be restricted by law - AI Now\\n\\tUS lawmakers concerned by accuracy of facial recognition\\nDuring the ban, which would last between three and five years, \"a sound methodology for assessing the impacts of this technology and possible risk management measures could be identified and developed\".\\nThe proposals come amid a calls from politicians and campaigners\\xa0in the UK to stop the police using live facial recognition for public surveillance.\\nMost recently the Kings Cross estate found itself at the centre of controversy,\\xa0when it was revealed its owners were using facial recognition technology without telling the public.\\nCampaigners claim the current technology is inaccurate, intrusive and infringes on an individual\\'s right to privacy.\\nA recent study\\xa0suggested facial recognition algorithms are far less accurate\\xa0at identifying black and Asian faces compared with white faces.\\n\\nMeanwhile, the Chinese government has begun rolling out facial recognition in pharmacies in Shanghai for people buying certain drugs.\\nIndividuals buying controlled medicines, such as those containing psychotropic substances, will be asked to verify their identity by scanning their face.\\nIt marks the latest in a series of moves from the Chinese state to prevent potential abusers from getting hold of certain medicines that can be used to produce illegal drugs.\\nThe country is a big supporter of facial recognition, and while the West remains cautious, China continues to embrace the technology.',\n",
              "  array([-0.12247922, -2.620095  , -3.1587296 ], dtype=float32),\n",
              "  0),\n",
              " ('Fyllo, a digital marketing company focused on the cannabis industry, has acquired\\xa0CannaRegs, a website offering subscription access to state and municipal cannabis regulations. Fyllo founder and CEO Chad Bronstein (pictured above) said his company paid $10 million in cash and stock.\\nBronstein previously served as chief revenue officer at digital marketing company Amobee, and he told me that the two companies are “very complementary,” particularly since regulations and compliance present “a unique technical challenge” when it comes to advertising cannabis products.\\nUltimately, his goal is for\\xa0Fyllo\\xa0\\xa0to offer “compliance as a service,” with artificial intelligence helping brands and publishers ensure that all their cannabis advertising follows local laws. At the same time, Bronstein said Fyllo will continue to support\\xa0CannaRegs’\\xa0\\xa0150-plus customers (mostly law firms, real estate professionals and cannabis operators) and work to bring more automation to the platform.\\nIn addition, CannaRegs founder and CEO Amanda Ostrowitz will become Fyllo’s chief strategy officer, with CannaRegs’ 30 employees continuing to work out of their Denver office. This brings Fyllo’s total headcount to around 70.\\n“In a short period of time, Fyllo has emerged as an essential platform for publishers and cannabis companies to build creative campaigns in a safe and compliant way,” Ostrowitz said in a statement. “By teaming up with Fyllo, we have the chance to build a truly remarkable brand that can disrupt the entire industry. We look forward to delivering our same quality of data to existing customers and incorporating that data into Fyllo’s platform to become a one-stop-shop for cannabis brands looking to grow their businesses.”\\nChicago-based\\xa0Fyllo raised $18 million in funding\\xa0last year.',\n",
              "  array([-4.6106734 , -3.606259  , -0.03780395], dtype=float32),\n",
              "  2),\n",
              " ('The\\xa0European Commission\\xa0\\xa0is considering a temporary ban on the use of facial recognition technology, according to a draft proposal for regulating artificial intelligence obtained by\\xa0Euroactiv.\\nCreating rules to ensure\\xa0AI\\xa0\\xa0is ‘trustworthy and human’ has been an\\xa0early flagship policy promise\\xa0of the new Commission, led by president Ursula von der Leyen.\\nBut the leaked proposal suggests the EU’s executive body is in fact leaning towards tweaks of existing rules and sector/app specific risk-assessments and requirements, rather than anything as firm as blanket sectoral requirements or bans.\\nThe leaked Commission\\xa0white paper\\xa0floats the idea of a three-to-five-year period in which the use of facial recognition technology could be prohibited in public places — to give EU lawmakers time to devise ways to assess and manage risks around the use of the technology, such as to people’s privacy rights or the risk of discriminatory impacts from biased algorithms.\\n“This would safeguard the rights of individuals, in particular against any possible abuse of the technology,” the Commission writes, adding that: “It would be necessary to foresee some exceptions, notably for activities in the context of research and development and for security purposes.”\\nHowever the text raises immediate concerns about imposing even a time-limited ban — which is described as “a far-reaching measure that might hamper the development and uptake of this technology” — and the Commission goes on to state that its preference “at this stage” is to rely on existing EU data protection rules, aka the General Data Protection Regulation (GDPR).\\nThe white paper contains a number of options the Commission is still considering for regulating the use of artificial intelligence more generally.\\nThese range from voluntary labelling; to imposing sectorial requirements for the public sector (including on the use of facial recognition tech); to mandatory risk-based requirements for “high-risk” applications (such as within risky sectors like healthcare, transport, policing and the judiciary, as well as for applications which can “produce legal effects for the individual or the legal entity or pose risk of injury, death or significant material damage”); to targeted amendments to existing EU product safety and liability legislation.\\nThe proposal also emphasizes the need for an oversight governance regime to ensure rules are followed — though the Commission suggests leaving it open to Member States to choose whether to rely on existing governance bodies for this task or create new ones dedicated to regulating AI.\\nPer the draft white paper, the Commission says its preference for regulating AI are options 3 combined with 4 & 5: Aka mandatory risk-based requirements on developers (of whatever sub-set of AI apps are deemed “high-risk”) that could result in some “mandatory criteria”, combined with relevant tweaks to existing product safety and liability legislation, and an overarching governance framework.\\nHence it appears to be leaning towards a relatively light-touch approach, focused on\\xa0“building on existing EU legislation” and creating app-specific rules for a sub-set of “high-risk” AI apps/uses — and which likely won’t stretch to even a temporary ban on facial recognition technology.\\nMuch of the white paper is also take up with discussion of strategies about “supporting the development and uptake of AI” and “facilitating access to data”.\\n“This risk-based approach would focus on areas where the public is at risk or an important legal interest is at stake,” the Commission writes. “This strictly targeted approach would not add any new additional administrative burden on applications that are deemed ‘low-risk’.”\\nEU commissioner Thierry Breton, who oversees the internal market portfolio, expressed resistance to creating rules for artificial intelligence last year — telling the EU parliament then that he “won’t be the voice of regulating AI“.\\nFor “low-risk” AI apps, the white paper notes that provisions in the GDPR which give individuals the right to receive information about automated processing and profiling, and set a requirement to carry out a data protection impact assessment, would apply.\\nAlbeit the\\xa0regulation\\xa0only defines limited rights and restrictions over automated processing — in instances where there’s a legal or similarly significant effect on the people involved. So it’s not clear how extensively it would in fact apply to “low-risk” apps.\\nIf it’s the Commission’s intention to also rely on GDPR to regulate higher risk stuff — such as, for example, police forces’ use of facial recognition tech — instead of creating a more explicit sectoral framework to restrict their use of a highly privacy-hostile AI technologies — it could exacerbate an already confusingly legislative picture where law enforcement is concerned, according to Dr Michael Veale, a lecturer in digital rights and regulation at UCL.\\n“The situation is extremely unclear in the area of law enforcement, and particularly the use of public private partnerships in law enforcement. I would argue the GDPR in practice forbids facial recognition by private companies in a surveillance context without member states actively legislating an exemption into the law using their powers to derogate. However, the merchants of doubt at facial recognition firms wish to sow heavy uncertainty into that area of law to legitimise their businesses,” he told TechCrunch.\\n“As a result, extra clarity would be extremely welcome,” Veale added. “The issue isn’t restricted to facial recognition however: Any type of biometric monitoring, such a voice or gait recognition, should be covered by any ban, because in practice they have the same effect on individuals.”\\nAn advisory body set up to advise the Commission on AI policy set out a number of\\xa0recommendations\\xa0in a report last year — including suggesting a ban on the use of AI for mass surveillance and social credit scoring systems of citizens.\\nBut its recommendations were criticized by privacy and rights experts for falling short by failing to grasp wider societal power imbalances and structural inequality issues which AI risks exacerbating — including by supercharging existing rights-eroding business models.\\nIn a\\xa0paper\\xa0last year Veale dubbed the advisory body’s work a “missed opportunity” — writing that the group “largely ignore infrastructure and power, which should be one of, if not the most, central concern around the regulation and governance of data, optimisation and ‘artificial intelligence’ in Europe going forwards”.',\n",
              "  array([-0.28692812, -1.5082448 , -3.5706983 ], dtype=float32),\n",
              "  0),\n",
              " ('The trade minister for India had harsh words for Amazon CEO Jeff Bezos this week when he came to visit: We don\\'t need your money.\\xa0\\nBezos offered to invest $1 billion in India, but at an industry conference, trade minister\\xa0Piyush Goyal was having none of it, saying that he hoped for an investigation into Amazon\\'s alleged “predatory pricing and unfair trade practices.”\\n“They may have put in a billion dollars,” Fortune reports Goyal saying at the Raisina Dialogue in New Delhi.\\xa0“But then if they make a loss of a billion dollars every year, then they jolly well have to finance that billion dollars. So it’s not as if they are doing a great favor to India when they invest a billion dollars.”\\n\\nAmazon Founder and CEO Jeff Bezos addresses the audience during a keynote session at the Amazon Re:MARS conference on robotics and artificial intelligence at the Aria Hotel in Las Vegas, Nevada on June 6, 2019. (Photo by Mark RALSTON / AFP) (Photo credit should read MARK RALSTON/AFP via Getty Images)\\n\\nMore:\\xa0Amazon lifts FedEx Ground restriction for third-party sellers\\nMore:\\xa0Attention Amazon shoppers:\\xa0E-retailer fires staffers for sharing customers\\' private data\\nAccording to Fortune, Bezos toured India this week, but he was not exactly welcome. \"Amazon Go Back,\" signs with Bezos\\' face were held by demonstrators outside an Amazon event, and local retailers organized sit-ins and public rallies in multiple cities to protest Amazon’s traditional cut-price approach and exclusive-selling practices.\\nIndia\\'s antitrust regulator has begun an investigation into Amazon\\'s business practices, said the report.\\nFortune reports Bezos visited a neighborhood store in\\xa0Mumbai and hung with Bollywood personalities. While in India, Amazon said it planned to create a million jobs in India within the next five years, and said it has\\xa0already\\xa0created 700,000 jobs locally.',\n",
              "  array([-1.2947798 , -0.57407415, -1.8151366 ], dtype=float32),\n",
              "  1),\n",
              " ('Millions of potential employees are subjected to artificial intelligence screenings during the hiring process every month. While some systems make it easier to weed out candidates who lack necessary educational or work qualifications, many AI hiring solutions are nothing more than snake oil.\\nThousands of companies around the world rely on outside businesses to provide so-called intelligent hiring solutions. These AI-powered packages are advertised as a way to narrow job applicants down to a ‘cream of the crop’ for humans to consider. On the surface, this seems like a good idea.\\nBirds are cool\\nEarly birds are even cooler. Get cheap tickets to TNW2020 right now\\nYEAH\\nAnyone who’s ever been responsible for the hiring at a decent-sized operation wishes they had a magic button that would save them from wasting their time interviewing the worst candidates.\\nRead next:\\xa0Tacoma convenience store’s facial recognition AI is a racist nightmare\\nUnfortunately, the companies creating the AI solutions are, often, offering something that’s simply too good to be true.\\nCNN’s Rachel Metz wrote the following in\\xa0a recent report\\xa0concerning AI-powered hiring solutions:\\nWith HireVue, businesses can pose pre-determined questions — often recorded by a hiring manager — that candidates answer on camera through a laptop or smartphone. Increasingly, those videos are then pored over by algorithms analyzing details such as words and grammar, facial expressions and the tonality of the job applicant’s voice, trying to determine what kinds of attributes a person may have. Based on this analysis, the algorithms will conclude whether the candidate is tenacious, resilient, or good at working on a team, for instance.\\nHere’s the problem: AI cannot determine whether a job candidate is tenacious, resilient, or good at working on a team. Humans can’t even do this. It’s impossible to qualify someone’s tenacity or resilience by monitoring the tone of their voice or their facial expressions over a few minutes of video or audio.\\nBut, for the sake of argument, lets concede we live in a parallel universe where humans magically have the ability to determine whether someone works well with others by observing their facial expressions while they answer questions about, presumably, whether they work well with others. An AI, even in this wacky universe where everyone was neurotypical and thus entirely predicable, still couldn’t make the same judgments because\\xa0AI is stupid.\\nAI doesn’t know what a smile means, or a frown, or\\xa0any human emotion. Developers train it to recognize a smile and then the developers determine what a smile means and assign that to the “smile output” paradigm. Maybe the company developing the AI has a psychiatrist or an MD standing around saying “in response to question 8, a smile indicates the candidate is sincere,” but that doesn’t make the statement true. Many experts consider this type of emotional simplification reductive and borderline\\xa0physiognomy.\\nThe bottom line is that the company using the software has no clue what the algorithms are doing, the PhDs or experts backing up the statements have no clue what kind of bias the algorithms are coded with – and all AI that judges human personality traits is\\xa0inherently biased.\\xa0 The developers coding the systems cannot protect the end users from inherent bias.\\nSimply put, there is no scientific basis by which an AI can determine human desirability traits by\\xa0applying computer vision/natural language-processing techniques to short video/audio clips. The analog version of this would be hiring based on what your gut tells you.\\nYou may as well decide that you’ll only hire people wearing charcoal suits or women with red lipstick for all the measurable good these systems do. After all, the most advanced facial recognition systems on the planet struggle to determine whether one black person is or isn’t\\xa0an entirely different black person.\\nAnyone who believes that an AI startup has built an algorithm that can tell whether a person of color, for example, is “tenacious” or “a good team worker”\\xa0based on a 3-5 minute video interview should email me right away. There’s a bridge in Brooklyn I’d like to sell them.',\n",
              "  array([-2.3992138 , -3.8629472 , -0.11855299], dtype=float32),\n",
              "  2),\n",
              " ('With each passing day, we’re using artificial intelligence for a variety of purposes and jobs. It has penetrated almost every industry and is helping them become innovative, develop authentic tools, and build strategies towards a sustainable future. Researchers are eagerly exploring new use cases of artificial intelligence that have the power to radically transform societies around us. But as we develop intelligence artificially, will be there a room for this AI to rewire us as humans?\\nArtificial intelligence has come a long way since its inception. We are at the brink of a massive change where world leaders are constantly discussing whether AI will take over the world and start controlling us. Even though it might not seem possible at the moment, there are plenty of theories that suggest the deadly impact of AI in the future. And who knows, the coming generation might have AI take the leadership role. Be it for the better or worse, artificial intelligence is all set to alter a human’s capacity for friendship, leadership, altruism, and ultimately love.\\n\\nA majority of us have grown up watching science fiction. There’s one thing we’ve learned from all of them, and that’s to fear AI. The staple of sci-fi movies for a long time has been how artificially intelligent robots will transform our lives. Go back to some of the classic AI depictions in movies. Be it Star Wars’ C3PO and R2D2 who work with the rebel alliance to thwart the empire or HAL 9000 and Ex-Machina who try to plot the murder of their master creators.\\nAll these imaginaries were focused on how this technology\\xa0can directly impact human beings from physical interaction. But, none of these narrates a story about the social effects or implications of AI. In other words, the way humans would interact with one other will be shaped by AI as we move into the future.\\nWhen the interaction between humans and artificial intelligence was still a distant prospect in the 1940s, Isaac Asimov came up with his three popular laws of robotics. These were to keep the emerging AI or robots from causing any harm to the human. Isaac’s very first law pointed out that robots would indeed affect humans via direct interaction, for good and evil.\\nHumans are directly interacting with AI\\nTake a look around today. We are surrounded by artificial intelligence with which we can interact directly. Be it the voice assistant on our phones, smart devices such as Google Home, Echo Dot, etc. in our homes or the plethora of applications on our smartphones. We live in the age of artificial intelligence. We have robots as home assistants, the ones who\\xa0perform surgeries,\\xa0and the ones who even look like us.\\nAs we move forward to beat our own technological advancement, we’re on the road to\\xa0developing more sophisticated AI. And as we do, machines will become the target of our affections. Remember the sci-fi drama ‘Her,’ where an introvert writer buys an artificial intelligence to help him write and ends up falling in love with it. He is captivated by the AI’s capability to learn and adapt. As humans, we are only as good as our imaginations. It might not seem like happening right away, but research suggests that humans could be falling in love and marrying artificial intelligence in the future.\\nFalling into the arms of AI\\nDr. Maciej Musial\\xa0from the University of Adam Mickiewicz in Poznan, Poland has pointed out that people will soon fall into the arms of humanoid robots and artificial intelligence apps on our smartphones. The evidence of this can be found in the fact that people are already seen growing attached to their gadgets such as smartphones. The research further suggested that a new phenomenon becoming frequent is the underlying formation of emotional relationships between humans and artificial intelligence under different disguises.\\nWhen noted carefully, the attitude of human beings towards robots is linked to the mechanism which refers to people’s attitude towards other objects along with emotional attachment and animation. In other words, giving robots the characteristics of living beings. Studies exist that show how people form rudimentary binds with household appliances like automatic vacuums and more. It was observed that the owner of these simpleton robots would leave the house dirty or do the work by themselves if the robots seemed in a bad mood.\\nEven though it might sound less serious, but the car and bike owners do give their vehicles affectionate names. If researchers manage to bridge the remaining gap between artificial intelligence and humans, the day won’t be far when humans will develop more serious emotional partnerships with robots. And who knows, one-day forming bonds with complex machines might become a societal norm, which will lead humans to get married with robots.\\nDavid Hanson, who created the famous lifelike Sophia Robot recently revealed that humans are only a few decades away from marrying droids. There is already the kind of robot in the world today that overcome the bridge of intimacy, which is required for a deep emotional partnership. The researcher suggests that humanoids will get the same rights as humans by the year 2045. This would include the right to own land, vote in general elections, and even marry.\\nHanson also suggests that by the year 2035, robots will be able to accomplish almost everything that humans do. They might even start their own ‘Global Robotic Civil Rights Moments’ by 2038 and compel leaders to provide them with equal status in the human world.\\nThe truth behind these predictions is only hidden in the future. But based on the evidence today, we do know that humans develop an emotional attachment to their devices that display human-like behaviors. The boundary between the real world and the virtual world is being narrowed down. In other words, the gaps between simulations and what is being simulated are blurred with advancing AI. In this way, the relationships between human and virtual realities tend to be more satisfying than traditional relationships.',\n",
              "  array([-0.8698769, -2.7470946, -0.6599386], dtype=float32),\n",
              "  2),\n",
              " ('Britain is more built-on than ever, with 44.8 million buildings in total at the end of the decade, up from 40.6 million in 2010.\\nBut this total accounts for just 1.4% of British land, compared with 40% covered by woodland, the natural environment, rivers and lakes, and a further 45% by agricultural land.\\nThe data comes from the largest land survey of its type by the Ordnance Survey (OS), which does not cover Northern Ireland.\\n\\tSee how much of your area is built on\\nWe\\'ve pulled together some before-and-after pictures showing some of the significant shifts over the last decade, as recognised by OS surveyors.\\n1. Renewable power\\nINTERACTIVE\\nRAF Lyneham solar farm\\nSeptember 2019\\nMarch 2014\\n\\n\\nIf you cannot see the slider, click here.\\nGrowth of the renewable energy industry has brought with it an increase in infrastructure.\\nWind power now accounts for one-fifth of total electricity use in the UK, compared with just 2% in 2009, according to the Department for Business, Energy and Industrial Strategy.\\nSolar energy accounted for 3.9% in 2018, up from 0.01% in 2010.\\nPart of that is thanks to the 160,000 solar panels placed in 2015 on the grounds of RAF Lyneham in Wiltshire.\\nIt is now the largest solar farm in the UK, with the capacity to power 10,000 local homes and an on-site military training college.\\n2. Global attractions\\nINTERACTIVE\\nOlympic Park, Stratford, London\\nJune 2019\\nSeptember 2009\\n\\n\\nINTERACTIVE\\nHow that looks on the Ordnance Survey map\\n2019\\n2010\\n\\n\\nAt the start of the decade, the Olympic Park was just beginning to take shape in Stratford, east London, ahead of the 2012 Games.\\nTen years on, what was the Olympic Stadium is now West Ham FC\\'s home ground, the Aquatics Centre is a public pool and the whole site is now the free-to-enter Queen Elizabeth Olympic Park.\\nAnother global attraction from the 2010s is the Harry Potter section of the Warner Bros Studio Tour in Watford. It first opened in 2012 but there has since been expansion.\\nINTERACTIVE\\nWarner Bros. Studio Tour in Watford expanded as its Harry Potter section grew\\nMay 2018\\nJune 2009\\n\\n\\nINTERACTIVE\\nHow that looks on the Ordnance Survey map\\n2019\\n2010\\n\\n\\nThe detail shown on the OS map is largely down to Tom Watts, a ground surveyor covering the area.\\nHe had to persuade Warner Bros security to let him in to measure each of the new buildings and was eventually rewarded with a tour.\\nEach time the OS ground and aerial surveying teams find a new physical object, spot one that has changed or is no longer there, they alter the map. There have been more than 360 million changes since 2010.\\nThis can be as subtle as a kerb being taken in or a front garden being paved over, which is important to record for things like flood planning.\\n\\tClimate change: Where we are in seven charts\\n\\tHouse price calculator: Where can I afford to rent or buy?\\n\\tClimate change: Are there too many people on Earth... and other questions\\nOS head of media Robert Andrews described ground surveyors like Mr Watts as \"looking at the world like Keanu Reeves in The Matrix - they see lines and curves where we see roads, buildings and landscapes\".\\nThis attention to detail is combined with local knowledge and contact with planning authorities, but the ground teams are also assisted - when the weather allows - by drones and two OS planes.\\nThey sweep back and forth like a lawnmower across target areas, taking hi-res photographs. These are then stitched together and analysed by artificial intelligence to try to spot what has changed since the previous survey.\\nThe technological advances are reflected also in what the OS data is used for. In 1791 they were set up with an initial task of mapping the south coast in case of French invasion.\\nMore than two centuries later, their data is used in almost every app on your smartphone and they are working to map the precise locations of lamp posts so they can host sensors for self-driving cars.\\n3. New roads\\nINTERACTIVE\\nQueensferry Crossing Bridge, near Edinburgh\\nJune 2018\\nSeptember 2010\\n\\n\\nINTERACTIVE\\nHow that looks on the OS map\\n2019\\n2010\\n\\n\\nAlthough not yet ready for self-driving cars, there are now 4,000 sq km of road in Britain, an increase of nearly 10% since 2010. More roads were built in every part of Great Britain.\\nMr Andrews said the mapping of new roads and buildings was \"like painting the Forth Bridge - as soon as you\\'ve finished surveying an area you need to start it again\".\\nNeatly illustrating his point is one of the most striking new roads in the past decade: the Queensferry Crossing erected in 2017, next to the famous Forth Bridges that link Edinburgh to north-east Scotland.\\n4. New towns\\nINTERACTIVE\\nNorthstowe, near Cambridge\\nOctober 2018\\nOctober 2008\\n\\n\\nHousing has been one of the most significant political talking points of the last 10 years, and Gordon Brown ended the last decade promising five new \"eco-towns\" to try to counter the shortfall in available homes.\\nWhile political events since 2010 didn\\'t quite work out exactly how Mr Brown might have planned, Northstowe, a sustainably built new town in Cambridgeshire, has made an impact noticeable from the sky.\\nIt\\'s still a work-in-progress but eventually 10,000 homes will replace what was farmland, a golf course and a former airfield.\\n5. Water features\\nINTERACTIVE\\nAbbotswood, Hampshire\\nSeptember 2019\\nSeptember 2008\\n\\n\\nINTERACTIVE\\nHow that looks on the OS map\\n2019\\n2010\\n\\n\\nAnother feature spotted by surveyors is the increase in ponds and waterways that have come up either naturally as new properties are built, or as part of manufactured efforts to avoid flooding in residential areas.\\nFor example, the housing development near Romsey in Hampshire, shown above, is accompanied by a couple of new ponds.\\nBut in other cases, like this development near the most well-known new town, Milton Keynes, existing ponds have been diverted to make way for homes.',\n",
              "  array([-3.7491875 , -3.1956983 , -0.06664714], dtype=float32),\n",
              "  2),\n",
              " (\"In early January, Facebook\\xa0released\\xa0software that turns speech into text more accurately than previous systems and does it in real time, opening up the possibility of better captioning of live video.\\n\\nThe system, which uses a different kind of A.I. software design than had previously been tried for automatic speech recognition, is a good example of the sort of advances that Facebook's A.I. research lab regularly churns out: ones that both push forward the state-of-the-art but also have clear implications for Facebook's business.\\n\\nLive captioning could be a useful feature for Facebook and Instagram posts. More importantly, it can help Facebook police that content for hate speech, bullying, and disinformation, which the social network is under\\xa0increasingly intense pressure\\xa0to prove it can do well.\\n\\nIt seems like a no-brainer that this kind of research would be beneficial to Facebook. So it's surprising to hear Mike Schroepfer, Facebook's chief technology officer, tell me the social network was initially reluctant to create an A.I. research lab.\",\n",
              "  array([-4.9799657 , -4.2542167 , -0.02130387], dtype=float32),\n",
              "  2),\n",
              " ('Google and Alphabet CEO Sundar Pichai has called for new regulations in the world of AI, highlighting the dangers posed by technology like facial recognition and deepfakes, while stressing that any legislation must balance “potential harms ... with social opportunities.”\\n“[T]here is no question in my mind that artificial intelligence needs to be regulated. It is too important not to,” writes Pichai in an\\xa0editorial for\\xa0The Financial Times. “The only question is how to approach it.”\\nAlthough Pichai says new regulation is needed, he advocates a cautious approach that might not see many significant controls placed on AI. He notes that for some products like self-driving cars, “appropriate new rules” should be introduced. But in other areas, like healthcare, existing frameworks can be extended to cover AI-assisted products.\\n““COMPANIES SUCH AS OURS CANNOT JUST BUILD PROMISING NEW TECHNOLOGY AND LET MARKET FORCES DECIDE HOW IT WILL BE USED.”\\n”\\n“Companies such as ours cannot just build promising new technology and let market forces decide how it will be used,” writes Pichai. “It is equally incumbent on us to make sure that technology is harnessed for good and available to everyone.”\\nThe Alphabet CEO, who heads perhaps the most prominent AI company in the world, also stresses that “international alignment will be critical to making global standards work,” highlighting a potential area of difficulty for tech companies when it comes to AI regulation.\\nCurrently, US and EU plans for AI regulation seem to be diverging. While the White House is advocating for\\xa0light-touch regulation\\xa0that avoids “overreach” in order to encourage innovation, the EU is considering more direct intervention, such as a\\xa0five-year ban on facial recognition. As with regulations on data privacy, any divergence between the US and EU will create additional costs and technical challenges for international firms like Google.\\nPichai’s editorial did not call out any specific proposals for regulations, but in comments made later in the day at a conference in Brussels he suggested a temporary ban on facial recognition — as being mooted by the EU —\\xa0might be welcome. This fits with Google’s own approach to facial recognition, which it refuses to sell because of worries it will be used for mass surveillance. Rivals like Microsoft and Amazon continue to sell the technology.\\nAs Pichai notes, “principles that remain on paper are meaningless.” Sooner or later, talk about the need for regulation is going to have to turn into action.\\n',\n",
              "  array([-0.0820126, -3.5243335, -3.010478 ], dtype=float32),\n",
              "  0),\n",
              " ('BRUSSELS (Reuters) - The EU’s proposal for a temporary ban on facial-recognition technology won backing from Alphabet Chief Executive Sundar Pichai on Monday but got a cool response from Microsoft President Brad Smith.\\n\\n\\nWhile Pichai cited the possibility that the technology could be used for nefarious purposes as a reason for a moratorium, Smith said a ban was akin to using a meat cleaver instead of a scalpel to solve potential problems.\\n“I think it is important that governments and regulations tackle it sooner rather than later and give a framework for it,” Pichai told a conference in Brussels organized by think-tank Bruegel.\\n“It can be immediate but maybe there’s a waiting period before we really think about how it’s being used,” he said. “It’s up to governments to chart the course” for the use of such technology.\\nSmith, who is also Microsoft’s chief legal officer, however cited the benefits of facial recognition technology in some instances such as NGOs using it to find missing children.\\n“I’m really reluctant to say let’s stop people from using technology in a way that will reunite families when it can help them do it,” Smith said.\\n“The second thing I would say is you don’t ban it if you actually believe there is a reasonable alternative that will enable us to, say, address this problem with a scalpel instead of a meat cleaver,” he said.\\nSmith said it was important to first identify problems and then craft rules to ensure that the technology would not be used for mass surveillance.\\n“There is only one way at the end of the day to make technology better and that is to use it,” he said.\\n\\nThe European Commission s taking a tougher line on artificial intelligence (AI) than the United States that would strengthen existing regulations on privacy and data rights, according to a proposal paper seen by Reuters.\\nPart of this includes a moratorium of up to five years on using facial recognition technology in public areas, to give the EU time to work out how to prevent abuses, the paper said.\\nPichai urged regulators to take a “proportionate approach” when drafting rules, days before the Commission is due to publish proposals on the issue.\\nRegulators are grappling with ways to govern AI, encouraging innovation while trying to curb potential misuse, as companies and law enforcement agencies increasingly adopt the technology.\\nThere was no question AI needs to be regulated, Pichai said, but rulemakers should tread carefully.\\n“Sensible regulation must also take a proportionate approach, balancing potential harms with social opportunities. This is especially true in areas that are high risk and high value,” he said.\\nRegulators should tailor rules according to different sectors, Pichai said, citing medical devices and self-driving cars as examples that require different rules. He said governments should align their rules and agree on core values.\\nEarlier this month, the U.S. government published regulatory guidelines on AI aimed at limiting authorities’ overreach, and urged Europe to avoid an aggressive approach.\\nPichai said it was important to be clear-eyed about what could go wrong with AI, and while it promised huge benefits there were real concerns about potential negative consequences.\\n\\nOne area of concern is so-called “deepfakes” - video or audio clips that have been manipulated using AI. Pichai said Google had released open datasets to help the research community build better tools to detect such fakes.\\nThe world’s most popular internet search engine said last month that Google Cloud was not offering general-purpose facial-recognition application programming interfaces (APIs) while it establishes policy and technical safeguards.\\n',\n",
              "  array([-0.10035335, -2.7777216 , -3.4021297 ], dtype=float32),\n",
              "  0),\n",
              " ('Alphabet\\xa0\\xa0and Google CEO, Sundar Pichai, is the latest tech giant kingpin to make a public call for AI to be regulated while simultaneously encouraging lawmakers towards a dilute enabling framework that does not put any hard limits on what can be done with AI technologies.\\nIn an op-ed published in today’s\\xa0Financial Times, Pichai makes a headline-grabbing call for artificial intelligence to be regulated. But his pitch injects a suggestive undercurrent that puffs up the risk for humanity of\\xa0not\\xa0letting technologists get on with business as usual and apply AI at population-scale — with the Google chief claiming: “AI has the potential to improve billions of lives, and the biggest risk may be failing to do so” — thereby seeking to frame ‘no hard limits’ as actually the safest option for humanity.\\nSimultaneously the pitch downplays any negatives that might cloud the greater good that Pichai implies AI will unlock — presenting “potential negative consequences” as simply the inevitable and necessary price of technological progress.\\nIt’s all about managing the level of risk, is the leading suggestion, rather than questioning outright whether the use of\\xa0a hugely risk-laden technology such as facial recognition should actually be viable in a democratic society.\\n“Internal combustion engines allowed people to travel beyond their own areas but also caused more accidents,” Pichai writes, raiding history for a self-serving example while ignoring the vast climate costs of combustion engines (and the resulting threat now posed to the survival of countless species on Earth).\\n“The internet made it possible to connect with anyone and get information from anywhere, but also easier for misinformation to spread,” he goes on.\\xa0“These lessons teach us that we need to be clear-eyed about what could go wrong.”\\nFor “clear-eyed” read: Accepting of the technology-industry’s interpretation of ‘collateral damage’. (Which, in the case of misinformation and Facebook, appears to run to feeding democracy itself into the ad-targeting meat-grinder.)\\nMeanwhile, not at all mentioned in Pichai’s discussion of AI risks: The concentration of monopoly power that artificial intelligence appears to be very good at supercharging.\\nFunny that.\\nOf course it’s hardly surprising a tech giant that, in recent years,\\xa0rebranded an entire research division to ‘Google AI’\\xa0— and has\\xa0previously been called out by some of its own workforce\\xa0over a project involving applying AI to military weapons technology — should be lobbying lawmakers to set AI ‘limits’ that are as dilute and abstract as possible.\\nThe only thing that’s better than zero regulation are laws made by useful idiots who’ve fallen hook, line and sinker for industry-expounded false dichotomies — such as those claiming it’s ‘innovation\\xa0or\\xa0privacy’.\\nPichai’s intervention also comes at a strategic moment, with US lawmakers eyeing AI regulation and the White House seemingly throwing itself into alignment with tech giants’ desires for ‘innovation-friendly’ rules which make their business easier. (To wit: This month White House CTO\\xa0Michael Kratsios\\xa0\\xa0warned in a\\xa0Bloomberg op-ed\\xa0against “preemptive, burdensome or duplicative rules that would needlessly hamper AI innovation and growth”.)\\nThe new\\xa0European Commission,\\xa0\\xa0meanwhile, has been sounding a firmer line on both AI and big tech.\\nIt\\xa0has made\\xa0tech-driven change a key policy priority, with president Ursula von der Leyen making public noises about reining in tech giants. She has also committed to publish “a coordinated European approach on the human and ethical implications of Artificial Intelligence” within her first 100 days in office. (She took up the post on December 1, 2019 so the clock is ticking.)\\nLast week\\xa0a leaked draft of the Commission proposals for pan-EU AI regulation suggest it’s leaning towards a relatively light touch approach (albeit, the European version of light touch is considerably more involved and interventionist than anything born in a Trump White House, clearly) — although the paper does float the idea of a temporary ban on the use of facial recognition technology in public places.\\nThe paper notes that such a ban would “safeguard the rights of individuals, in particular against any possible abuse of the technology” — before arguing against such a “far-reaching measure that might hamper the development and uptake of this technology”, in favor of relying on provisions in existing EU law (such as the EU data protection framework, GDPR), in addition to relevant tweaks to current product safety and liability laws.\\nWhile it’s not yet clear which way the Commission will jump on regulating AI, even the lightish-touch version its considering would likely be a lot more onerous than Pichai would like.\\nIn the op-ed he calls for what he couches as “sensible regulation” — aka taking a “proportionate approach, balancing potential harms, especially in high-risk areas, with social opportunities”.\\nFor “social opportunities” read: The plentiful ‘business opportunities’\\xa0Google\\xa0\\xa0is spying — assuming the hoped for vast additional revenue scale it can get by supercharging expansion of AI-powered services into all sorts of industries and sectors (from health to transportation to everywhere else in between) isn’t derailed by hard legal limits on where AI can\\xa0actually\\xa0be applied.\\n“Regulation can provide broad guidance while allowing for tailored implementation in different sectors,” Pichai urges, setting out a preference for enabling “principles” and post-application “reviews”, to keep the AI spice flowing.\\nThe op-ed only touches very briefly on facial recognition — despite the FT editors choosing to illustrate it with an image of the tech. Here Pichai again seeks to reframe the debate around what is, by nature, an extremely rights-hostile technology — talking only in passing of “nefarious uses” of facial recognition.\\nOf course this wilfully obfuscates the inherent risks of letting blackbox machines make algorithmic guesses at identity every time a face happens to pass through a public space.\\nYou can’t hope to protect people’s privacy in such a scenario. Many other rights are also at risk, depending on what else the technology is being used for. So, really, any use of facial recognition is laden with individual and societal risk.\\nBut Pichai is seeking to put blinkers on lawmakers. He doesn’t want them to see inherent risks baked into such a potent and powerful technology — pushing them towards only a narrow, ill-intended subset of “nefarious” and “negative” AI uses and “consequences” as being worthy of “real concerns”.\\xa0\\nAnd so he returns to banging the drum for “a principled and regulated approach to\\xa0applying\\xa0AI” [emphasis ours] — putting the emphasis on regulation that, above all, gives the green light for AI to be applied.\\nWhat technologists fear most here is rules that tell them when artificial intelligence absolutely cannot apply.\\nEthics and principles are, to a degree, mutable concepts — and ones which the tech giants have become very practiced at claiming as their own, for PR purposes, including by attaching self-styled ‘guard-rails’ to their own AI operations. (But of course there’s no actual legal binds there.)\\nAt the same time data-mining giants like Google are very smooth operators when it comes to gaming existing EU rules around data protection, such as by infesting their user-interfaces with confusing dark patterns that push people to click or swipe their rights away.\\nBut a ban on applying certain types of AI would change the rules of the game. Because it would put society in the driving seat.\\nLaws that contained at least a moratorium on certain “dangerous” applications of AI — such as facial recognition technology, or autonomous weapons like the drone-based system Google was previously working on — have been called for by some\\xa0far-sighted regulators.\\nAnd a ban would be far harder for platform giants to simply bend to their will.',\n",
              "  array([-0.09106059, -2.6519723 , -4.1028414 ], dtype=float32),\n",
              "  0),\n",
              " ('The head of Google and parent company Alphabet has called for artificial intelligence (AI) to be regulated.\\nWriting in the Financial Times, Sundar Pichai said it was \"too important not to\" impose regulation but argued for \"a sensible approach\".\\nHe said that individual areas of AI development, like self-driving cars and health tech, required tailored rules.\\nLast week it was revealed that the European Commission is\\xa0considering a five-year ban on facial recognition.\\nEarlier this month, the White House published its own proposed regulatory principles and urged Europe to \"avoid heavy-handed innovation-killing models\".\\nMr Pichai noted that while AI had enormous potential there were also considerable dangers, such as the misuse of deepfakes, which are computer-generated clips that are designed to look real.\\nMaria Axente, responsible AI lead at Pricewaterhouse Coopers, told the BBC she believes regulation is the right path for the sector.\\n\"The question is how can it be done in a way that doesn\\'t kill innovation, as well as continue to balance the benefits of AI with the risks it poses, as AI becomes more embedded in our lives?\" she said.\\n\"Regulation and self-regulation, via a code of ethics and an ethics board, might not be enough to do that.\"\\nGoogle launched its own independent ethics board in 2019,\\xa0but shut it down less than two weeks later\\xa0following controversy about who had been appointed to it.\\nMr Pichai will be speaking at the World Economic Forum,\\xa0which takes place this week in Switzerland, along with Microsoft\\'s Satya Nadella and Huawei founder Ren Zhengfei.',\n",
              "  array([-0.06503656, -3.1993215 , -3.8086987 ], dtype=float32),\n",
              "  0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w91NA6DqVdlV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence=[]\n",
        "polarity=[]\n",
        "distribution=[]\n",
        "for i in range(len(test)):\n",
        "    sentence.append(predictions[i][0])\n",
        "    polarity.append(predictions[i][2])\n",
        "    distribution.append(predictions[i][1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PL5nIuvVdld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results=pd.DataFrame(data={'Source':test['Source'],'Author':test['Author'],'Url':test['Url'],'Date':test['Date'],'Article':test['Article'],'Title':test['Title'],'Polarity_true': test['Polarity'], 'Polarity_predict': polarity})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVzY3s09Vdlg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_wide=pd.DataFrame(data={'Source':test['Source'],'Author':test['Author'],'Url':test['Url'],'Date':test['Date'],'Article':test['Article'],'Title':test['Title'],'Polarity distribution':distribution,'Polarity_true': test['Polarity'], 'Polarity_predict': polarity})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSdxRl-sVdls",
        "colab_type": "text"
      },
      "source": [
        "### Accuracy of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTgHx9hhVdlt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b7e2bd7a-4aca-47ba-a59d-55da459d988b"
      },
      "source": [
        "predict=0\n",
        "total = len(results)\n",
        "print(total)\n",
        "for i in range(len(results)):\n",
        "    if (results['Polarity_true'][i]==results['Polarity_predict'][i]):\n",
        "        predict+=1\n",
        "        \n",
        "accuracy=predict/total\n",
        "print(accuracy)\n",
        "        "
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "178\n",
            "0.6966292134831461\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOqZwJPTVdlw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "6b5ef059-9043-4885-9cfe-142980ba3362"
      },
      "source": [
        "for i in range(len(results)):\n",
        "    if results['Polarity_true'][i]==0:\n",
        "            results['Polarity_true'][i]='negative'\n",
        "    elif results['Polarity_true'][i]==1:\n",
        "            results['Polarity_true'][i]='neutral'\n",
        "    elif results['Polarity_true'][i]==2:\n",
        "            results['Polarity_true'][i]='positive'   \n",
        "            \n",
        "    if results['Polarity_predict'][i]==0:\n",
        "            results['Polarity_predict'][i]='negative'\n",
        "    elif results['Polarity_predict'][i]==1:\n",
        "            results['Polarity_predict'][i]='neutral'\n",
        "    elif results['Polarity_predict'][i]==2:\n",
        "            results['Polarity_predict'][i]='positive' "
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:205: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt4KDIhnVdl1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "outputId": "18ec24fd-e853-4b23-ba32-e6d4362d46ca"
      },
      "source": [
        "results"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source</th>\n",
              "      <th>Author</th>\n",
              "      <th>Url</th>\n",
              "      <th>Date</th>\n",
              "      <th>Article</th>\n",
              "      <th>Title</th>\n",
              "      <th>Polarity_true</th>\n",
              "      <th>Polarity_predict</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wired</td>\n",
              "      <td>Paris Martineau</td>\n",
              "      <td>https://www.wired.com/story/facebook-removes-a...</td>\n",
              "      <td>2019-12-21T01:21:05Z</td>\n",
              "      <td>Facebook on Friday removed what it called a gl...</td>\n",
              "      <td>Facebook Removes Accounts With AI-Generated Pr...</td>\n",
              "      <td>positive</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Wired</td>\n",
              "      <td>Tom Simonite</td>\n",
              "      <td>https://www.wired.com/story/ai-doctor-will-see...</td>\n",
              "      <td>2019-12-21T13:00:00Z</td>\n",
              "      <td>When MIT professor Regina Barzilay received he...</td>\n",
              "      <td>The AI Doctor Will See You Now</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Wired</td>\n",
              "      <td>Andy Greenberg, Brian Barrett</td>\n",
              "      <td>https://www.wired.com/story/facebook-two-facto...</td>\n",
              "      <td>2019-12-21T14:00:00Z</td>\n",
              "      <td>It's beginning to look a lot like the end of t...</td>\n",
              "      <td>Facebook Finally Fixes Its Two-Factor Mess</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TechCrunch</td>\n",
              "      <td>Devin Coldewey</td>\n",
              "      <td>http://techcrunch.com/2019/12/21/how-to-bring-...</td>\n",
              "      <td>2019-12-21T17:47:37Z</td>\n",
              "      <td>Artificial intelligence is a powerful tool, bu...</td>\n",
              "      <td>Luminance and Omnius are bringing AI to legacy...</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The Next Web</td>\n",
              "      <td>John Gikopoulos</td>\n",
              "      <td>https://thenextweb.com/podium/2019/12/21/2020-...</td>\n",
              "      <td>2019-12-21T20:00:39Z</td>\n",
              "      <td>One of the ways you can tell that a new techno...</td>\n",
              "      <td>2020 will see AI move from buzzwords to solutions</td>\n",
              "      <td>negative</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>Fortune</td>\n",
              "      <td>Jeremy Kahn</td>\n",
              "      <td>https://fortune.com/2020/01/20/facebook-artifi...</td>\n",
              "      <td>2020-01-20T09:30:00Z</td>\n",
              "      <td>In early January, Facebook released software t...</td>\n",
              "      <td>Facebook wants better A.I. tools. But superint...</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>The Verge</td>\n",
              "      <td>James Vincent</td>\n",
              "      <td>https://www.theverge.com/2020/1/20/21073682/ai...</td>\n",
              "      <td>2020-01-20T10:30:05Z</td>\n",
              "      <td>Google and Alphabet CEO Sundar Pichai has call...</td>\n",
              "      <td>Alphabet CEO Sundar Pichai says there is ‘no q...</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>Reuters</td>\n",
              "      <td>Foo Yun Chee and John Chalmers</td>\n",
              "      <td>https://ca.reuters.com/article/technologyNews/...</td>\n",
              "      <td>2020-01-20T12:01:46Z</td>\n",
              "      <td>BRUSSELS (Reuters) - The EU’s proposal for a t...</td>\n",
              "      <td>Google owner calls for 'proportionate approach...</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>TechCrunch</td>\n",
              "      <td>Natasha Lomas</td>\n",
              "      <td>https://techcrunch.com/2020/01/20/googles-sund...</td>\n",
              "      <td>2020-01-20T14:11:15Z</td>\n",
              "      <td>Alphabet  and Google CEO, Sundar Pichai, is th...</td>\n",
              "      <td>Google's Sundar Pichai doesn't want you to be ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>BBC News</td>\n",
              "      <td>https://www.facebook.com/bbcnews</td>\n",
              "      <td>https://www.bbc.co.uk/news/technology-51178198</td>\n",
              "      <td>2020-01-20T15:40:00Z</td>\n",
              "      <td>The head of Google and parent company Alphabet...</td>\n",
              "      <td>Google boss Sundar Pichai calls for AI regulation</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>178 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Source  ... Polarity_predict\n",
              "0           Wired  ...         negative\n",
              "1           Wired  ...         positive\n",
              "2           Wired  ...         positive\n",
              "3      TechCrunch  ...         positive\n",
              "4    The Next Web  ...         positive\n",
              "..            ...  ...              ...\n",
              "173       Fortune  ...         positive\n",
              "174     The Verge  ...         negative\n",
              "175       Reuters  ...         negative\n",
              "176    TechCrunch  ...         negative\n",
              "177      BBC News  ...         negative\n",
              "\n",
              "[178 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gIAifYGlg-K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "8e86c9b3-ce67-49d9-9ff3-04f77c111624"
      },
      "source": [
        "results['Polarity_predict'].value_counts()"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "positive    123\n",
              "negative     41\n",
              "neutral      14\n",
              "Name: Polarity_predict, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iQZ65O2lmn4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "f293acbf-c357-4155-a8c1-a25db6e437b7"
      },
      "source": [
        "results['Polarity_true'].value_counts()"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "positive    101\n",
              "negative     51\n",
              "neutral      26\n",
              "Name: Polarity_true, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PRcYxLOnFCM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q6AOH4LnHDw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "3cdfe614-133a-41d3-9470-c485f1bf0db0"
      },
      "source": [
        "confusion_matrix(results['Polarity_true'], results['Polarity_predict'])"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[29,  5, 17],\n",
              "       [ 5,  5, 16],\n",
              "       [ 7,  4, 90]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUXp3upvpZvK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "6b07fe67-6f3c-4289-d22a-4eedc80ece57"
      },
      "source": [
        "results_wide['Polarity distribution']"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        [-0.5121273, -2.8696733, -1.0669303]\n",
              "1       [-4.0325685, -4.2178593, -0.03299731]\n",
              "2        [-1.710292, -1.7299899, -0.44332144]\n",
              "3       [-4.8914614, -4.3072643, -0.02120397]\n",
              "4        [-1.0109742, -1.5238645, -0.8716344]\n",
              "                        ...                  \n",
              "173    [-4.9799657, -4.2542167, -0.021303868]\n",
              "174       [-0.0820126, -3.5243335, -3.010478]\n",
              "175     [-0.10035335, -2.7777216, -3.4021297]\n",
              "176    [-0.091060586, -2.6519723, -4.1028414]\n",
              "177     [-0.06503656, -3.1993215, -3.8086987]\n",
              "Name: Polarity distribution, Length: 178, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGDblQ0AqLps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import log_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtATyPjbu963",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.special import softmax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjkid8Unp1rq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "distribution_01 =softmax(distribution)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVaqqRn21ORz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eada4aa7-022d-4ffb-aa70-e229ee2062c8"
      },
      "source": [
        "distribution_01"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.36640095e-03, 3.18637496e-04, 1.93293835e-03],\n",
              "       [9.95996743e-05, 8.27536060e-05, 5.43562463e-03],\n",
              "       [1.01580331e-03, 9.95989656e-04, 3.60618485e-03],\n",
              "       [4.21934710e-05, 7.56761146e-05, 5.50010754e-03],\n",
              "       [2.04418204e-03, 1.22397882e-03, 2.34981626e-03],\n",
              "       [8.24053568e-05, 3.76082462e-04, 5.15948934e-03],\n",
              "       [4.11400724e-05, 6.79454388e-05, 5.50889224e-03],\n",
              "       [7.26787257e-05, 3.16579943e-04, 5.22871781e-03],\n",
              "       [2.11284481e-04, 1.57796880e-04, 5.24889491e-03],\n",
              "       [8.96581332e-04, 2.50287791e-04, 4.47110785e-03],\n",
              "       [3.83548485e-03, 3.31659685e-04, 1.45083235e-03],\n",
              "       [6.17003127e-04, 3.89711873e-04, 4.61126119e-03],\n",
              "       [1.02959608e-03, 6.72341790e-04, 3.91603913e-03],\n",
              "       [3.88703193e-04, 3.90542089e-04, 4.83873067e-03],\n",
              "       [1.50289081e-04, 7.60836352e-04, 4.70685167e-03],\n",
              "       [2.52623722e-04, 1.16132847e-04, 5.24922041e-03],\n",
              "       [8.05347590e-05, 1.90780978e-04, 5.34666004e-03],\n",
              "       [1.35272494e-04, 6.69684785e-04, 4.81302012e-03],\n",
              "       [2.16283399e-04, 2.35906825e-03, 3.04262573e-03],\n",
              "       [5.77736704e-04, 4.86066705e-03, 1.79573050e-04],\n",
              "       [5.11980895e-03, 2.25779382e-04, 2.72388686e-04],\n",
              "       [7.01065699e-04, 1.74220884e-04, 4.74269129e-03],\n",
              "       [1.76332542e-04, 2.06028135e-03, 3.38136265e-03],\n",
              "       [1.79216004e-04, 9.16437188e-04, 4.52232361e-03],\n",
              "       [4.24423395e-03, 2.24917167e-04, 1.14882586e-03],\n",
              "       [9.95425071e-05, 4.42866440e-04, 5.07556694e-03],\n",
              "       [7.32577610e-05, 2.04647280e-04, 5.34007140e-03],\n",
              "       [3.36431479e-03, 7.05262239e-04, 1.54839968e-03],\n",
              "       [8.20307177e-05, 1.67499456e-04, 5.36844647e-03],\n",
              "       [5.70515986e-04, 1.52652297e-04, 4.89480887e-03],\n",
              "       [2.55097984e-04, 1.92544845e-04, 5.17033273e-03],\n",
              "       [3.27073620e-03, 4.48889623e-04, 1.89835147e-03],\n",
              "       [4.79718298e-03, 6.95406343e-04, 1.25387174e-04],\n",
              "       [7.53608110e-05, 1.46016508e-04, 5.39659942e-03],\n",
              "       [2.06960889e-04, 1.30793708e-03, 4.10307944e-03],\n",
              "       [7.35999129e-05, 1.83243552e-04, 5.36113279e-03],\n",
              "       [5.03765077e-05, 1.04148181e-04, 5.46345254e-03],\n",
              "       [6.58658100e-04, 3.85641330e-03, 1.10290642e-03],\n",
              "       [1.07838970e-03, 2.09498568e-03, 2.44460162e-03],\n",
              "       [6.13629236e-05, 1.40908858e-04, 5.41570643e-03],\n",
              "       [4.82737366e-03, 4.94304404e-04, 2.96297803e-04],\n",
              "       [3.52908159e-04, 4.38596308e-03, 8.79106170e-04],\n",
              "       [4.59273392e-03, 7.89757178e-04, 2.35486703e-04],\n",
              "       [7.15842325e-05, 6.48355999e-05, 5.48155699e-03],\n",
              "       [6.56466727e-05, 6.98316144e-05, 5.48249809e-03],\n",
              "       [1.03664905e-04, 7.76061861e-05, 5.43670589e-03],\n",
              "       [3.71627789e-03, 1.35831663e-03, 5.43382601e-04],\n",
              "       [3.88380977e-05, 2.22886301e-04, 5.35625219e-03],\n",
              "       [4.92045147e-05, 3.19518760e-04, 5.24925254e-03],\n",
              "       [7.95515734e-05, 1.88212114e-04, 5.35021257e-03],\n",
              "       [3.25835426e-05, 7.48629682e-05, 5.51053183e-03],\n",
              "       [1.40896620e-04, 7.51803454e-05, 5.40190004e-03],\n",
              "       [1.00742305e-04, 1.08858301e-04, 5.40837739e-03],\n",
              "       [8.27693148e-05, 9.57242592e-05, 5.43948309e-03],\n",
              "       [1.63163393e-04, 3.35126242e-04, 5.11968695e-03],\n",
              "       [5.06643625e-03, 4.50637919e-04, 1.00901816e-04],\n",
              "       [3.38418875e-03, 2.08554580e-03, 1.48241830e-04],\n",
              "       [1.71298496e-04, 9.54475909e-05, 5.35123097e-03],\n",
              "       [4.56300098e-03, 9.68410808e-04, 8.65651455e-05],\n",
              "       [5.25349678e-05, 9.17172001e-05, 5.47372410e-03],\n",
              "       [5.05905366e-04, 2.85046292e-04, 4.82702628e-03],\n",
              "       [4.41611082e-05, 1.74886416e-04, 5.39892865e-03],\n",
              "       [6.03251683e-05, 7.21731994e-05, 5.48547925e-03],\n",
              "       [2.94907438e-03, 2.54088314e-03, 1.28020052e-04],\n",
              "       [3.36839657e-05, 7.25248756e-05, 5.51176677e-03],\n",
              "       [4.32966699e-05, 1.52984721e-04, 5.42169577e-03],\n",
              "       [1.18805481e-04, 1.15458781e-04, 5.38371224e-03],\n",
              "       [1.91610353e-03, 1.16196391e-03, 2.53990921e-03],\n",
              "       [3.32575670e-04, 2.30865716e-03, 2.97674444e-03],\n",
              "       [7.65571313e-05, 2.49398348e-04, 5.29202027e-03],\n",
              "       [1.76420683e-04, 1.51065621e-03, 3.93089931e-03],\n",
              "       [9.57174125e-05, 1.15719151e-04, 5.40654128e-03],\n",
              "       [8.24419913e-05, 1.67145365e-04, 5.36839012e-03],\n",
              "       [1.60109033e-04, 1.14881655e-03, 4.30905074e-03],\n",
              "       [2.74803629e-03, 2.73918081e-03, 1.30759334e-04],\n",
              "       [9.39042147e-05, 9.39841266e-05, 5.43008884e-03],\n",
              "       [4.08863882e-03, 1.37221557e-03, 1.57122646e-04],\n",
              "       [1.18550262e-04, 5.31614583e-04, 4.96781245e-03],\n",
              "       [4.45071273e-05, 5.87319300e-05, 5.51473768e-03],\n",
              "       [6.91999274e-04, 6.60670630e-04, 4.26530745e-03],\n",
              "       [4.28118437e-05, 7.65513687e-05, 5.49861277e-03],\n",
              "       [2.22967734e-04, 2.48451740e-03, 2.91049178e-03],\n",
              "       [2.33199238e-03, 1.69021951e-03, 1.59576500e-03],\n",
              "       [8.59764768e-05, 7.46573496e-05, 5.45734400e-03],\n",
              "       [5.72528552e-05, 1.07303560e-04, 5.45342127e-03],\n",
              "       [2.59858090e-03, 2.90724752e-03, 1.12147936e-04],\n",
              "       [1.00044964e-03, 1.06108328e-03, 3.55644361e-03],\n",
              "       [5.77522696e-05, 9.75005096e-05, 5.46272332e-03],\n",
              "       [7.60636467e-04, 6.00457308e-04, 4.25688317e-03],\n",
              "       [5.09089045e-03, 3.78485594e-04, 1.48600651e-04],\n",
              "       [4.18796153e-05, 1.16226474e-04, 5.45987161e-03],\n",
              "       [5.14785293e-03, 1.50609034e-04, 3.19515093e-04],\n",
              "       [2.64903624e-03, 2.67927814e-03, 2.89663265e-04],\n",
              "       [4.58822493e-03, 3.01907305e-04, 7.27845414e-04],\n",
              "       [7.73987922e-05, 1.35005583e-04, 5.40557224e-03],\n",
              "       [1.38850603e-03, 2.73707090e-03, 1.49240030e-03],\n",
              "       [2.35808990e-03, 2.59136572e-03, 6.68520224e-04],\n",
              "       [4.83775439e-05, 1.17529918e-04, 5.45206945e-03],\n",
              "       [1.34155562e-03, 4.03769733e-03, 2.38723122e-04],\n",
              "       [1.59197778e-04, 1.19588360e-04, 5.33919036e-03],\n",
              "       [1.14851038e-03, 1.99251881e-04, 4.27021552e-03],\n",
              "       [4.30663908e-03, 1.97627218e-04, 1.11371011e-03],\n",
              "       [4.94066626e-03, 1.76662448e-04, 5.00648865e-04],\n",
              "       [5.10703726e-03, 4.14844340e-04, 9.60954276e-05],\n",
              "       [4.19887911e-05, 7.86460660e-05, 5.49734104e-03],\n",
              "       [7.37314258e-05, 1.58557465e-04, 5.38568897e-03],\n",
              "       [8.58313870e-05, 1.59289804e-04, 5.37285628e-03],\n",
              "       [5.11687994e-03, 3.90318688e-04, 1.10777444e-04],\n",
              "       [6.51171722e-05, 1.43684607e-04, 5.40917413e-03],\n",
              "       [4.51836167e-05, 1.52639055e-04, 5.42015489e-03],\n",
              "       [1.01325531e-04, 1.32673551e-04, 5.38397674e-03],\n",
              "       [4.14298520e-05, 1.49537067e-04, 5.42701082e-03],\n",
              "       [1.06467705e-04, 2.98735336e-04, 5.21277310e-03],\n",
              "       [4.74202607e-05, 2.13682506e-04, 5.35687525e-03],\n",
              "       [1.41192623e-03, 9.64526786e-04, 3.24152340e-03],\n",
              "       [1.24532846e-03, 8.45807197e-04, 3.52684106e-03],\n",
              "       [2.81703367e-04, 2.25838390e-04, 5.11043565e-03],\n",
              "       [3.18713044e-03, 9.06702364e-04, 1.52414385e-03],\n",
              "       [1.83563738e-03, 2.15392234e-03, 1.62841694e-03],\n",
              "       [3.89982155e-03, 1.25626335e-03, 4.61892370e-04],\n",
              "       [2.15643697e-04, 4.47178783e-04, 4.95515531e-03],\n",
              "       [5.01793483e-03, 4.98116424e-04, 1.01925936e-04],\n",
              "       [5.11190249e-03, 1.52071225e-04, 3.54003016e-04],\n",
              "       [1.65529083e-04, 1.38852512e-04, 5.31359669e-03],\n",
              "       [3.56303295e-03, 5.52152575e-04, 1.50279049e-03],\n",
              "       [2.22214570e-04, 1.12365902e-04, 5.28339716e-03],\n",
              "       [8.42987429e-05, 3.59380268e-04, 5.17429644e-03],\n",
              "       [5.36788793e-05, 9.58145829e-05, 5.46848262e-03],\n",
              "       [6.75965566e-05, 1.41672179e-04, 5.40870754e-03],\n",
              "       [9.93084977e-05, 2.56254716e-04, 5.26241260e-03],\n",
              "       [4.04286220e-05, 1.54091700e-04, 5.42345643e-03],\n",
              "       [3.62750987e-04, 3.51577764e-03, 1.73944875e-03],\n",
              "       [1.53748944e-04, 4.92951891e-04, 4.97127697e-03],\n",
              "       [4.57767819e-05, 8.45653485e-05, 5.48763527e-03],\n",
              "       [4.06440673e-03, 6.87868567e-04, 8.65700596e-04],\n",
              "       [4.25236719e-03, 7.60530587e-04, 6.05078181e-04],\n",
              "       [3.12929886e-04, 1.31011606e-04, 5.17403474e-03],\n",
              "       [1.69513805e-04, 1.84194418e-04, 5.26426733e-03],\n",
              "       [2.02564974e-04, 3.32858704e-04, 5.08255372e-03],\n",
              "       [2.04671291e-04, 1.28081057e-03, 4.13249480e-03],\n",
              "       [2.59671663e-03, 2.16664092e-04, 2.80459574e-03],\n",
              "       [2.03395821e-03, 1.23153382e-03, 2.35248450e-03],\n",
              "       [6.68365610e-05, 8.92003154e-05, 5.46193914e-03],\n",
              "       [5.18208882e-03, 2.29379730e-04, 2.06507815e-04],\n",
              "       [4.06186082e-05, 1.75045425e-04, 5.40231261e-03],\n",
              "       [4.44165152e-03, 2.67016294e-04, 9.09308437e-04],\n",
              "       [5.74058868e-05, 2.08922938e-04, 5.35164680e-03],\n",
              "       [1.25647901e-04, 9.69991379e-04, 4.52233665e-03],\n",
              "       [3.10716059e-05, 1.39148004e-04, 5.44775790e-03],\n",
              "       [1.99758171e-04, 1.22965442e-03, 4.18856507e-03],\n",
              "       [1.88271550e-03, 3.55936843e-03, 1.75893510e-04],\n",
              "       [1.71989610e-03, 3.78247630e-03, 1.15603441e-04],\n",
              "       [2.05794946e-04, 1.94483699e-04, 5.21769701e-03],\n",
              "       [3.39661565e-05, 3.04237765e-04, 5.27977291e-03],\n",
              "       [2.55929393e-04, 9.54295756e-05, 5.26661752e-03],\n",
              "       [3.06340703e-03, 1.55846914e-03, 9.96099901e-04],\n",
              "       [5.15885149e-05, 7.80490736e-05, 5.48833888e-03],\n",
              "       [6.05732857e-05, 1.11362890e-04, 5.44604147e-03],\n",
              "       [4.17751835e-05, 2.35760177e-04, 5.34044066e-03],\n",
              "       [1.99217210e-03, 2.74934177e-03, 8.76462553e-04],\n",
              "       [4.12696318e-05, 2.13564344e-04, 5.36314258e-03],\n",
              "       [6.50802904e-05, 8.47516058e-05, 5.46814408e-03],\n",
              "       [3.94591916e-05, 2.01729068e-04, 5.37678786e-03],\n",
              "       [4.93748486e-03, 5.56261395e-04, 1.24230486e-04],\n",
              "       [4.74446424e-05, 1.16836709e-04, 5.45369461e-03],\n",
              "       [1.18639044e-04, 7.70833693e-04, 4.72850353e-03],\n",
              "       [4.97035962e-03, 4.08965832e-04, 2.38649824e-04],\n",
              "       [5.58714200e-05, 1.52546199e-04, 5.40955830e-03],\n",
              "       [4.21666028e-03, 1.24324777e-03, 1.58068593e-04],\n",
              "       [1.53909076e-03, 3.16419080e-03, 9.14694800e-04],\n",
              "       [5.10052196e-04, 1.18011187e-04, 4.98991273e-03],\n",
              "       [2.35394994e-03, 3.60190199e-04, 2.90383655e-03],\n",
              "       [1.32229543e-04, 2.29988451e-04, 5.25575969e-03],\n",
              "       [3.86196662e-05, 7.97988978e-05, 5.49955945e-03],\n",
              "       [5.17561892e-03, 1.65569814e-04, 2.76787177e-04],\n",
              "       [5.08156046e-03, 3.49325972e-04, 1.87091311e-04],\n",
              "       [5.12900110e-03, 3.96134827e-04, 9.28407389e-05],\n",
              "       [5.26422961e-03, 2.29156489e-04, 1.24589991e-04]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6HKw0VBpCVu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "4094299e-2cf2-4da3-d5ba-8938f7925344"
      },
      "source": [
        "!pip install chart_studio"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting chart_studio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/3f/d2f3f506ba1aaf109f549f8b01d1483cd3e324c5ebe6b206acee66efdf46/chart_studio-1.0.0-py3-none-any.whl (76kB)\n",
            "\r\u001b[K     |████▎                           | 10kB 15.8MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 2.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from chart_studio) (1.3.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from chart_studio) (4.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from chart_studio) (1.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from chart_studio) (2.21.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->chart_studio) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->chart_studio) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->chart_studio) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->chart_studio) (2019.11.28)\n",
            "Installing collected packages: chart-studio\n",
            "Successfully installed chart-studio-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNUjxhTEVdl5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "9e553cdf-26a4-4ad2-af23-834d2387f831"
      },
      "source": [
        "from plotly.offline import iplot, init_notebook_mode\n",
        "# standard plotly imports\n",
        "import chart_studio.plotly as py\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import iplot, init_notebook_mode\n",
        "# using plotly + cu|fflinks in offline mode\n",
        "import cufflinks\n",
        "cufflinks.go_offline(connected=True)\n",
        "init_notebook_mode(connected=True)\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5aAZBsQVdl8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "0996d37b-05c5-4496-aa15-5ff7c0b3867e"
      },
      "source": [
        "results['Polarity_true'].iplot(\n",
        "    kind='hist',\n",
        "    histnorm='percent',\n",
        "    barmode='overlay',\n",
        "    xTitle='Positivity',\n",
        "    yTitle='Number of articles')"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "        \n",
              "        \n",
              "            <div id=\"33280001-c15f-4005-b168-d427632c8da7\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                require([\"plotly\"], function(Plotly) {\n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
              "                    \n",
              "                if (document.getElementById(\"33280001-c15f-4005-b168-d427632c8da7\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '33280001-c15f-4005-b168-d427632c8da7',\n",
              "                        [{\"histfunc\": \"count\", \"histnorm\": \"percent\", \"marker\": {\"color\": \"rgba(255, 153, 51, 1.0)\", \"line\": {\"color\": \"#4D5663\", \"width\": 1.3}}, \"name\": \"Polarity_true\", \"opacity\": 0.8, \"orientation\": \"v\", \"type\": \"histogram\", \"x\": [\"positive\", \"positive\", \"positive\", \"positive\", \"negative\", \"positive\", \"positive\", \"positive\", \"positive\", \"negative\", \"neutral\", \"neutral\", \"positive\", \"positive\", \"negative\", \"positive\", \"positive\", \"positive\", \"negative\", \"positive\", \"negative\", \"neutral\", \"neutral\", \"neutral\", \"negative\", \"neutral\", \"positive\", \"negative\", \"neutral\", \"negative\", \"neutral\", \"negative\", \"neutral\", \"neutral\", \"neutral\", \"negative\", \"positive\", \"neutral\", \"neutral\", \"neutral\", \"negative\", \"positive\", \"neutral\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"neutral\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"negative\", \"negative\", \"positive\", \"negative\", \"positive\", \"positive\", \"positive\", \"positive\", \"negative\", \"positive\", \"positive\", \"positive\", \"negative\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"negative\", \"positive\", \"negative\", \"positive\", \"positive\", \"negative\", \"positive\", \"negative\", \"positive\", \"positive\", \"positive\", \"negative\", \"neutral\", \"positive\", \"positive\", \"negative\", \"positive\", \"neutral\", \"negative\", \"negative\", \"positive\", \"negative\", \"neutral\", \"positive\", \"positive\", \"neutral\", \"negative\", \"negative\", \"negative\", \"negative\", \"positive\", \"positive\", \"positive\", \"negative\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"negative\", \"negative\", \"negative\", \"neutral\", \"neutral\", \"negative\", \"neutral\", \"negative\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"negative\", \"positive\", \"positive\", \"neutral\", \"positive\", \"positive\", \"negative\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"neutral\", \"negative\", \"positive\", \"positive\", \"negative\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"neutral\", \"positive\", \"positive\", \"positive\", \"negative\", \"positive\", \"positive\", \"positive\", \"negative\", \"positive\", \"positive\", \"positive\", \"negative\", \"positive\", \"positive\", \"negative\", \"positive\", \"negative\", \"negative\", \"negative\", \"negative\", \"positive\", \"positive\", \"negative\", \"negative\", \"negative\", \"negative\"]}],\n",
              "                        {\"barmode\": \"overlay\", \"legend\": {\"bgcolor\": \"#F5F6F9\", \"font\": {\"color\": \"#4D5663\"}}, \"paper_bgcolor\": \"#F5F6F9\", \"plot_bgcolor\": \"#F5F6F9\", \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"font\": {\"color\": \"#4D5663\"}}, \"xaxis\": {\"gridcolor\": \"#E1E5ED\", \"showgrid\": true, \"tickfont\": {\"color\": \"#4D5663\"}, \"title\": {\"font\": {\"color\": \"#4D5663\"}, \"text\": \"Positivity\"}, \"zerolinecolor\": \"#E1E5ED\"}, \"yaxis\": {\"gridcolor\": \"#E1E5ED\", \"showgrid\": true, \"tickfont\": {\"color\": \"#4D5663\"}, \"title\": {\"font\": {\"color\": \"#4D5663\"}, \"text\": \"Number of articles\"}, \"zerolinecolor\": \"#E1E5ED\"}},\n",
              "                        {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('33280001-c15f-4005-b168-d427632c8da7');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                });\n",
              "            </script>\n",
              "        </div>"
            ],
            "application/vnd.plotly.v1+json": {
              "data": [
                {
                  "opacity": 0.8,
                  "orientation": "v",
                  "histnorm": "percent",
                  "histfunc": "count",
                  "marker": {
                    "color": "rgba(255, 153, 51, 1.0)",
                    "line": {
                      "color": "#4D5663",
                      "width": 1.3
                    }
                  },
                  "x": [
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "negative",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "negative",
                    "neutral",
                    "neutral",
                    "positive",
                    "positive",
                    "negative",
                    "positive",
                    "positive",
                    "positive",
                    "negative",
                    "positive",
                    "negative",
                    "neutral",
                    "neutral",
                    "neutral",
                    "negative",
                    "neutral",
                    "positive",
                    "negative",
                    "neutral",
                    "negative",
                    "neutral",
                    "negative",
                    "neutral",
                    "neutral",
                    "neutral",
                    "negative",
                    "positive",
                    "neutral",
                    "neutral",
                    "neutral",
                    "negative",
                    "positive",
                    "neutral",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "neutral",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "negative",
                    "negative",
                    "positive",
                    "negative",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "negative",
                    "positive",
                    "positive",
                    "positive",
                    "negative",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "negative",
                    "positive",
                    "negative",
                    "positive",
                    "positive",
                    "negative",
                    "positive",
                    "negative",
                    "positive",
                    "positive",
                    "positive",
                    "negative",
                    "neutral",
                    "positive",
                    "positive",
                    "negative",
                    "positive",
                    "neutral",
                    "negative",
                    "negative",
                    "positive",
                    "negative",
                    "neutral",
                    "positive",
                    "positive",
                    "neutral",
                    "negative",
                    "negative",
                    "negative",
                    "negative",
                    "positive",
                    "positive",
                    "positive",
                    "negative",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "negative",
                    "negative",
                    "negative",
                    "neutral",
                    "neutral",
                    "negative",
                    "neutral",
                    "negative",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "negative",
                    "positive",
                    "positive",
                    "neutral",
                    "positive",
                    "positive",
                    "negative",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "neutral",
                    "negative",
                    "positive",
                    "positive",
                    "negative",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "neutral",
                    "positive",
                    "positive",
                    "positive",
                    "negative",
                    "positive",
                    "positive",
                    "positive",
                    "negative",
                    "positive",
                    "positive",
                    "positive",
                    "negative",
                    "positive",
                    "positive",
                    "negative",
                    "positive",
                    "negative",
                    "negative",
                    "negative",
                    "negative",
                    "positive",
                    "positive",
                    "negative",
                    "negative",
                    "negative",
                    "negative"
                  ],
                  "type": "histogram",
                  "name": "Polarity_true"
                }
              ],
              "config": {
                "plotlyServerURL": "https://plot.ly",
                "linkText": "Export to plot.ly",
                "showLink": true
              },
              "layout": {
                "xaxis": {
                  "tickfont": {
                    "color": "#4D5663"
                  },
                  "zerolinecolor": "#E1E5ED",
                  "title": {
                    "text": "Positivity",
                    "font": {
                      "color": "#4D5663"
                    }
                  },
                  "gridcolor": "#E1E5ED",
                  "showgrid": true
                },
                "title": {
                  "font": {
                    "color": "#4D5663"
                  }
                },
                "paper_bgcolor": "#F5F6F9",
                "plot_bgcolor": "#F5F6F9",
                "yaxis": {
                  "tickfont": {
                    "color": "#4D5663"
                  },
                  "zerolinecolor": "#E1E5ED",
                  "title": {
                    "text": "Number of articles",
                    "font": {
                      "color": "#4D5663"
                    }
                  },
                  "gridcolor": "#E1E5ED",
                  "showgrid": true
                },
                "barmode": "overlay",
                "template": {
                  "layout": {
                    "shapedefaults": {
                      "line": {
                        "color": "#2a3f5f"
                      }
                    },
                    "polar": {
                      "radialaxis": {
                        "ticks": "",
                        "gridcolor": "white",
                        "linecolor": "white"
                      },
                      "bgcolor": "#E5ECF6",
                      "angularaxis": {
                        "ticks": "",
                        "gridcolor": "white",
                        "linecolor": "white"
                      }
                    },
                    "colorway": [
                      "#636efa",
                      "#EF553B",
                      "#00cc96",
                      "#ab63fa",
                      "#FFA15A",
                      "#19d3f3",
                      "#FF6692",
                      "#B6E880",
                      "#FF97FF",
                      "#FECB52"
                    ],
                    "xaxis": {
                      "automargin": true,
                      "title": {
                        "standoff": 15
                      },
                      "zerolinewidth": 2,
                      "ticks": "",
                      "zerolinecolor": "white",
                      "gridcolor": "white",
                      "linecolor": "white"
                    },
                    "colorscale": {
                      "diverging": [
                        [
                          0,
                          "#8e0152"
                        ],
                        [
                          0.1,
                          "#c51b7d"
                        ],
                        [
                          0.2,
                          "#de77ae"
                        ],
                        [
                          0.3,
                          "#f1b6da"
                        ],
                        [
                          0.4,
                          "#fde0ef"
                        ],
                        [
                          0.5,
                          "#f7f7f7"
                        ],
                        [
                          0.6,
                          "#e6f5d0"
                        ],
                        [
                          0.7,
                          "#b8e186"
                        ],
                        [
                          0.8,
                          "#7fbc41"
                        ],
                        [
                          0.9,
                          "#4d9221"
                        ],
                        [
                          1,
                          "#276419"
                        ]
                      ],
                      "sequential": [
                        [
                          0,
                          "#0d0887"
                        ],
                        [
                          0.1111111111111111,
                          "#46039f"
                        ],
                        [
                          0.2222222222222222,
                          "#7201a8"
                        ],
                        [
                          0.3333333333333333,
                          "#9c179e"
                        ],
                        [
                          0.4444444444444444,
                          "#bd3786"
                        ],
                        [
                          0.5555555555555556,
                          "#d8576b"
                        ],
                        [
                          0.6666666666666666,
                          "#ed7953"
                        ],
                        [
                          0.7777777777777778,
                          "#fb9f3a"
                        ],
                        [
                          0.8888888888888888,
                          "#fdca26"
                        ],
                        [
                          1,
                          "#f0f921"
                        ]
                      ],
                      "sequentialminus": [
                        [
                          0,
                          "#0d0887"
                        ],
                        [
                          0.1111111111111111,
                          "#46039f"
                        ],
                        [
                          0.2222222222222222,
                          "#7201a8"
                        ],
                        [
                          0.3333333333333333,
                          "#9c179e"
                        ],
                        [
                          0.4444444444444444,
                          "#bd3786"
                        ],
                        [
                          0.5555555555555556,
                          "#d8576b"
                        ],
                        [
                          0.6666666666666666,
                          "#ed7953"
                        ],
                        [
                          0.7777777777777778,
                          "#fb9f3a"
                        ],
                        [
                          0.8888888888888888,
                          "#fdca26"
                        ],
                        [
                          1,
                          "#f0f921"
                        ]
                      ]
                    },
                    "paper_bgcolor": "white",
                    "plot_bgcolor": "#E5ECF6",
                    "title": {
                      "x": 0.05
                    },
                    "scene": {
                      "zaxis": {
                        "backgroundcolor": "#E5ECF6",
                        "ticks": "",
                        "gridwidth": 2,
                        "showbackground": true,
                        "zerolinecolor": "white",
                        "gridcolor": "white",
                        "linecolor": "white"
                      },
                      "xaxis": {
                        "backgroundcolor": "#E5ECF6",
                        "ticks": "",
                        "gridwidth": 2,
                        "showbackground": true,
                        "zerolinecolor": "white",
                        "gridcolor": "white",
                        "linecolor": "white"
                      },
                      "yaxis": {
                        "backgroundcolor": "#E5ECF6",
                        "ticks": "",
                        "gridwidth": 2,
                        "showbackground": true,
                        "zerolinecolor": "white",
                        "gridcolor": "white",
                        "linecolor": "white"
                      }
                    },
                    "yaxis": {
                      "automargin": true,
                      "title": {
                        "standoff": 15
                      },
                      "zerolinewidth": 2,
                      "ticks": "",
                      "zerolinecolor": "white",
                      "gridcolor": "white",
                      "linecolor": "white"
                    },
                    "annotationdefaults": {
                      "arrowwidth": 1,
                      "arrowhead": 0,
                      "arrowcolor": "#2a3f5f"
                    },
                    "hoverlabel": {
                      "align": "left"
                    },
                    "ternary": {
                      "bgcolor": "#E5ECF6",
                      "baxis": {
                        "ticks": "",
                        "gridcolor": "white",
                        "linecolor": "white"
                      },
                      "caxis": {
                        "ticks": "",
                        "gridcolor": "white",
                        "linecolor": "white"
                      },
                      "aaxis": {
                        "ticks": "",
                        "gridcolor": "white",
                        "linecolor": "white"
                      }
                    },
                    "mapbox": {
                      "style": "light"
                    },
                    "hovermode": "closest",
                    "font": {
                      "color": "#2a3f5f"
                    },
                    "geo": {
                      "showland": true,
                      "landcolor": "#E5ECF6",
                      "showlakes": true,
                      "bgcolor": "white",
                      "subunitcolor": "white",
                      "lakecolor": "white"
                    },
                    "coloraxis": {
                      "colorbar": {
                        "outlinewidth": 0,
                        "ticks": ""
                      }
                    }
                  },
                  "data": {
                    "mesh3d": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "mesh3d"
                      }
                    ],
                    "scattercarpet": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattercarpet"
                      }
                    ],
                    "pie": [
                      {
                        "automargin": true,
                        "type": "pie"
                      }
                    ],
                    "surface": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "surface",
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ]
                      }
                    ],
                    "table": [
                      {
                        "header": {
                          "line": {
                            "color": "white"
                          },
                          "fill": {
                            "color": "#C8D4E3"
                          }
                        },
                        "cells": {
                          "line": {
                            "color": "white"
                          },
                          "fill": {
                            "color": "#EBF0F8"
                          }
                        },
                        "type": "table"
                      }
                    ],
                    "scatterpolargl": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatterpolargl"
                      }
                    ],
                    "contour": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "contour",
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ]
                      }
                    ],
                    "carpet": [
                      {
                        "type": "carpet",
                        "baxis": {
                          "minorgridcolor": "white",
                          "gridcolor": "white",
                          "linecolor": "white",
                          "endlinecolor": "#2a3f5f",
                          "startlinecolor": "#2a3f5f"
                        },
                        "aaxis": {
                          "minorgridcolor": "white",
                          "gridcolor": "white",
                          "linecolor": "white",
                          "endlinecolor": "#2a3f5f",
                          "startlinecolor": "#2a3f5f"
                        }
                      }
                    ],
                    "contourcarpet": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "contourcarpet"
                      }
                    ],
                    "heatmap": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "heatmap",
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ]
                      }
                    ],
                    "scattermapbox": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattermapbox"
                      }
                    ],
                    "scatter3d": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "line": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatter3d"
                      }
                    ],
                    "scattergl": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattergl"
                      }
                    ],
                    "scatterternary": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatterternary"
                      }
                    ],
                    "histogram": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "histogram"
                      }
                    ],
                    "heatmapgl": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "heatmapgl",
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ]
                      }
                    ],
                    "scatterpolar": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatterpolar"
                      }
                    ],
                    "histogram2d": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "histogram2d",
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ]
                      }
                    ],
                    "scattergeo": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattergeo"
                      }
                    ],
                    "bar": [
                      {
                        "marker": {
                          "line": {
                            "color": "#E5ECF6",
                            "width": 0.5
                          }
                        },
                        "type": "bar",
                        "error_x": {
                          "color": "#2a3f5f"
                        },
                        "error_y": {
                          "color": "#2a3f5f"
                        }
                      }
                    ],
                    "choropleth": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "choropleth"
                      }
                    ],
                    "parcoords": [
                      {
                        "line": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "parcoords"
                      }
                    ],
                    "barpolar": [
                      {
                        "marker": {
                          "line": {
                            "color": "#E5ECF6",
                            "width": 0.5
                          }
                        },
                        "type": "barpolar"
                      }
                    ],
                    "histogram2dcontour": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "histogram2dcontour",
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ]
                      }
                    ],
                    "scatter": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatter"
                      }
                    ]
                  }
                },
                "legend": {
                  "bgcolor": "#F5F6F9",
                  "font": {
                    "color": "#4D5663"
                  }
                }
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcYMYlc3Vdl_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "3811147e-1f72-4181-8871-44564da21c71"
      },
      "source": [
        "results['Polarity_predict'].iplot(\n",
        "    kind='hist',\n",
        "    histnorm='percent',\n",
        "    barmode='overlay',\n",
        "    xTitle='Positivity',\n",
        "    yTitle='Number of articles')"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "        \n",
              "        \n",
              "            <div id=\"cf0b1890-4aba-4070-bef0-6789d5784a30\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                require([\"plotly\"], function(Plotly) {\n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
              "                    \n",
              "                if (document.getElementById(\"cf0b1890-4aba-4070-bef0-6789d5784a30\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'cf0b1890-4aba-4070-bef0-6789d5784a30',\n",
              "                        [{\"histfunc\": \"count\", \"histnorm\": \"percent\", \"marker\": {\"color\": \"rgba(255, 153, 51, 1.0)\", \"line\": {\"color\": \"#4D5663\", \"width\": 1.3}}, \"name\": \"Polarity_predict\", \"opacity\": 0.8, \"orientation\": \"v\", \"type\": \"histogram\", \"x\": [\"negative\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"negative\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"neutral\", \"negative\", \"positive\", \"positive\", \"positive\", \"negative\", \"positive\", \"positive\", \"negative\", \"positive\", \"positive\", \"positive\", \"negative\", \"negative\", \"positive\", \"positive\", \"positive\", \"positive\", \"neutral\", \"positive\", \"positive\", \"negative\", \"neutral\", \"negative\", \"positive\", \"positive\", \"positive\", \"negative\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"negative\", \"negative\", \"positive\", \"negative\", \"positive\", \"positive\", \"positive\", \"positive\", \"negative\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"negative\", \"positive\", \"negative\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"negative\", \"positive\", \"positive\", \"neutral\", \"positive\", \"positive\", \"positive\", \"negative\", \"positive\", \"negative\", \"neutral\", \"negative\", \"positive\", \"neutral\", \"neutral\", \"positive\", \"neutral\", \"positive\", \"positive\", \"negative\", \"negative\", \"negative\", \"positive\", \"positive\", \"positive\", \"negative\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"negative\", \"neutral\", \"negative\", \"positive\", \"negative\", \"negative\", \"positive\", \"negative\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"neutral\", \"positive\", \"positive\", \"negative\", \"negative\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"negative\", \"positive\", \"negative\", \"positive\", \"positive\", \"positive\", \"positive\", \"neutral\", \"neutral\", \"positive\", \"positive\", \"positive\", \"negative\", \"positive\", \"positive\", \"positive\", \"neutral\", \"positive\", \"positive\", \"positive\", \"negative\", \"positive\", \"positive\", \"negative\", \"positive\", \"negative\", \"neutral\", \"positive\", \"positive\", \"positive\", \"positive\", \"negative\", \"negative\", \"negative\", \"negative\"]}],\n",
              "                        {\"barmode\": \"overlay\", \"legend\": {\"bgcolor\": \"#F5F6F9\", \"font\": {\"color\": \"#4D5663\"}}, \"paper_bgcolor\": \"#F5F6F9\", \"plot_bgcolor\": \"#F5F6F9\", \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"font\": {\"color\": \"#4D5663\"}}, \"xaxis\": {\"gridcolor\": \"#E1E5ED\", \"showgrid\": true, \"tickfont\": {\"color\": \"#4D5663\"}, \"title\": {\"font\": {\"color\": \"#4D5663\"}, \"text\": \"Positivity\"}, \"zerolinecolor\": \"#E1E5ED\"}, \"yaxis\": {\"gridcolor\": \"#E1E5ED\", \"showgrid\": true, \"tickfont\": {\"color\": \"#4D5663\"}, \"title\": {\"font\": {\"color\": \"#4D5663\"}, \"text\": \"Number of articles\"}, \"zerolinecolor\": \"#E1E5ED\"}},\n",
              "                        {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('cf0b1890-4aba-4070-bef0-6789d5784a30');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                });\n",
              "            </script>\n",
              "        </div>"
            ],
            "application/vnd.plotly.v1+json": {
              "data": [
                {
                  "opacity": 0.8,
                  "orientation": "v",
                  "histnorm": "percent",
                  "histfunc": "count",
                  "marker": {
                    "color": "rgba(255, 153, 51, 1.0)",
                    "line": {
                      "color": "#4D5663",
                      "width": 1.3
                    }
                  },
                  "x": [
                    "negative",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "negative",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "neutral",
                    "negative",
                    "positive",
                    "positive",
                    "positive",
                    "negative",
                    "positive",
                    "positive",
                    "negative",
                    "positive",
                    "positive",
                    "positive",
                    "negative",
                    "negative",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "neutral",
                    "positive",
                    "positive",
                    "negative",
                    "neutral",
                    "negative",
                    "positive",
                    "positive",
                    "positive",
                    "negative",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "negative",
                    "negative",
                    "positive",
                    "negative",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "negative",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "negative",
                    "positive",
                    "negative",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "negative",
                    "positive",
                    "positive",
                    "neutral",
                    "positive",
                    "positive",
                    "positive",
                    "negative",
                    "positive",
                    "negative",
                    "neutral",
                    "negative",
                    "positive",
                    "neutral",
                    "neutral",
                    "positive",
                    "neutral",
                    "positive",
                    "positive",
                    "negative",
                    "negative",
                    "negative",
                    "positive",
                    "positive",
                    "positive",
                    "negative",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "negative",
                    "neutral",
                    "negative",
                    "positive",
                    "negative",
                    "negative",
                    "positive",
                    "negative",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "neutral",
                    "positive",
                    "positive",
                    "negative",
                    "negative",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "negative",
                    "positive",
                    "negative",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "neutral",
                    "neutral",
                    "positive",
                    "positive",
                    "positive",
                    "negative",
                    "positive",
                    "positive",
                    "positive",
                    "neutral",
                    "positive",
                    "positive",
                    "positive",
                    "negative",
                    "positive",
                    "positive",
                    "negative",
                    "positive",
                    "negative",
                    "neutral",
                    "positive",
                    "positive",
                    "positive",
                    "positive",
                    "negative",
                    "negative",
                    "negative",
                    "negative"
                  ],
                  "type": "histogram",
                  "name": "Polarity_predict"
                }
              ],
              "config": {
                "plotlyServerURL": "https://plot.ly",
                "linkText": "Export to plot.ly",
                "showLink": true
              },
              "layout": {
                "xaxis": {
                  "tickfont": {
                    "color": "#4D5663"
                  },
                  "zerolinecolor": "#E1E5ED",
                  "title": {
                    "text": "Positivity",
                    "font": {
                      "color": "#4D5663"
                    }
                  },
                  "gridcolor": "#E1E5ED",
                  "showgrid": true
                },
                "title": {
                  "font": {
                    "color": "#4D5663"
                  }
                },
                "paper_bgcolor": "#F5F6F9",
                "plot_bgcolor": "#F5F6F9",
                "yaxis": {
                  "tickfont": {
                    "color": "#4D5663"
                  },
                  "zerolinecolor": "#E1E5ED",
                  "title": {
                    "text": "Number of articles",
                    "font": {
                      "color": "#4D5663"
                    }
                  },
                  "gridcolor": "#E1E5ED",
                  "showgrid": true
                },
                "barmode": "overlay",
                "template": {
                  "layout": {
                    "shapedefaults": {
                      "line": {
                        "color": "#2a3f5f"
                      }
                    },
                    "polar": {
                      "radialaxis": {
                        "ticks": "",
                        "gridcolor": "white",
                        "linecolor": "white"
                      },
                      "bgcolor": "#E5ECF6",
                      "angularaxis": {
                        "ticks": "",
                        "gridcolor": "white",
                        "linecolor": "white"
                      }
                    },
                    "colorway": [
                      "#636efa",
                      "#EF553B",
                      "#00cc96",
                      "#ab63fa",
                      "#FFA15A",
                      "#19d3f3",
                      "#FF6692",
                      "#B6E880",
                      "#FF97FF",
                      "#FECB52"
                    ],
                    "xaxis": {
                      "automargin": true,
                      "title": {
                        "standoff": 15
                      },
                      "zerolinewidth": 2,
                      "ticks": "",
                      "zerolinecolor": "white",
                      "gridcolor": "white",
                      "linecolor": "white"
                    },
                    "colorscale": {
                      "diverging": [
                        [
                          0,
                          "#8e0152"
                        ],
                        [
                          0.1,
                          "#c51b7d"
                        ],
                        [
                          0.2,
                          "#de77ae"
                        ],
                        [
                          0.3,
                          "#f1b6da"
                        ],
                        [
                          0.4,
                          "#fde0ef"
                        ],
                        [
                          0.5,
                          "#f7f7f7"
                        ],
                        [
                          0.6,
                          "#e6f5d0"
                        ],
                        [
                          0.7,
                          "#b8e186"
                        ],
                        [
                          0.8,
                          "#7fbc41"
                        ],
                        [
                          0.9,
                          "#4d9221"
                        ],
                        [
                          1,
                          "#276419"
                        ]
                      ],
                      "sequential": [
                        [
                          0,
                          "#0d0887"
                        ],
                        [
                          0.1111111111111111,
                          "#46039f"
                        ],
                        [
                          0.2222222222222222,
                          "#7201a8"
                        ],
                        [
                          0.3333333333333333,
                          "#9c179e"
                        ],
                        [
                          0.4444444444444444,
                          "#bd3786"
                        ],
                        [
                          0.5555555555555556,
                          "#d8576b"
                        ],
                        [
                          0.6666666666666666,
                          "#ed7953"
                        ],
                        [
                          0.7777777777777778,
                          "#fb9f3a"
                        ],
                        [
                          0.8888888888888888,
                          "#fdca26"
                        ],
                        [
                          1,
                          "#f0f921"
                        ]
                      ],
                      "sequentialminus": [
                        [
                          0,
                          "#0d0887"
                        ],
                        [
                          0.1111111111111111,
                          "#46039f"
                        ],
                        [
                          0.2222222222222222,
                          "#7201a8"
                        ],
                        [
                          0.3333333333333333,
                          "#9c179e"
                        ],
                        [
                          0.4444444444444444,
                          "#bd3786"
                        ],
                        [
                          0.5555555555555556,
                          "#d8576b"
                        ],
                        [
                          0.6666666666666666,
                          "#ed7953"
                        ],
                        [
                          0.7777777777777778,
                          "#fb9f3a"
                        ],
                        [
                          0.8888888888888888,
                          "#fdca26"
                        ],
                        [
                          1,
                          "#f0f921"
                        ]
                      ]
                    },
                    "paper_bgcolor": "white",
                    "plot_bgcolor": "#E5ECF6",
                    "title": {
                      "x": 0.05
                    },
                    "scene": {
                      "zaxis": {
                        "backgroundcolor": "#E5ECF6",
                        "ticks": "",
                        "gridwidth": 2,
                        "showbackground": true,
                        "zerolinecolor": "white",
                        "gridcolor": "white",
                        "linecolor": "white"
                      },
                      "xaxis": {
                        "backgroundcolor": "#E5ECF6",
                        "ticks": "",
                        "gridwidth": 2,
                        "showbackground": true,
                        "zerolinecolor": "white",
                        "gridcolor": "white",
                        "linecolor": "white"
                      },
                      "yaxis": {
                        "backgroundcolor": "#E5ECF6",
                        "ticks": "",
                        "gridwidth": 2,
                        "showbackground": true,
                        "zerolinecolor": "white",
                        "gridcolor": "white",
                        "linecolor": "white"
                      }
                    },
                    "yaxis": {
                      "automargin": true,
                      "title": {
                        "standoff": 15
                      },
                      "zerolinewidth": 2,
                      "ticks": "",
                      "zerolinecolor": "white",
                      "gridcolor": "white",
                      "linecolor": "white"
                    },
                    "annotationdefaults": {
                      "arrowwidth": 1,
                      "arrowhead": 0,
                      "arrowcolor": "#2a3f5f"
                    },
                    "hoverlabel": {
                      "align": "left"
                    },
                    "ternary": {
                      "bgcolor": "#E5ECF6",
                      "baxis": {
                        "ticks": "",
                        "gridcolor": "white",
                        "linecolor": "white"
                      },
                      "caxis": {
                        "ticks": "",
                        "gridcolor": "white",
                        "linecolor": "white"
                      },
                      "aaxis": {
                        "ticks": "",
                        "gridcolor": "white",
                        "linecolor": "white"
                      }
                    },
                    "mapbox": {
                      "style": "light"
                    },
                    "hovermode": "closest",
                    "font": {
                      "color": "#2a3f5f"
                    },
                    "geo": {
                      "showland": true,
                      "landcolor": "#E5ECF6",
                      "showlakes": true,
                      "bgcolor": "white",
                      "subunitcolor": "white",
                      "lakecolor": "white"
                    },
                    "coloraxis": {
                      "colorbar": {
                        "outlinewidth": 0,
                        "ticks": ""
                      }
                    }
                  },
                  "data": {
                    "mesh3d": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "mesh3d"
                      }
                    ],
                    "scattercarpet": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattercarpet"
                      }
                    ],
                    "pie": [
                      {
                        "automargin": true,
                        "type": "pie"
                      }
                    ],
                    "surface": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "surface",
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ]
                      }
                    ],
                    "table": [
                      {
                        "header": {
                          "line": {
                            "color": "white"
                          },
                          "fill": {
                            "color": "#C8D4E3"
                          }
                        },
                        "cells": {
                          "line": {
                            "color": "white"
                          },
                          "fill": {
                            "color": "#EBF0F8"
                          }
                        },
                        "type": "table"
                      }
                    ],
                    "scatterpolargl": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatterpolargl"
                      }
                    ],
                    "contour": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "contour",
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ]
                      }
                    ],
                    "carpet": [
                      {
                        "type": "carpet",
                        "baxis": {
                          "minorgridcolor": "white",
                          "gridcolor": "white",
                          "linecolor": "white",
                          "endlinecolor": "#2a3f5f",
                          "startlinecolor": "#2a3f5f"
                        },
                        "aaxis": {
                          "minorgridcolor": "white",
                          "gridcolor": "white",
                          "linecolor": "white",
                          "endlinecolor": "#2a3f5f",
                          "startlinecolor": "#2a3f5f"
                        }
                      }
                    ],
                    "contourcarpet": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "contourcarpet"
                      }
                    ],
                    "heatmap": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "heatmap",
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ]
                      }
                    ],
                    "scattermapbox": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattermapbox"
                      }
                    ],
                    "scatter3d": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "line": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatter3d"
                      }
                    ],
                    "scattergl": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattergl"
                      }
                    ],
                    "scatterternary": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatterternary"
                      }
                    ],
                    "histogram": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "histogram"
                      }
                    ],
                    "heatmapgl": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "heatmapgl",
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ]
                      }
                    ],
                    "scatterpolar": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatterpolar"
                      }
                    ],
                    "histogram2d": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "histogram2d",
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ]
                      }
                    ],
                    "scattergeo": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattergeo"
                      }
                    ],
                    "bar": [
                      {
                        "marker": {
                          "line": {
                            "color": "#E5ECF6",
                            "width": 0.5
                          }
                        },
                        "type": "bar",
                        "error_x": {
                          "color": "#2a3f5f"
                        },
                        "error_y": {
                          "color": "#2a3f5f"
                        }
                      }
                    ],
                    "choropleth": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "choropleth"
                      }
                    ],
                    "parcoords": [
                      {
                        "line": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "parcoords"
                      }
                    ],
                    "barpolar": [
                      {
                        "marker": {
                          "line": {
                            "color": "#E5ECF6",
                            "width": 0.5
                          }
                        },
                        "type": "barpolar"
                      }
                    ],
                    "histogram2dcontour": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "histogram2dcontour",
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ]
                      }
                    ],
                    "scatter": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatter"
                      }
                    ]
                  }
                },
                "legend": {
                  "bgcolor": "#F5F6F9",
                  "font": {
                    "color": "#4D5663"
                  }
                }
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnNzec9HVdmG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results.to_csv('results_bert.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9f9PjYgVdmL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "c55304f7-5a07-4483-b3d8-3cec4992ffff"
      },
      "source": [
        "results_wide.head()"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source</th>\n",
              "      <th>Author</th>\n",
              "      <th>Url</th>\n",
              "      <th>Date</th>\n",
              "      <th>Article</th>\n",
              "      <th>Title</th>\n",
              "      <th>Polarity distribution</th>\n",
              "      <th>Polarity_true</th>\n",
              "      <th>Polarity_predict</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wired</td>\n",
              "      <td>Paris Martineau</td>\n",
              "      <td>https://www.wired.com/story/facebook-removes-a...</td>\n",
              "      <td>2019-12-21T01:21:05Z</td>\n",
              "      <td>Facebook on Friday removed what it called a gl...</td>\n",
              "      <td>Facebook Removes Accounts With AI-Generated Pr...</td>\n",
              "      <td>[-0.5121273, -2.8696733, -1.0669303]</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Wired</td>\n",
              "      <td>Tom Simonite</td>\n",
              "      <td>https://www.wired.com/story/ai-doctor-will-see...</td>\n",
              "      <td>2019-12-21T13:00:00Z</td>\n",
              "      <td>When MIT professor Regina Barzilay received he...</td>\n",
              "      <td>The AI Doctor Will See You Now</td>\n",
              "      <td>[-4.0325685, -4.2178593, -0.03299731]</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Wired</td>\n",
              "      <td>Andy Greenberg, Brian Barrett</td>\n",
              "      <td>https://www.wired.com/story/facebook-two-facto...</td>\n",
              "      <td>2019-12-21T14:00:00Z</td>\n",
              "      <td>It's beginning to look a lot like the end of t...</td>\n",
              "      <td>Facebook Finally Fixes Its Two-Factor Mess</td>\n",
              "      <td>[-1.710292, -1.7299899, -0.44332144]</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TechCrunch</td>\n",
              "      <td>Devin Coldewey</td>\n",
              "      <td>http://techcrunch.com/2019/12/21/how-to-bring-...</td>\n",
              "      <td>2019-12-21T17:47:37Z</td>\n",
              "      <td>Artificial intelligence is a powerful tool, bu...</td>\n",
              "      <td>Luminance and Omnius are bringing AI to legacy...</td>\n",
              "      <td>[-4.8914614, -4.3072643, -0.02120397]</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The Next Web</td>\n",
              "      <td>John Gikopoulos</td>\n",
              "      <td>https://thenextweb.com/podium/2019/12/21/2020-...</td>\n",
              "      <td>2019-12-21T20:00:39Z</td>\n",
              "      <td>One of the ways you can tell that a new techno...</td>\n",
              "      <td>2020 will see AI move from buzzwords to solutions</td>\n",
              "      <td>[-1.0109742, -1.5238645, -0.8716344]</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Source                         Author  ... Polarity_true Polarity_predict\n",
              "0         Wired                Paris Martineau  ...             2                0\n",
              "1         Wired                   Tom Simonite  ...             2                2\n",
              "2         Wired  Andy Greenberg, Brian Barrett  ...             2                2\n",
              "3    TechCrunch                 Devin Coldewey  ...             2                2\n",
              "4  The Next Web                John Gikopoulos  ...             0                2\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d3fWjCQzRQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from google.colab import files\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K81nkKrEVdmP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_wide.to_csv(\"results_bert_wide.csv\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThEGt7qIVdmS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "distribution1=[]\n",
        "distribution2=[]\n",
        "distribution3=[]\n",
        "for i in range(len(distribution)):\n",
        "    distribution1.append(distribution_01[i][0])\n",
        "    distribution2.append(distribution_01[i][1])\n",
        "    distribution3.append(distribution_01[i][2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f_2OfpNVdmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_wide_1=pd.DataFrame(data={'Source':test['Source'],'Author':test['Author'],'Url':test['Url'],'Date':test['Date'],'Article':test['Article'],'Title':test['Title'],'Polarity distribution1':distribution1,'Polarity distribution2':distribution2,'Polarity distribution3':distribution3, 'Polarity_true': test['Polarity'], 'Polarity_predict': polarity})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSajb0a07OP9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "64536fd3-57f4-4e5d-b91c-331b2498eb64"
      },
      "source": [
        "#result_wide_1['Max probability']=max(results_wide_1['Polarity distribution1'], results_wide_1['Polarity distribution2'],  results_wide_1['Polarity distribution3'])\n",
        "result_wide_1['Max probability'] = results_wide_1[['Polarity distribution1','Polarity distribution2','Polarity distribution3']].apply(np.max,axis=1)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-132-c03855e79421>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult_wide_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Max probability'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_wide_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Polarity distribution1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Polarity distribution2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Polarity distribution3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'result_wide_1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL7-y7Ym7_s4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "e40d91e5-27ab-4a44-b289-147bc5656b2e"
      },
      "source": [
        "results_wide_1['Polarity distribution1']"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      0.003366\n",
              "1      0.000100\n",
              "2      0.001016\n",
              "3      0.000042\n",
              "4      0.002044\n",
              "         ...   \n",
              "173    0.000039\n",
              "174    0.005176\n",
              "175    0.005082\n",
              "176    0.005129\n",
              "177    0.005264\n",
              "Name: Polarity distribution1, Length: 178, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXL2ZanPVdmY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "outputId": "27053319-b478-458f-a661-79b99b65fb42"
      },
      "source": [
        "results_wide_1.head()"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source</th>\n",
              "      <th>Author</th>\n",
              "      <th>Url</th>\n",
              "      <th>Date</th>\n",
              "      <th>Article</th>\n",
              "      <th>Title</th>\n",
              "      <th>Polarity distribution1</th>\n",
              "      <th>Polarity distribution2</th>\n",
              "      <th>Polarity distribution3</th>\n",
              "      <th>Polarity_true</th>\n",
              "      <th>Polarity_predict</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wired</td>\n",
              "      <td>Paris Martineau</td>\n",
              "      <td>https://www.wired.com/story/facebook-removes-a...</td>\n",
              "      <td>2019-12-21T01:21:05Z</td>\n",
              "      <td>Facebook on Friday removed what it called a gl...</td>\n",
              "      <td>Facebook Removes Accounts With AI-Generated Pr...</td>\n",
              "      <td>0.003366</td>\n",
              "      <td>0.000319</td>\n",
              "      <td>0.001933</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Wired</td>\n",
              "      <td>Tom Simonite</td>\n",
              "      <td>https://www.wired.com/story/ai-doctor-will-see...</td>\n",
              "      <td>2019-12-21T13:00:00Z</td>\n",
              "      <td>When MIT professor Regina Barzilay received he...</td>\n",
              "      <td>The AI Doctor Will See You Now</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.000083</td>\n",
              "      <td>0.005436</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Wired</td>\n",
              "      <td>Andy Greenberg, Brian Barrett</td>\n",
              "      <td>https://www.wired.com/story/facebook-two-facto...</td>\n",
              "      <td>2019-12-21T14:00:00Z</td>\n",
              "      <td>It's beginning to look a lot like the end of t...</td>\n",
              "      <td>Facebook Finally Fixes Its Two-Factor Mess</td>\n",
              "      <td>0.001016</td>\n",
              "      <td>0.000996</td>\n",
              "      <td>0.003606</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TechCrunch</td>\n",
              "      <td>Devin Coldewey</td>\n",
              "      <td>http://techcrunch.com/2019/12/21/how-to-bring-...</td>\n",
              "      <td>2019-12-21T17:47:37Z</td>\n",
              "      <td>Artificial intelligence is a powerful tool, bu...</td>\n",
              "      <td>Luminance and Omnius are bringing AI to legacy...</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>0.000076</td>\n",
              "      <td>0.005500</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The Next Web</td>\n",
              "      <td>John Gikopoulos</td>\n",
              "      <td>https://thenextweb.com/podium/2019/12/21/2020-...</td>\n",
              "      <td>2019-12-21T20:00:39Z</td>\n",
              "      <td>One of the ways you can tell that a new techno...</td>\n",
              "      <td>2020 will see AI move from buzzwords to solutions</td>\n",
              "      <td>0.002044</td>\n",
              "      <td>0.001224</td>\n",
              "      <td>0.002350</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Source                         Author  ... Polarity_true Polarity_predict\n",
              "0         Wired                Paris Martineau  ...             2                0\n",
              "1         Wired                   Tom Simonite  ...             2                2\n",
              "2         Wired  Andy Greenberg, Brian Barrett  ...             2                2\n",
              "3    TechCrunch                 Devin Coldewey  ...             2                2\n",
              "4  The Next Web                John Gikopoulos  ...             0                2\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaBetV-cVdmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_wide_1.to_csv('results_bert_wide1.csv') \n",
        "files.download('results_bert_wide1.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}